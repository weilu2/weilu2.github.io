<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[基于经验数据最小化风险泛函的问题]]></title>
    <url>%2F2018%2F12%2F26%2F%E5%9F%BA%E4%BA%8E%E7%BB%8F%E9%AA%8C%E6%95%B0%E6%8D%AE%E6%9C%80%E5%B0%8F%E5%8C%96%E9%A3%8E%E9%99%A9%E6%B3%9B%E5%87%BD%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[摘要描述最小化风险泛函问题的几个特殊的例子，包括模式识别、回归估计和密度估计问题。 1. 问题的抽象描述 用所期望的品质来选取一个函数，就相当于在所有可能的函数中，用我们自定义的一个度量标准来选择一个符合我们度量标准的最合适的函数。 形式上：在向量空间 $R^n$ 的子集 $Z$ 上，给定一个函数集 $\{g(z)\}, z \in Z$，定义一个泛函： C = C(g(z) \tag{E1.1}1.1. 直接最小化泛函 该泛函就是选取函数的度量标准。然后从函数集 $\{g(z)\}$ 中找出一个能够最小化泛函（E1.1）的函数 $g^*(z)$。 假设当泛函取最小值时对应最合适的函数 $g^(z)$ ，并且 $\{g(z)\}$ 中存在泛函（E1.1）的最小值。在显示的给出函数集 $\{g(z)\}$ 和泛函 $C = C(g(z)$ 的情况下，寻找最小化泛函的函数 $g^(z)$ 是变分法的研究主题。 1.2. 基于经验数据最小化风险泛函 另一种情况是在 $Z$ 上定义概率分布函数 $F(z)$，然后将泛函定义为数学期望： C(g(z)) = \intop L(z, g(z)) d F(z) \tag{E1.2} 其中函数 $L(z, g(z))$ 对任意 $g(z) \in \{g(z)\}$ 都是可积的。在这种情况下通过最小化泛函（E1.2）来找到最合适的函数 $g^*(z)$。 现在这个概率分布函数 $F(z)$ 是未知的，但是有依据 $F(z)$ 独立同分布的抽取的观测样本： z_1, z_2, \ldots, z_l \tag{E1.3} 因此需要依据这些观测样本来最小化泛函（E1.2）。 1.3. 区别 第一种情况的基本问题是构造一个搜索过程，从函数集 $\{g(z)\}$ 中找出最小化泛函的一个函数 $g^(z)$。搜索过程使用最小化泛函（E1.1）作为度量标准。其中泛函（E1.1）是已知的，重点在于定义如何搜索 $g^(z)$ 的这个过程。怎么去找。 而第二种情况的基本问题是用公式来表示一个选择函数的准则。因为泛函（E1.2）中的概率分布函数 $F(z)$ 是未知的，因此不能直接用泛函（E1.2）作为度量标准，而是首先需要明确这个度量标准。 1.4. 度量标准 在讨论泛函（1.2）的最小化问题时，我们用 $\{g(z, \alpha), \alpha \in \Lambda \}$ 的形式来表示函数集 $\{g(z)\}$。这里给出的 $\alpha \in \Lambda$ 表示的是一个参数集合，每个特定的参数 $\alpha^$ 确定了函数集 $\{g(z, \alpha), \alpha \in \Lambda \}$ 中一个特定的函数 $g(z, \alpha^)$，要找到所求的函数，实际上就是找到这个特定的 $\alpha^*$ 的值。 因此我们可以将泛函（E1.2）重新表示成： R(\alpha) = \intop Q(z, \alpha) \space d F(z), \alpha \in \Lambda \tag{E1.4} 其中： Q(z, \alpha) = L(z, g(z, \alpha)) 我们将函数 $R(z, \alpha)$ 称为损失函数，它与变量 $z$ 和 $\alpha$ 有关。对于一个确定的参数 $\alpha^*$，其期望损失为： R(\alpha^*) = \intop Q(z, \alpha^*) \space d F(z) 对于一个固定的问题，其概率分布函数 $F(z)$ 是未知但固定的，因此最终的风险就由 $Q(z, \alpha)$ 所决定，其中 $z$ 是独立同分布的观测数据 $z_1, z_2, \ldots, z_l$，那么我们的问题就变成了在函数集 $\{Q(z, \alpha), \alpha \in \Lambda\}$ 中选取一个最小化风险的函数 $Q(z, \alpha^*)$。 1.5. 理解猜想 我的理解中，直接最小化泛函相当于是基于经验数据最小化泛函的更高层次的抽象。把直接最小化泛函中的这个泛函说成已知的或许不太合适，更合适的说法应该是不关心这个泛函的具体形式，而只是从抽象的层面去解释。 而基于经验数据的最小化泛函相当于给定了泛函的一种形式，不过这个形式有点特别，是基于位置的概率分布函数的形式，这种概率分布只能通过一些观测数据进行猜想，在这种情况下去解决最小化泛函问题。 2. 模式识别问题 基于经验数据最小化风险泛函的问题是一个比较一般的抽象问题，其可以对应到几个具体的基本统计学问题，模式识别是其中一个比较典型的问题。 模式识别问题是 20 世纪 50 年代末正式提出来的。可以表示为：目标函数对观测到的每个事件确定其所属的分类；学习机器构造一个函数用于模拟目标函数的分类工作。 也就是说在一个概率分布函数为 $F(x)$ 的环境中，目标函数将每个独立随机出现的事件归到 $k$ 个分类中的一个，那么这个目标函数在对事件进行归类的时候也一定存在一个条件概率分布，我们假设为 $F(y|x)$，其中 $y \in \{0, 1, \ldots, k-1\}$，目标函数依据这个条件概率分布将事件划分到 $k$ 中的一类，完成分类。因此一个事件属于某个分类的联合分布为 $F(y,x) = F(y|x)F(x)$。 现在给定一个函数集 $\{\phi(x, \alpha), \alpha \in \Lambda\}$，这个函数 $\phi$ 的取值为集合 $\{0, 1, \ldots, k-1\}$ 中的一个。考虑一个最简单的损失函数： L(y, \phi) = \begin{cases} 0 &\text{if } y = \phi \\ 1 &\text{if } y \not = \phi \end{cases} \tag{E1.5} 其中如果 $y = \phi$ 表明预测正确，否则预测错误。模式识别问题就是在函数集 $\{\phi(x, \alpha), \alpha \in \Lambda\}$ 上最小化泛函： R(\alpha) = \intop L(y, \phi(x, \alpha)) \space d F(y, x) \tag{E1.6} 对于损失函数（E1.5），泛函（E1.6）确定了对于任何给定的函数 $\phi(x, \alpha)$，分类错误的概率。 其中的联合分布 $F(y, x)$ 是未知的，但是给出了一些独立同分布的样本对： (x_1, y_1), \ldots, (x_l, y_l) \tag{E1.7} 其问题就是在未知分布但给定数据的情况下最小化分类错误的概率。 综上所述，模式识别问题其实可以归结为基于经验数据最小化风险的问题。但是这里有两个特殊之处： 1. 观测数据对中的观测结果 $y$ 是属于有限离散集合中的一个值，如果是二分类问题，则是只有两个取值； 2. 损失函数 $L(y, \phi)$ 同样也是只有两个取值，0 表示分类错误，1 表示分类正确； 模式识别问题是基于经验数据的风险最小化问题的一个特殊问题。这个例子中给出的损失函数是最简单的指示函数，实际上还可能会使用一些其他的函数来表示损失函数，其值可能就不是属于有限离散集合了，也有可能是连续型值。 3. 回归估计问题3.1. 回归估计问题 一般来说我们把每个元素 $x \in X$ 对应于惟一的元素 $y \in Y$，则两个元素集合 $X$ 和 $Y$ 通过某一函数依赖关系相互关联，并且 $X$ 是一个向量集合，$Y$ 是一个标量集合，那么这种关系被称为函数。 不过也存在这样的随机关系，即每个向量 $x$ 对应于一个 $y$，而这个对应是基于某个随机试验的结果得出的。对于每个 $x$，在 $Y$ 上定义一个分部 $F(y|x)$，使 $y$ 值的选取是依据这一分部来实现的。这一的条件概率函数表达了 $y$ 和 $x$ 之间的随机关系。 对于上面的这两段内容我们可以这样来理解。比如我们建立一个模型表示光照、风速、湿度等几个变量和温度之间的关系，光照、风速和湿度三个值构成了向量 $x$，而温度则构成了变量 $y$。 理论上来说这其中肯定是存在某种对应关系的，在不考虑其他任何因素的影响条件下，我们可以使用一个函数来将这个关系的抽象形式表达出来。 但如果我们要通过实际的实验来获取一批数据去找到这个对应关系的话，就会碰到这样一些问题，即使我们通过控制变量法进行实验，控制的变量也没有办法达到绝对的精确，比如不同的温度计之间的测量结果存在很小的误差，温度和湿度可能存在局部的不均衡等情况，也就是说实际的观测数据是由一个核心规律和一些随机的干扰因素合并得到的最终结果。 这种情况下，我们直接认为 $x$ 和 $y$ 之间存在某种随机对应关系。我们使用 $F(x)$ 表示随机生成向量 $x$ 的概率分布。使用 $F(y|x)$ 表示在随机试验中观测到 $y$ 值的条件概率。这种情况下就存在一个联合分布函数 $F(x, y) = F(x)F(y|x)$，根据这一联合分布函数独立同分布的生成观测点： (x_1, y_1), \ldots, (x_l, y_l) 要将 $F(y|x)$ 求出来是相当困难的问题，不过我们可以退而求其次，只求条件期望函数： r(x) = \intop y \space d F(y|x) \tag{E1.8} 求出条件期望函数相当于是在条件 $x$ 下，该分布对应的一个近似值，用这个近似值表示 $y$。我们要做的就是将这个条件期望函数求出来。这一函数被称为回归。在函数集 $\{f(x, \alpha), \alpha \in \Lambda\}$ 上估计这一函数的问题被称为回归估计问题。 3.2. 风险泛函 对于回归问题，我们使用欧式距离来度量风险： R(\alpha) = \intop (y - f(x, \alpha))^2 \space d F(x, y) \tag{E1.9}3.3. 通过最小化经验风险能够解决回归估计问题的证明 在集合 $\{f(x, \alpha), \alpha \in \Lambda \space\space (f(x, \alpha) \in L_2(P)) \}$ 上： 1. 如果回归 $r(x)$ 属于 $\{f(x, \alpha), \alpha \in \Lambda \}$ 则在回归函数上能够得到泛函（E1.9）的最小值。 2. 如果回归 $r(x)$ 不属于 $\{f(x, \alpha), \alpha \in \Lambda \}$ 则可以在集合 $\{f(x, \alpha), \alpha \in \Lambda \}$ 上求得一个函数 $f(x, \alpha^*)$，使得这个函数在 $L_2(P)$ 度量： \varrho(f_1, f_2) = \sqrt{\intop (f_1(x) - f_2(x))^2 \space d F(x)} 下最接近于回归 $r(x)$。 证明 设： \Delta f(x, \alpha) = f(x, \alpha) - r(x) 则： f(x, \alpha) = \Delta f(x, \alpha) + r(x) 将这个式子代入泛函（E1.9）中，则泛函（E1.9）可以改写为以下形式： R(\alpha) = \intop (y - \Delta f(x, \alpha) - r(x))^2 \space d F(x, y) 将这个平方求出来可以得到： R(\alpha) = \intop (y - r(x))^2 \space d F(x, y) + \intop (\Delta f(x, \alpha))^2 \space d F(x, y) - 2 \intop (y - r(x))(\Delta f(x, \alpha)) \space d F(x, y) 根据（E1.8）可以将联合分布函数 $F(x,y)$ 拆分为 $F(x)$ 和条件分布函数 $F(y|x)$，就可以将以上式子中的第三项约去： \intop (y - r(x))(\Delta f(x, \alpha)) \space d F(x, y) = \intop (\Delta f(x, \alpha)) \bigg [ \intop (y - r(x)) \space d F(y|x) \bigg ] \space d F(x) 这样就证明了： R(\alpha) = \intop (y - r(x))^2 \space d F(x, y) + \intop (\Delta f(x, \alpha))^2 \space d F(x, y) 第一项与 $\alpha$ 无关，相当于一个固定值。不去管它。如果 $r(x) \in f(x, \alpha)$，说明一定存在某个 $\alpha^$ 使得 $r(x) = f(x, \alpha^)$，这种情况下第二项就是零，也就是风险是说求得的这个函数 $f(x, \alpha^*)$ 就是回归函数；如果 $r(x) \not \in f(x, \alpha)$，那么最小化风险泛函 $R(\alpha)$ 就相当于找到最接近于回归的函数。 因此，回归估计问题也可以归纳到最小化期望风险的体系中。特殊之处在于： 1. 观测数据对中的观测结果 $y$ 是属于正无穷到负无穷内的一个任意值； 2. 损失函数集合 $Q(z, \alpha), \alpha \in \Lambda$ 的形式为：$Q(z, \alpha) = (y - f(x, \alpha))^2$ 3.4. 理解 上文中说如果回归 $r(x)$ 属于 $\{f(x, \alpha), \alpha \in \Lambda \}$，我的理解是在一个有限函数集合中能够找到这个回归，就是实际的目标函数，并且理论上来说这个回归对应的风险应该是 0。 如果回归 $r(x)$ 不属于 $\{f(x, \alpha), \alpha \in \Lambda \}$，说明无法在一个有限集合中找到这个回归函数。那么我们的问题就变成了求一个回归函数 $r(x)$ 的近似解。并且需要证明通过使用欧氏距离的度量能够找到这个函数 $f(x, \alpha^*)$ 是回归的最近似解。 4. 解释间接测量结果的问题 假设我们要估计一个函数 $f(t)$，但是对于任何的一个 $t^$，我们都无法测量到函数 $f(t^)$ 的值。 与此同时，我们知道一个函数： F(x) = A f(t) 通过某种依赖关系函数 $F(x)$ 与函数 $f(t)$ 相关联，同时 $F(x)$ 是可以被测量的。其观测值为带有误差 $\xi$ 的结果： y_1, y_2, \ldots, y_l, \quad y_i = F(x_i) + \xi_i 在集合 $\{ f(t, \alpha) \}$ 上根据这些观测值求出函数 $f(t)$。这类问题被称为间接测量结果的问题。 这个问题可以形式化的表示为：在未知函数 $F(x)$ 但给定测量值 $y_1, \ldots, y_l$ 的条件下，假定有一个连续函数 $A$，它以一对一的方式将度量空间 $E_1$ 中的元素 $f(t, \alpha)$ 映射到度量空间 $E_2$ 中的元素 $F(x, \alpha)$，问题是从函数集 $\{f(x, \alpha), \alpha \in \Lambda\}$ 中求出函数 $A$ 的解。 我们假设，$F(x)$ 的测量值中不包含系统误差，即： Ey_{x_i} = F(x_i) 并且随机变量 $y_{x_i}$ 和 $y_{x_j} \space (i \not = j)$ 相互独立。我们还假设函数定义在区间 $[a,b]$ 上。函数 $F(x)$ 在该区间上测量的点是根据均匀分布律独立随机的散布在该区间上的。 解释间接实验结果的问题也可以归结为基于经验数据最小化期望风险的问题。考虑泛函： R(\alpha) = \intop (y - Af(t, \alpha))^2 p(y|x) \space dy \space dx 实际上可以使用与回归分析问题中类似的解法对泛函进行分解。 # 5. 密度估计问题（Fisher-Wald 表达）【挖个坑】 设 $\{p(x, \alpha), \alpha \in \Lambda \}$ 为概率密度集，它包含所要求的密度： p(x, \alpha^*) = \frac{d F(x)}{d x} 泛函： R(\alpha) = - \intop \ln p(x, \alpha) \space d F(x) \tag{E1.10} 损失函数： L(p(x, \alpha)) = - \log p(x, \alpha)]]></content>
      <categories>
        <category>Learning Theory</category>
      </categories>
      <tags>
        <tag>Empirical Risk</tag>
        <tag>模式识别</tag>
        <tag>回归估计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习问题表示]]></title>
    <url>%2F2018%2F12%2F24%2F%E5%AD%A6%E4%B9%A0%E9%97%AE%E9%A2%98%E8%A1%A8%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[摘要描述机器学习需要解决的学习问题的模型。 学习问题 学习问题的一般模型。 1. 学习模型 学习模型包含三部分组成： 1. 数据生成器（G） 2. 目标函数（S） 3. 学习机器（LM） 数据生成器是源头，它依据某一未知但固定的概率分布函数 $F(x)$ 独立同分布的产生数据 $x \in X$。 数据生成器生成的数据输入到目标函数中，目标函数根据条件分布 $F(y|x)$ 返回输出值 $y$。将输入 $\bf{x}$ 转换为 $y$ 的这个目标函数是未知但固定的。 学习机器从数据生成器和目标函数中观测到 $l$ 个样本： (x_1,y_1), \ldots, (x_l, y_l) 这些样本是依据联合分布函数 $F(x,y) = F(x)F(y|x)$ 获取到的。然后学习机器构造一个函数用于预测由生成器产生的某个特定的 $x_i$ 对应由目标函数所生成的 $y_i$。学习机器的目标是构造一个目标函数 $S$ 适当的逼近。 学习机器追求以下两个目标之一： 1. 模仿目标函数：试图构造一个函数，对一个给定的生成器 G，该函数可以对目标函数输出提供最佳的预测效果； 2. 辨识目标函数：试图构造一个非常接近于目标函数的函数； 2. 解析 比如说我们现在经营着一个酒庄，我从一片果园中采了一些葡萄，然后经过一系列酿造工艺，生产出了不同品质的红酒。 从果园中采摘葡萄的过程就相当于生成器生成 $x$，由于整个果园中不同品质的葡萄分布肯定是有一定的分布特点的，我们假设这个分布是 $F(x)$，而我每次采摘一颗葡萄是没有任何挑选的，并且前后葡萄的采摘过程是没有影响的，这样我每次采摘葡萄就是根据分布函数 $F(x)$ 进行的独立同分布采集。 将采集到的葡萄送到酿造车间中进行生产酿造酒相当于目标函数产生 $y$。葡萄酿造出来的红酒可能会有不同的品质，即使是完全相同的两个葡萄，经过一系列的酿造过程产生的酒的品质也可能是不同的，那么就相当于酿造的过程本身也包含某种分布规律，并且酿酒的过程提供的葡萄的不同，对酒的品质是有很大影响的，因此这个酒的品质应该是一个依赖于输入葡萄的条件分布 $F(y|x)$。 我们酒庄的酒卖的很好，现在有人想要研究破解我们的酿造工艺，他们获取到了我们每个用于酿造红酒的葡萄的信息以及对应的最终产生的酒的品质，也就是： (x_1,y_1), \ldots, (x_l, y_l) 这每一组数据的概率就是葡萄的分布 $F(x)$ 和红酒品质的分布 $F(y|x)$ 共同产生的联合概率 $F(x,y) = F(x)F(y|x)$。 却始终无法获取到我们的酿造工艺，因此，他们打算模仿我们的酿造工艺进行生产，根据用于酿造的葡萄的情况和红酒的品质来矫正他们的酿造工艺。]]></content>
      <categories>
        <category>Learning Theory</category>
      </categories>
      <tags>
        <tag>Learning Theory</tag>
        <tag>Machine Learning</tag>
        <tag>学习问题</tag>
        <tag>数据生成器</tag>
        <tag>目标函数</tag>
        <tag>学习机器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习原理]]></title>
    <url>%2F2018%2F12%2F21%2F%E5%AD%A6%E4%B9%A0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[文档说明文档整体结构采用摘要+主题的形式编写。由于要阐述一个问题以及其解决方式会有很深的问题，一层套一层，导致文章结构不好组织，结构树太深。因此将抽象描述和具体内容描述进行剥离。 文档第一部分进行抽象的整体描述，说明问题是什么，解决思路是什么，解决过程中会碰到什么新的问题，又是如何解决的。问题的抽象是如何的，问题的推广是如何的等等这一系列内容，并且整体以良好的组织结构描述。 文档第二部分按照主题进行具体阐述，主题的规模根据具体主题内容进行调整。 正文 是指依据经验数据选取所期望的依赖关系的问题。这里我们介绍两种解决学习问题的方法。 第一种方法是基于所选函数的品质可以用风险泛函来评估的思路。在这种思路下从给定函数集中选取逼近函数就是基于经验数据最小化风险泛函的问题。 第二种方法是基于估计所期望的随机依赖关系。在仅已知方程某些成分的近似值的情况下，这种方法需要解积分方程。但解积分方程得到的函数能够提供比这些问题本身所需的更多的细节。为这些细节需要解不适定问题。]]></content>
      <categories>
        <category>Learning Theory</category>
      </categories>
      <tags>
        <tag>Learning Theory</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 机器学习类库 Scikit-Learn Tutorial]]></title>
    <url>%2F2018%2F12%2F01%2FPython-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B1%BB%E5%BA%93-Scikit-Learn-Tutorial%2F</url>
    <content type="text"><![CDATA[摘要Scikit-Learn 是机器学习领域中非常著名的一个类库，其中封装了很多机器学习中需要使用到的算法工具。Scikit-Learn 构建在 NumPy 和 SciPy 等常见的数据科学类库之上。其核心算法使用更底层的语言实现，通过 Python 进行调用。 1. 基本介绍1.1. 环境导入相关类库： 12345678910111213141516171819202122232425import sysprint("Python version: &#123;&#125;". format(sys.version))import pandas as pdprint("pandas version: &#123;&#125;". format(pd.__version__))import matplotlibprint("matplotlib version: &#123;&#125;". format(matplotlib.__version__))import numpy as npprint("NumPy version: &#123;&#125;". format(np.__version__))import scipy as spprint("SciPy version: &#123;&#125;". format(sp.__version__)) import sklearnprint("scikit-learn version: &#123;&#125;". format(sklearn.__version__))#显示所有列pd.set_option('display.max_columns', None)#显示所有行pd.set_option('display.max_rows', None)#设置value的显示长度为100，默认为50pd.set_option('max_colwidth',100) 1.2. 算法Scikit-Learn 包含机器学习领域中比较主流的算法，包括： Supervised learning Linear model (Ridge, Lasso, Elastic Net, …) Support Vector Machine Tree-bassed methods (Random Forests, Bagging, GBRT, …) Nearest neighbors Neural networks (basics) Gaussian Processes Feature selection Unsupervised learning Clustering (KMeans, Ward, …) Matrix decomposition (PCA, ICA, …) Density estimation Outlier detection Model selection and evaluation Cross-validation Grid-search Lost of metrics 等等 1.3. 数据集1.3.1. 鸢尾花数据右加州大学开放的用于机器学习测试的清洗好的数据集，鸢尾花数据集，包含三种不同类型的鸢尾花，特征包含鸢尾花花瓣和花萼的长度和宽度。 导入数据： 123dataset = pd.read_csv('./input/Iris.csv')print(type(dataset)) 打印结果： 1pandas.core.frame.DataFrame 数据是 DataFrame 类型，然后查看一下数据内容： 1print(dataset.head()) 打印结果： df_index Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species 0 1 5.1 3.5 1.4 0.2 Iris-setosa 1 2 4.9 3.0 1.4 0.2 Iris-setosa 2 3 4.7 3.2 1.3 0.2 Iris-setosa 3 4 4.6 3.1 1.5 0.2 Iris-setosa 4 5 5.0 3.6 1.4 0.2 Iris-setosa 拆分训练集和测试集，加载的数据集的特征和标签是在一起的，但是在使用算法训练模型时，需要将特征和标签拆分开来，同时还需要拆分训练集和测试集。 12X = dataset.iloc[:, :-1].valuesy = dataset.iloc[:, -1].values 首先将特征和输入拆分，这里 [:, :-1] 中的第一个冒号表示的是所截取的行，开始到结束都是空的，也就是截取所有，而第 :-1 表示要截取的列，是从开始到最后第二列，也就是特征了。对应的 y 截取的是最后一列，也就是标签了。 然后使用 sklearn 中的方法来拆分训练集和测试集。 12from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) 这里 test_size 是 0.2 表示训练集和测试集是 8:2，random_state 表示随机数的种子，每次拆分如果使用相同的种子，那么就会得到两组相同的随机数。种子还可以设置为 None，那么每次得到的随机数种子也是随机的。 1.3.2. 构造数据集这里使用 make_blobs 来生成测试数据。这是一个聚类数据生成器。这个方法会生成一个包含若干个中心点的分类数据、中心点个数、特征数量、样本条数等都是可以指定的。 1234567from sklearn.datasets import make_blobsX, y = make_blobs(n_samples=1000, centers=20, random_state=123)labels = ["b", "r"]y = np.take(labels, (y &lt; 10))print(pd.DataFrame(X).head())print(pd.DataFrame(y).head()) 打印的特征： df_index 0 1 0 -6.452556 -8.763583 1 0.289821 0.146772 2 -5.184123 -1.253470 3 -4.713888 3.674405 4 4.515583 -2.881380 打印的标签： df_index 0 0 r 1 r 2 b 3 r 4 b 绘制散点图看看： 12345678910from matplotlib import pyplot as pltplt.rcParams["figure.figsize"] = (10, 8)plt.rcParams["figure.max_open_warning"] = -1plt.figure()for label in labels: mask = (y == label) plt.scatter(X[mask, 0], X[mask, 1], c=label)plt.xlim(-10, 10)plt.ylim(-10, 10)plt.show() 效果如下： 1.4. 接口Scikit-Learn 中所有的学习算法采用类似的 API 设计，每个学习算法都包含以下一些接口： 一个用于构建和拟合模型的接口 一个用于进行预测的接口 一个用于转换数据的接口 2. K-Nearest Neighbours在机器学习中，KNN 算法是一种无参的分类和回归算法。在 KNN 分类中采用多数投票制，邻居进行投票，占多数的那个即为这个对象的分类。如果是回归，那么结果就是其最近的几个邻居的平均值。 下面使用 KNN 对鸢尾花数据进行模型训练和评估： 12345678from sklearn.neighbors import KNeighborsClassifierModel = KNeighborsClassifier(n_neighbors=8)Model.fit(X_train, y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 这里使用 KNeighborsClassifier 进行分类，设置近邻数量为 n_neighbors=8。然后用 fit() 方法训练模型，使用 predict() 方法预测结果。 然后使用 accuracy_score() 函数计算评分。评分结果： 1accuracy is 1.0 除了准确率，还可以使用混淆矩阵观察预测效果： 12from sklearn.metrics import confusion_matrixprint(confusion_matrix(y_test, y_pred)) 打印结果： 123[[11 0 0] [ 0 13 0] [ 0 0 6]] 此外，还可以查看准确率、召回率等指标： 12from sklearn.metrics import classification_reportprint(classification_report(y_test, y_pred)) 打印结果： - precision recall f1-score support Iris-setosa 1.00 1.00 1.00 11 Iris-versicolor 1.00 1.00 1.00 13 Iris-virginica 1.00 1.00 1.00 6 micro avg 1.00 1.00 1.00 30 macro avg 1.00 1.00 1.00 30 weighted avg 1.00 1.00 1.00 30 3. Radius Neighbors Classifier有限半径的近邻分类。在 sklean 中 RadiusNeighborsClassifier 与 KNeighborsClassifier 非常相似，除了两个参数。第一个参数是 RadiusNeighborsClassifier 需要指定固定半径，并使用在这个半径区域内的近邻进行分类。第二个参数用来表明在指定半径区域内，哪一个标签是不应该出现的，如果该区域内出现这个标签对应的输入，那么这个输入就是异常值。 12345678from sklearn.neighbors import RadiusNeighborsClassifierModel=RadiusNeighborsClassifier(radius=8.0)Model.fit(X_train,y_train)y_pred=Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is ', accuracy_score(y_test,y_pred)) 评分结果： 1accuracy is 1.0 4. Logistic RegressionLogistic Regression 也是一种回归分析，并且其因变量是只有两种可能性。Logistic Regression 是一种使用非常广泛的分类算法，很多复杂的方法都基于这个方法。 12345678from sklearn.linear_model import LogisticRegressionModel = LogisticRegression(solver='liblinear', multi_class='ovr')Model.fit(X_train, y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 0.933333333333 5. Passive Aggressive ClassifierPassive Aggressive Classifier 是一种增量学习算法，当训练数据比较大时，可以一次只传入部分训练数据进行训练，不同的迭代传入训练数据。使用 partial_fit() 方法进行增量训练，不过这里实例只是简单用法。 12345678from sklearn.linear_model import PassiveAggressiveClassifierModel = PassiveAggressiveClassifier()Model.fit(X_train, y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 0.7 6. Naive Bayes在机器学习中，Naive Bayes 是一种基于贝叶斯定律的分类器，它假设特征之间都是相互独立的。 12345678from sklearn.naive_bayes import GaussianNBModel = GaussianNB()Model.fit(X_train, y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 1.0 7. BernoulliNB适合二分类问题的分类器。 12345678from sklearn.naive_bayes import BernoulliNBModel = BernoulliNB()Model.fit(X_train, y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 0.2 8. SVM支持向量机，一种适合处理高维度数据的算法。在数据的特征数量比数据的样本数据量还大的时候仍然有效。它可以使用样本集中的一部分进行计算，也就是可以完全在内存中计算。此外，用户还可以自定义核函数。 123456789from sklearn.svm import SVCModel = SVC()Model.fit(X_train, y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 1.0 9. Nu-Support Vector Classification类似于 SVM，但是可以使用一个参数指定参与计算的支持向量的数量。 123456789from sklearn.svm import NuSVCModel = NuSVC()Model.fit(X_train, y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 1.0 10. Linear Support Vector Classification指定线性核的 SVM 分类器。 123456789from sklearn.svm import LinearSVCModel = LinearSVC()Model.fit(X_train, y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 0.8 11. Decision Tree决策树是一种无参的监督学习方法，可以用来解决分类或回归问题。 123456789from sklearn.tree import DecisionTreeClassifierModel = DecisionTreeClassifier()Model.fit(X_train, y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 0.966666666667 12. ExtraTreeClassifier一种随机树分类器。其与决策树的主要区别在于树的构建过程。 123456789from sklearn.tree import ExtraTreeClassifierModel = ExtraTreeClassifier()Model.fit(X_train, y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 1.0 13. Neural network多层感知机。 123456789from sklearn.neural_network import MLPClassifierModel=MLPClassifier()Model.fit(X_train,y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 0.933333333333 14. RandomForest随机森林通过基于不同的训练样本子集来训练出许多决策树分类器，然后使用这些决策树的平均值来进行预测。 123456789from sklearn.ensemble import RandomForestClassifierModel=RandomForestClassifier(max_depth=2)Model.fit(X_train,y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 1.0 15. Bagging classifierBagging classifier 通过随机子集训练处众多分类器，然后将所有分类器的结果进行聚合，得到最终的预测结果。 12345678from sklearn.ensemble import BaggingClassifierModel=BaggingClassifier()Model.fit(X_train,y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 0.966666666667 16. AdaBoost classifierAdaBoost classifier 首先在原始的数据集上拟合一个分类器，然后利用第一个分类器分类的结果训练第二个分类器，并且调整其中错误分类的权重，这样子分类器就会关注更难以分类的那部分数据集。 12345678from sklearn.ensemble import AdaBoostClassifierModel=AdaBoostClassifier()Model.fit(X_train,y_train)y_pred = Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 0.966666666667 17. Gradient Boosting Classifier1234567from sklearn.ensemble import GradientBoostingClassifierModel=GradientBoostingClassifier()Model.fit(X_train,y_train)y_pred=Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 0.966666666667 18. Linear Discriminant AnalysisLinear Discriminant Analysis 和 Quadratic Discriminant Analysis 是两种经典的分类方法，如两个方法的名字所示，一个是使用一条直线进行分类，一个是使用一个二次决策曲面进行分类。 1234567from sklearn.discriminant_analysis import LinearDiscriminantAnalysisModel=LinearDiscriminantAnalysis()Model.fit(X_train,y_train)y_pred=Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 1.0 19. Quadratic Discriminant Analysis1234567from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysisModel=QuadraticDiscriminantAnalysis()Model.fit(X_train,y_train)y_pred=Model.predict(X_test)from sklearn.metrics import accuracy_score print('accuracy is',accuracy_score(y_pred,y_test)) 评分结果： 1accuracy is 1.0 20. KmeansKmeans 是一种无监督学习，用来处理未分类数据。其目标是利用算法从数据中找出分组，然后将数据划分到各个分组中，分组的数量是不一定的。 1234567891011121314151617181920from sklearn.cluster import KMeansiris_SP = dataset[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] from scipy.spatial.distance import cdistclusters=range(1,15)meandist=[]for k in clusters: model=KMeans(n_clusters=k) model.fit(iris_SP) clusassign=model.predict(iris_SP) meandist.append(sum(np.min(cdist(iris_SP, model.cluster_centers_, 'euclidean'), axis=1)) / iris_SP.shape[0])import matplotlib.pyplot as pltplt.plot(clusters, meandist)plt.xlabel('Number of clusters')plt.ylabel('Average distance')plt.title('Selecting k with the Elbow Method')plt.show() 设置分组的数量从 1 - 15，根据分组数量以及分组之间的距离判断应该设置的合适的分组数量。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Python</tag>
        <tag>KNeighborsClassifier</tag>
        <tag>RadiusNeighborsClassifier</tag>
        <tag>LogisticRegression</tag>
        <tag>PassiveAggressiveClassifier</tag>
        <tag>GaussianNB</tag>
        <tag>BernoulliNB</tag>
        <tag>SVC</tag>
        <tag>NuSVC</tag>
        <tag>LinearSVC</tag>
        <tag>DecisionTreeClassifier</tag>
        <tag>ExtraTreeClassifier</tag>
        <tag>MLPClassifier</tag>
        <tag>RandomForestClassifier</tag>
        <tag>BaggingClassifier</tag>
        <tag>AdaBoostClassifier</tag>
        <tag>GradientBoostingClassifier</tag>
        <tag>LinearDiscriminantAnalysis</tag>
        <tag>QuadraticDiscriminantAnalysis</tag>
        <tag>KMeans</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[交互式数据可视化 Plotly Tutorial]]></title>
    <url>%2F2018%2F11%2F30%2F%E4%BA%A4%E4%BA%92%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-Plotly-Tutorial%2F</url>
    <content type="text"><![CDATA[摘要Plotly 类库提供了一个可交互的，出版级别的在线图形库。Plotly 绘制的图形是以 HTML 页面的形式提供的，基于 JavaScript 提供交互功能。 下面提供了一些图形的示例，包括折线图、散点图、区域图、柱状图、箱线图、直方图等。 1. 数据准备数据来源：https://www.kaggle.com/mylesoneill/world-university-rankings 加载数据： 1234import pandas as pdtimesData = pd.read_csv("./input/timesData.csv")print(timesData.info()) 打印结果： 1234567891011121314151617181920&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 2603 entries, 0 to 2602Data columns (total 14 columns):world_rank 2603 non-null objectuniversity_name 2603 non-null objectcountry 2603 non-null objectteaching 2603 non-null float64international 2603 non-null objectresearch 2603 non-null float64citations 2603 non-null float64income 2603 non-null objecttotal_score 2603 non-null objectnum_students 2544 non-null objectstudent_staff_ratio 2544 non-null float64international_students 2536 non-null objectfemale_male_ratio 2370 non-null objectyear 2603 non-null int64dtypes: float64(4), int64(1), object(9)memory usage: 284.8+ KBNone 查看数据： 1print(timesData.head(10)) 打印结果： df_num world_rank university_name country teaching international research citations income total_score num_students student_staff_ratio international_students female_male_ratio year 0 1 Harvard University United States of America 99.7 72.4 98.7 98.8 34.5 96.1 20,152 8.9 25% - 2011 1 2 California Institute of Technology United States of America 97.7 54.6 98 99.9 83.7 96 2,243 6.9 27% 33 : 67 2011 2 3 Massachusetts Institute of Technology United States of America 97.8 82.3 91.4 99.9 87.5 95.6 11,074 9 33% 37 : 63 2011 3 4 Stanford University United States of America 98.3 29.5 98.1 99.2 64.3 94.3 15,596 7.8 22% 42:58:00 2011 4 5 Princeton University United States of America 90.9 70.3 95.4 99.9 - 94.2 7,929 8.4 27% 45:55:00 2011 5 6 University of Cambridge United Kingdom 90.5 77.7 94.1 94 57 91.2 18,812 11.8 34% 46:54:00 2011 6 6 University of Oxford United Kingdom 88.2 77.2 93.9 95.1 73.5 91.2 19,919 11.6 34% 46:54:00 2011 7 8 University of California, Berkeley United States of America 84.2 39.6 99.3 97.8 - 91.1 36,186 16.4 15% 50:50:00 2011 8 9 Imperial College London United Kingdom 89.2 90 94.5 88.3 92.9 90.6 15,060 11.7 51% 37 : 63 2011 9 10 Yale University United States of America 92.1 59.2 89.7 91.5 - 89.5 11,751 4.4 20% 50:50:00 2011 2. 折线图 Line Charts2.1. 过程说明 导入数据 创建 trace x = x 轴数据 y = y 轴数据 mode = 绘制标记的类型 name = 图例名称 marker = 标记的样式 color = 线条的颜色，使用 RGB 定义 text = 坐标的名字 data = 一个列表，表示要绘制的数据 layout = 一个字典，表示布局信息 title = 图标的标题 x axis = 表示 x 轴的样式信息 title = x 轴的标签 ticklen = x 轴坐标轴上竖线的长度 zeroline = 布尔值，是否显示0坐标位置的线条，也就是 y 轴 fig = 一个包含数据和布局信息的字典 plot = 绘制图形，这里采用本地离线方式绘制 2.2. 示例1234567891011121314151617181920212223242526import plotly as pltimport plotly.offline as pltofdf = timesData.iloc[:100,:]trace1 = plt.graph_objs.Scatter( x = df.world_rank, y = df.citations, mode = "lines", name = "citations", marker = dict(color = 'rgba(16, 112, 2, 0.8)'), text= df.university_name)trace2 = plt.graph_objs.Scatter( x = df.world_rank, y = df.teaching, mode = "lines+markers", name = "teaching", marker = dict(color = 'rgba(80, 26, 80, 0.8)'), text= df.university_name)data = [trace1, trace2]layout = dict(title = 'Citation and Teaching vs World Rank of Top 100 Universities', xaxis= dict(title= 'World Rank',ticklen= 5,zeroline= False) )fig = dict(data = data, layout = layout)pltof.plot(fig) 执行成功之后，会在浏览器中打开一个页面： 当鼠标在图形上移动时，会有实时的反馈数据出来： 3. 散点图 Scatter1234567891011121314151617181920212223242526272829303132333435363738import plotly as pltimport plotly.offline as pltofdf2014 = timesData[timesData.year == 2014].iloc[:100,:]df2015 = timesData[timesData.year == 2015].iloc[:100,:]df2016 = timesData[timesData.year == 2016].iloc[:100,:]trace1 =plt.graph_objs.Scatter( x = df2014.world_rank, y = df2014.citations, mode = "markers", name = "2014", marker = dict(color = 'rgba(255, 128, 255, 0.8)'), text= df2014.university_name)trace2 =plt.graph_objs.Scatter( x = df2015.world_rank, y = df2015.citations, mode = "markers", name = "2015", marker = dict(color = 'rgba(255, 128, 2, 0.8)'), text= df2015.university_name)trace3 =plt.graph_objs.Scatter( x = df2016.world_rank, y = df2016.citations, mode = "markers", name = "2016", marker = dict(color = 'rgba(0, 255, 200, 0.8)'), text= df2016.university_name)data = [trace1, trace2, trace3]layout = dict(title = 'Citation vs world rank of top 100 universities with 2014, 2015 and 2016 years', xaxis= dict(title= 'World Rank',ticklen= 5,zeroline= False), yaxis= dict(title= 'Citation',ticklen= 5,zeroline= False) )fig = dict(data = data, layout = layout)pltof.plot(fig) 效果图： 4. 柱状图12345678910111213141516171819202122232425import plotly.graph_objs as goimport plotly.offline as pltofdf2014 = timesData[timesData.year == 2014].iloc[:3,:]trace1 = go.Bar( x = df2014.university_name, y = df2014.citations, name = "citations", marker = dict(color = 'rgba(255, 174, 255, 0.5)', line=dict(color='rgb(0,0,0)',width=1.5)), text = df2014.country)trace2 = go.Bar( x = df2014.university_name, y = df2014.teaching, name = "teaching", marker = dict(color = 'rgba(255, 255, 128, 0.5)', line=dict(color='rgb(0,0,0)',width=1.5)), text = df2014.country)data = [trace1, trace2]layout = go.Layout(barmode = "group")fig = go.Figure(data = data, layout = layout)pltof.plot(fig) 效果图： 5. 堆叠柱状图12345678910111213141516171819202122232425262728import plotly.graph_objs as goimport plotly.offline as pltofdf2014 = timesData[timesData.year == 2014].iloc[:3,:]x = df2014.university_nametrace1 = &#123; 'x': x, 'y': df2014.citations, 'name': 'citation', 'type': 'bar'&#125;;trace2 = &#123; 'x': x, 'y': df2014.teaching, 'name': 'teaching', 'type': 'bar'&#125;;data = [trace1, trace2];layout = &#123; 'xaxis': &#123;'title': 'Top 3 universities'&#125;, 'barmode': 'relative', 'title': 'citations and teaching of top 3 universities in 2014'&#125;;fig = go.Figure(data = data, layout = layout)pltof.plot(fig) 效果图： 6. 带折线图的柱状图1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import numpy as npimport plotly.graph_objs as gofrom plotly import toolsimport matplotlib.pyplot as pltimport plotly.offline as pltofdf2016 = timesData[timesData.year == 2016].iloc[:7,:]y_saving = [each for each in df2016.research]y_net_worth = [float(each) for each in df2016.income]x_saving = [each for each in df2016.university_name]x_net_worth = [each for each in df2016.university_name]trace0 = go.Bar( x=y_saving, y=x_saving, marker=dict(color='rgba(171, 50, 96, 0.6)',line=dict(color='rgba(171, 50, 96, 1.0)',width=1)), name='research', orientation='h',)trace1 = go.Scatter( x=y_net_worth, y=x_net_worth, mode='lines+markers', line=dict(color='rgb(63, 72, 204)'), name='income',)layout = dict( title='Citations and income', yaxis=dict(showticklabels=True,domain=[0, 0.85]), yaxis2=dict(showline=True,showticklabels=False,linecolor='rgba(102, 102, 102, 0.8)',linewidth=2,domain=[0, 0.85]), xaxis=dict(zeroline=False,showline=False,showticklabels=True,showgrid=True,domain=[0, 0.42]), xaxis2=dict(zeroline=False,showline=False,showticklabels=True,showgrid=True,domain=[0.47, 1],side='top',dtick=25), legend=dict(x=0.029,y=1.038,font=dict(size=10) ), margin=dict(l=200, r=20,t=70,b=70), paper_bgcolor='rgb(248, 248, 255)', plot_bgcolor='rgb(248, 248, 255)',)annotations = []y_s = np.round(y_saving, decimals=2)y_nw = np.rint(y_net_worth)for ydn, yd, xd in zip(y_nw, y_s, x_saving): annotations.append(dict(xref='x2', yref='y2', y=xd, x=ydn - 4,text='&#123;:,&#125;'.format(ydn),font=dict(family='Arial', size=12,color='rgb(63, 72, 204)'),showarrow=False)) annotations.append(dict(xref='x1', yref='y1', y=xd, x=yd + 3,text=str(yd),font=dict(family='Arial', size=12,color='rgb(171, 50, 96)'),showarrow=False))layout['annotations'] = annotationsfig = tools.make_subplots(rows=1, cols=2, specs=[[&#123;&#125;, &#123;&#125;]], shared_xaxes=True, shared_yaxes=False, vertical_spacing=0.001)fig.append_trace(trace0, 1, 1)fig.append_trace(trace1, 1, 2)fig['layout'].update(layout)pltof.plot(fig) 效果图： 7. 饼状图123456789101112131415161718192021222324252627282930313233343536import numpy as npimport plotly.graph_objs as gofrom plotly import toolsimport matplotlib.pyplot as pltimport plotly.offline as pltofdf2016 = timesData[timesData.year == 2016].iloc[:7,:]pie1 = df2016.num_studentspie1_list = [float(each.replace(',', '.')) for each in df2016.num_students]labels = df2016.university_namefig = &#123; "data": [ &#123; "values": pie1_list, "labels": labels, "domain": &#123;"x": [0, .5]&#125;, "name": "Number Of Students Rates", "hoverinfo":"label+percent+name", "hole": .3, "type": "pie" &#125;,], "layout": &#123; "title":"Universities Number of Students rates", "annotations": [ &#123; "font": &#123; "size": 20&#125;, "showarrow": False, "text": "Number of Students", "x": 0.20, "y": 1 &#125;, ] &#125;&#125;pltof.plot(fig) 效果： 8. 气泡图 Bubble Charts123456789101112131415161718import plotly.offline as pltofdf2016 = timesData[timesData.year == 2016].iloc[:20,:]num_students_size = [float(each.replace(',', '.')) for each in df2016.num_students]international_color = [float(each) for each in df2016.international]data = [&#123; 'y': df2016.teaching, 'x': df2016.world_rank, 'mode': 'markers', 'marker': &#123; 'color': international_color, 'size': num_students_size, 'showscale': True &#125;, "text" : df2016.university_name &#125;]pltof.plot(data) 效果图： 9. 直方图12345678910111213141516171819202122232425import plotly.graph_objs as goimport plotly.offline as pltofx2011 = timesData.student_staff_ratio[timesData.year == 2011]x2012 = timesData.student_staff_ratio[timesData.year == 2012]trace1 = go.Histogram( x=x2011, opacity=0.75, name = "2011", marker=dict(color='rgba(171, 50, 96, 0.6)'))trace2 = go.Histogram( x=x2012, opacity=0.75, name = "2012", marker=dict(color='rgba(12, 50, 196, 0.6)'))data = [trace1, trace2]layout = go.Layout(barmode='overlay', title=' students-staff ratio in 2011 and 2012', xaxis=dict(title='students-staff ratio'), yaxis=dict( title='Count'),)fig = go.Figure(data=data, layout=layout)pltof.plot(data) 效果图： 10. 词云 Word Cloud1234567891011121314151617181920212223import pandas as pdfrom wordcloud import WordCloudtimesData = pd.read_csv("./input/timesData.csv")timesData.info()res = timesData.head(10)res.to_csv("./temp.csv")import matplotlib.pyplot as pltx2011 = timesData.country[timesData.year == 2011]plt.subplots(figsize=(8,8))wordcloud = WordCloud( background_color='white', width=512, height=384 ).generate(" ".join(x2011))plt.imshow(wordcloud)plt.axis('off')plt.show() 效果图： 11. 箱线图123456789101112131415161718192021222324import pandas as pdimport plotly.graph_objs as goimport plotly.offline as pltoftimesData = pd.read_csv("./input/timesData.csv")x2015 = timesData[timesData.year == 2015]trace0 = go.Box( y=x2015.total_score, name = 'total score of universities in 2015', marker = dict( color = 'rgb(12, 12, 140)', ))trace1 = go.Box( y=x2015.research, name = 'research of universities in 2015', marker = dict( color = 'rgb(12, 128, 128)', ))data = [trace0, trace1]pltof.plot(data) 效果图： 12. 散点矩阵图123456789101112131415161718import numpy as npimport pandas as pdimport plotly.graph_objs as goimport plotly.offline as pltoftimesData = pd.read_csv("./input/timesData.csv")import plotly.figure_factory as ffdataframe = timesData[timesData.year == 2015]data2015 = dataframe.loc[:,["research","international", "total_score"]]data2015["index"] = np.arange(1,len(data2015)+1)fig = ff.create_scatterplotmatrix(data2015, diag='box', index='index',colormap='Portland', colormap_type='cat', height=1100, width=1100)pltof.plot(fig) 效果图： 13. 3D 散点图12345678910111213141516171819202122232425262728293031import numpy as npimport pandas as pdimport plotly.graph_objs as goimport plotly.offline as pltoftimesData = pd.read_csv("./input/timesData.csv")dataframe = timesData[timesData.year == 2015]trace1 = go.Scatter3d( x=dataframe.world_rank, y=dataframe.research, z=dataframe.citations, mode='markers', marker=dict( size=10, color='rgb(255,0,0)' ))data = [trace1]layout = go.Layout( margin=dict( l=0, r=0, b=0, t=0 ) )fig = go.Figure(data=data, layout=layout)pltof.plot(fig) 效果图： 查看具体点的信息：]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
        <tag>Data Visualization</tag>
        <tag>3D</tag>
        <tag>Line Charts</tag>
        <tag>折线图</tag>
        <tag>Historgrams</tag>
        <tag>直方图</tag>
        <tag>Bar Charts</tag>
        <tag>柱状图</tag>
        <tag>Pie Charts</tag>
        <tag>饼状图</tag>
        <tag>3D Scatter Plots</tag>
        <tag>3D 散点图</tag>
        <tag>Python</tag>
        <tag>Plotly</tag>
        <tag>交互式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据可视化 Matplotlib Tutorial]]></title>
    <url>%2F2018%2F11%2F30%2F%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96-Matplotlib-Tutorial%2F</url>
    <content type="text"><![CDATA[摘要Matplotlib 是一个 Python 的可视化类库，用于开发二维和三维图表。最近几年，它被广泛的应用于科学和工程领域。 1. Matplotlib 架构Matplotlib 架构从逻辑上分为三层，位于三个不同的层级上。上层内容可以与下层内容进行通讯，但下层内容不可以与上层内容通讯。 三层从上至下依次是： Scripting Artist Backend 1.1. Backend 层位于架构的最底层。这一层包含了 matplotlib 的 API，也就是若干类的集合，这些类用来在底层实现各种图形元素。主要包括： FigureCanves —— 用来表示图形的绘制区域 Renderer —— 用来在 FigureCanves 上绘制内容的对象 Event —— 用来处理用户输入（键盘、鼠标输入）的对象 1.2. Artist 层整个架构的中间层，所有用来构成一个图像的各种元素都在这里，比如标题、坐标轴、坐标轴标签、标记等内容都是 Artist 的实例。并且这些元素构成了一个层级结构。 Artist 分为两类，一类是 primitive artist，一类是 composite artist： Primitive artist 是包含基本图形的独立元素，例如一个矩形、一个圆或者是一个文本标签 Composite artist 是这些简单元素的组合，比如横坐标、纵坐标、图形等 在处理这一层内容的时候，主要打交道的都是上层的图形结构，需要理解其中每个对象在图形中所代表的含义。下图表示了一个 Artist 对象的结构。 1.3. Scripting 层该层包含了一个 pyplot 接口。这个包提供了经典的 Python 接口，基于编程的方式操作 matplotlib，它有自己的命名空间，需要导入 NumPy包。 2. 绘制图形2.1. 简单示例123import matplotlib.pyplot as pltprint(plt.plot([1, 2, 3, 4])) 打印输出： 1[&lt;matplotlib.lines.Line2D object at 0x000000000D55EB70&gt;] 这里创建了一个 Line2D 对象。这个对象是一条符合给定点趋势的直线。接下来我们可以使用一个命令来将这个图片显示出来： 1plt.show() 显示结果： 这个显示出来的窗口被称为 plotting window，这个窗口上有一些工具栏可以用来修改图形。 这里我们只给 plot() 方法传入了一个数字列表或数字数组，那么这个数组表示的是 y 坐标的点，对应每个点默认的 x 坐标的值是 0, 1, 2, 3, ... 。 如果要正确的绘制图形，那么应该给定明确的横坐标和纵坐标，也就是需要给定两个数组，第一个表示 x 轴，第二个表示 y 轴。此外，plot() 方法还可以接受第三个参数，用于表示绘制图形的属性。 2.2. 设置样式1234import matplotlib.pyplot as pltplt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')plt.show() 显示结果： 设置坐标样式 123456import matplotlib.pyplot as pltplt.axis([0, 5, 0, 20]) # [xmin, xmax, ymin, ymax]plt.title("My first plot")plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')plt.show() 显示结果： 2.3. 在一个图上绘制多条线这里使用 sin() 函数来绘制三角函数的图形。 12345678910111213import mathimport numpy as npimport matplotlib.pyplot as pltt = np.arange(0, 2.5, 0.1)y1 = np.sin(math.pi * t)y2 = np.sin(math.pi * t + math.pi / 2)y3 = np.sin(math.pi * t - math.pi / 2)plt.axis([-0.5, 3, -1.5, 1.5])plt.plot(t, y1, 'b*', t, y2, 'g^', t, y3, 'ys')plt.show() 显示结果： 还可以修改线条的样式： 1plt.plot(t, y1, 'b--', t, y2, 'g', t, y3, 'r-.') 显示结果： 2.4. 使用关键字参数之前在使用 plot() 方法绘制线条时，传入了一些参数，该方法还提供了很多参数以供使用，具体可以参考： matplotlib.lines.Line2D 2.5. 在一个窗口绘制多个图形如果需要在一个窗口绘制多个图形，那么可以使用 subplot() 方法设置当前图形所位于整个窗口的位置。然后使用 plot() 方法绘制当前区域的图形即可。 12345678910111213141516import mathimport numpy as npimport matplotlib.pyplot as pltt = np.arange(0,5, 0.1)y1 = np.sin(2 * np.pi * t)y2 = np.sin(2 * np.pi * t)plt.subplot(211)plt.plot(t, y1, 'b-.')plt.subplot(212)plt.plot(t, y2, 'r--')plt.show() 显示结果： 2.6. 设置标签2.6.1. 设置坐标轴标签前面已经用过使用 title() 方法设置图形的标题，另外还有方法可以设置坐标的标签。 12345678import matplotlib.pyplot as pltplt.axis([0, 5, 0, 20]) # [xmin, xmax, ymin, ymax]plt.title("My first plot")plt.xlabel("Counting")plt.ylabel("Square values")plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')plt.show() 显示结果： 设置字体 123plt.title("My first plot", fontsize = 20, fontname = 'Times New Roman')plt.xlabel("Counting", color = 'gray')plt.ylabel("Square values", color = 'gray') 显示结果： 2.6.2. 设置数据点标签1234567891011121314import matplotlib.pyplot as pltplt.axis([0, 5, 0, 20]) # [xmin, xmax, ymin, ymax]plt.title("My first plot", fontsize = 20, fontname = 'Times New Roman')plt.xlabel("Counting", color = 'gray')plt.ylabel("Square values", color = 'gray')plt.text(1, 1.5, 'First')plt.text(2, 4.5, 'Second')plt.text(3, 9.5, 'Third')plt.text(4, 16.5, 'Fourth')plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')plt.show() 显示结果： 2.6.3. 添加 LaTex 表达式1plt.text(1.1, 12, r'$y=x^2$', fontsize=20,bbox=&#123;'facecolor':'yellow', 'alpha': 0.2&#125;) 显示结果： 2.6.4. 添加网格1plt.grid(linestyle='--') 显示结果： 2.6.5. 添加图例12345678910111213141516171819import matplotlib.pyplot as pltplt.axis([0, 5, 0, 20]) # [xmin, xmax, ymin, ymax]plt.title("My first plot", fontsize = 20, fontname = 'Times New Roman')plt.xlabel("Counting", color = 'gray')plt.ylabel("Square values", color = 'gray')plt.text(1, 1.5, 'First')plt.text(2, 4.5, 'Second')plt.text(3, 9.5, 'Third')plt.text(4, 16.5, 'Fourth')plt.text(1.1, 12, r'$y=x^2$', fontsize=20,bbox=&#123;'facecolor':'yellow', 'alpha': 0.2&#125;)plt.grid(linestyle='--')plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'ro')plt.legend(['First series'])plt.show() 显示结果： 注意，添加的图例中接受的是一个数组，数组中元素的顺序与 plot() 方法绘制的元素顺序对应。 3. 常用图表3.1. 折线图（Line Charts）折线图的思路是给定 x 一个细密度的坐标数组，给定 y 一个以 x 作为变量的表达式进行绘制。 1234567import numpy as npimport matplotlib.pyplot as pltx = np.arange(-2 * np.pi, 2 * np.pi, 0.01)y = np.sin(3 * x) / xplt.plot(x, y)plt.show() 显示结果： 3.1.1. 用 Pandas 绘制12345678910111213141516import numpy as npimport matplotlib.pyplot as pltimport pandas as pddata = &#123; 'series1':[1,3,4,3,5], 'series2':[2,4,5,2,4], 'series3':[3,2,3,1,3]&#125;df = pd.DataFrame(data)x = np.arange(5)plt.axis([0,5,0,7])plt.plot(x, df)plt.show() 显示结果： 3.2. 直方图（Histograms）使用 hist() 方法绘制直方图。 123pop = np.random.randint(0,100,100)n, bins, patches = plt.hist(pop, bins=20)plt.show() 显示结果： 3.3. 柱状图（Bar Charts）柱状图与直方图非常相似，但是注意，柱状图的横坐标是分类，而不是连续型的值。 12345index = [0,1,2,3,4]values = [5,7,3,4,6]plt.bar(index,values)plt.xticks(index,['A','B','C','D','E'])plt.show() 显示结果： 3.3.1. 标准差123456index = [0,1,2,3,4]values = [5,7,3,4,6]std = [0.8,1,0.4,0.9,1.3]plt.bar(index, values, yerr=std, error_kw=&#123;'ecolor':'0.1', 'capsize':6&#125;)plt.xticks(index, ['A','B','C','D','E'])plt.show() 显示结果： 3.3.2. 水平柱状图123456index = [0,1,2,3,4]values = [5,7,3,4,6]std = [0.8,1,0.4,0.9,1.3]plt.barh(index, values, xerr=std, error_kw=&#123;'ecolor':'0.1', 'capsize':6&#125;)plt.xticks(index, ['A','B','C','D','E'])plt.show() 显示结果： 3.3.3. 多列柱状图1234567891011index = np.array([1,2,3,4,5])values1 = [5,7,3,4,6]values2 = [6,6,4,5,7]values3 = [5,6,5,4,6]bw=0.3plt.axis([0, 6, 0, 8])plt.bar(index-bw, values1, bw, color='b')plt.bar(index, values2, bw, color='g')plt.bar(index+bw, values3, bw, color='r')plt.xticks(index, ['A','B','C','D','E'])plt.show() 显示结果： 3.3.4. 堆叠柱状图1234567891011series1 = np.array([3,4,5,3])series2 = np.array([1,2,2,5])series3 = np.array([2,3,3,4])index = np.arange(4)plt.axis([-0.5,3.5,0,15])plt.title('A Multiseries Stacked Bar Chart')plt.bar(index,series1,color='r')plt.bar(index,series2,color='b',bottom=series1)plt.bar(index,series3,color='g',bottom=(series2+series1))plt.xticks(index+0.4,['Jan18','Feb18','Mar18','Apr18'])plt.show() 显示结果： 3.4. 饼状图（Pie Charts）1234567labels = ['Nokia','Samsung','Apple','Lumia']values = [10,30,45,15]colors = ['yellow','green','red','blue']plt.pie(values,labels=labels,colors=colors)plt.axis('equal')plt.show() 显示结果： 4. 3D 图形4.1. 3D 曲面123456789101112131415import numpy as npfrom mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltfig = plt.figure()ax = Axes3D(fig)X = np.arange(-2,2,0.1)Y = np.arange(-2,2,0.1)X,Y = np.meshgrid(X,Y)def f(x,y): return (1 - y**5 + x**5)*np.exp(-x**2-y**2)ax.plot_surface(X,Y,f(X,Y), rstride=1, cstride=1)plt.show() 显示结果： 4.2. 3D 散点图12345678910111213141516171819202122import numpy as npfrom mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltxs = np.random.randint(30,40,100)ys = np.random.randint(20,30,100)zs = np.random.randint(10,20,100)xs2 = np.random.randint(50,60,100)ys2 = np.random.randint(30,40,100)zs2 = np.random.randint(50,70,100)xs3 = np.random.randint(10,30,100)ys3 = np.random.randint(40,50,100)zs3 = np.random.randint(40,50,100)fig = plt.figure()ax = Axes3D(fig)ax.scatter(xs,ys,zs)ax.scatter(xs2,ys2,zs2,c='r',marker='^')ax.scatter(xs3,ys3,zs3,c='g',marker='*')ax.set_xlabel('X Label')ax.set_ylabel('Y Label')ax.set_zlabel('Z Label')plt.show() 显示结果： 4.3. 3D 柱状图1234567891011121314151617181920212223import numpy as npfrom mpl_toolkits.mplot3d import Axes3Dimport matplotlib.pyplot as pltx = np.arange(8)y = np.random.randint(0,10,8)y2 = y + np.random.randint(0,3,8)y3 = y2 + np.random.randint(0,3,8)y4 = y3 + np.random.randint(0,3,8)y5 = y4 + np.random.randint(0,3,8)clr = ['#4bb2c5', '#c5b47f', '#EAA228', '#579575', '#839557', '#958c12', '#953579', '#4b5de4']fig = plt.figure()ax = Axes3D(fig)ax.bar(x,y,0,zdir='y',color=clr)ax.bar(x,y2,10,zdir='y',color=clr)ax.bar(x,y3,20,zdir='y',color=clr)ax.bar(x,y4,30,zdir='y',color=clr)ax.bar(x,y5,40,zdir='y',color=clr)ax.set_xlabel('X Axis')ax.set_ylabel('Y Axis')ax.set_zlabel('Z Axis')ax.view_init(elev=40)plt.show() 显示结果：]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
        <tag>Data Visualization</tag>
        <tag>Matplotlib</tag>
        <tag>3D</tag>
        <tag>Line Charts</tag>
        <tag>折线图</tag>
        <tag>Historgrams</tag>
        <tag>直方图</tag>
        <tag>Bar Charts</tag>
        <tag>柱状图</tag>
        <tag>Pie Charts</tag>
        <tag>饼状图</tag>
        <tag>3D Surfaces</tag>
        <tag>3D 曲面图</tag>
        <tag>3D Scatter Plots</tag>
        <tag>3D 散点图</tag>
        <tag>3D Bar Charts</tag>
        <tag>3D 柱状图</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas Tutorial]]></title>
    <url>%2F2018%2F11%2F29%2FPandas-Tutorial%2F</url>
    <content type="text"><![CDATA[摘要Pandas 是 Python 语言下的一个用于数据分析的工具类库。使用 Pandas 可以方便的对数据进行处理和分析。 1. Data StructuresPandas 处理数据靠的是两个核心数据结构，Series 和 DataFrame，将会贯穿于整个数据分析过程。 Series 用来处理一维的序列数据，而 DataFrame 用来处理更复杂的多维数据。 1.1. SeriesSeries 是 Pandas 中用来处理一维数据的结构，有点类似于数组，但是增加了许多额外的特性。其数据结构如下所示： index value 0 12 1 -4 2 7 3 9 Series 包含两个数组，一个是存储的实际的值，另一个存储的是值的索引。Series 中存储的值可以是所有的 NumPy 中的数据结构。 1.1.1. 创建 Series1.1.1.1. 从 list 中创建12345import numpy as npimport pandas as pds = pd.Series([12, -4, 7, 9])print(s) 打印结果： 123450 121 -42 73 9dtype: int64 1.1.1.2. 创建指定索引的 Series12345import numpy as npimport pandas as pds = pd.Series([12, -4, 7, 9], index = ['a', 'b', 'c', 'd'])print(s) 打印结果： 12345a 12b -4c 7d 9dtype: int64 1.1.1.3. 从 Numpy 数组创建123arr = np.array([1, 2, 3, 4])ser = pd.Series(arr)print(ser) 打印结果： 123450 11 22 33 4dtype: int32 需要注意的是，从 Numpy 中创建的 Series，只是引用，对 Series 中值操作的影响会直接反应到原始的 Numpy 中 123print(arr)ser[0] = 0print(arr) 打印结果： 12[1 2 3 4][0 2 3 4] 1.1.1.4. 从 dict 创建123dic = &#123;'red': 2000, 'blue': 1000, 'yellow': 500, 'orange': 1000&#125;ser = pd.Series(dic)print(ser) 打印结果： 12345red 2000blue 1000yellow 500orange 1000dtype: int64 1.1.2. 查看 Series1.1.2.1. 访问元素&amp;查看Series使用指定标签创建的 Series 既可以用下标访问元素，也可以用标签访问元素： 12print("s[1]: " + str(s[1]))print("s['b']: " + str(s['b'])) 打印结果： 12s[1]: -4s['b']: -4 另外还可以直接查看 Series 的索引和值： 12print("s.values: " + str(s.values))print("s.index: " + str(s.index)) 打印结果： 12s.values: [12 -4 7 9]s.index: Index(['a', 'b', 'c', 'd'], dtype='object') 1.1.2.2. 选取值从 Series 中选取值与 NumPy 中类似，可以直接使用切片的方式选取。 1print(s[0:2]) 打印结果： 123a 12b -4dtype: int64 另外 Series 还支持使用标签的形式来选取对应的值： 1print(s[['b', 'd']]) 注意，这里的标签是一个数组，打印结果： 123b -4d 9dtype: int64 使用表达式选择值： 1print(s[s &gt; 8]) 打印结果： 123a 12d 9dtype: int64 1.1.2.3. 赋值可以直接使用标签或索引，类似于数组进行赋值。 12s['b'] = 1print(s) 打印结果： 12345a 12b 1c 7d 9dtype: int64 1.1.3. 数学运算类似于 Numpy 中对数学运算的支持，可以使用 Series 直接与数值进行加减乘除。 1.1.4. 常用操作1.1.4.1. 去重12ser = pd.Series([1, 0, 2, 1, 2, 3])print(ser.unique()) 打印结果： 1[1 0 2 3] 1.1.4.2. 统计12ser = pd.Series([1, 0, 2, 1, 2, 3])print(ser.value_counts()) 打印结果： 123452 21 23 10 1dtype: int64 其中第一列表示的是 Series 中的值，第二列表示的是在 Series 中出现的次数。 1.1.4.3. 是否存在12ser = pd.Series([1, 0, 2, 1, 2, 3])print(ser.isin([0, 3])) 打印结果： 12345670 False1 True2 False3 False4 False5 Truedtype: bool 直接将结果返回回来： 1print(ser[ser.isin([0, 3])]) 打印结果： 1231 05 3dtype: int64 1.1.4.4. 空值在 Pandas 中使用 Numpy 中的 NaN 表示空值。可以使用 isnull() 和 notnull() 方法筛选结果。 1234ser = pd.Series([5, -3, np.NaN, 15])print(ser)print(ser.isnull()) 打印结果： 12345678910110 5.01 -3.02 NaN3 15.0dtype: float640 False1 False2 True3 Falsedtype: bool 1.2. DataFrameDataFrame 是一种类似于表格的结构，用于处理多维数据。 index color object price 0 blue ball 1.2 1 green pen 1.0 2 yellow pencil 0.5 3 red paper 0.8 4 white mug 1.5 不同于 Series，DataFrame 有两列索引，第一个索引是行索引，每个索引关联一行的数据；第二个索引包含的是一系列的标签，关联的是每个特定的列。 我们一般把行索引称为索引（index），把列索引称为标签（label）。 1.2.1. 创建 DataFrame1.2.1.1. 从字典中创建1234567891011import numpy as npimport pandas as pdmyDict = &#123; 'color': ['blue', 'green', 'yellow', 'red', 'white'], 'object': ['ball', 'pen', 'pencil', 'paper', 'mug'], 'price': [1.2, 1.0, 0.5, 0.8, 1.5]&#125;df = pd.DataFrame(myDict)print(df) 打印结果： 123456 color object price0 blue ball 1.21 green pen 1.02 yellow pencil 0.53 red paper 0.84 white mug 1.5 在创建时还可以指定需要的列，以及指定索引。 123df = pd.DataFrame(myDict, columns=['object','price'], index=['one','two','three','four','five'])print(df) 打印结果： 123456 object priceone ball 1.2two pen 1.0three pencil 0.5four paper 0.8five mug 1.5 1.2.1.2. 从 Numpy 数组中创建123arr = np.arange(16).reshape((4, 4))df = pd.DataFrame(arr, columns=['colA','colB', 'colC', 'colD'])print(df) 打印结果： 12345 colA colB colC colD0 0 1 2 31 4 5 6 72 8 9 10 113 12 13 14 15 1.2.2. 查看 DataFrame1.2.2.1. 基本信息查看索引： 1print(df.index) 打印结果： 1RangeIndex(start=0, stop=4, step=1) 查看标签： 1print(df.columns) 打印结果： 1Index(['colA', 'colB', 'colC', 'colD'], dtype='object') 查看值：1df.values 1.2.2.2. 选择列使用标签选择对应列的值： 12345678myDict = &#123; 'color': ['blue', 'green', 'yellow', 'red', 'white'], 'object': ['ball', 'pen', 'pencil', 'paper', 'mug'], 'price': [1.2, 1.0, 0.5, 0.8, 1.5]&#125;df = pd.DataFrame(myDict)print(df[["object", "price"]]) 打印结果： 123456 object price0 ball 1.21 pen 1.02 pencil 0.53 paper 0.84 mug 1.5 1.2.2.3. 选择行使用索引选择对应行的记录。其中 loc[] 中接受的可以是一个索引值，也可以是一个索引数组。 12345678myDict = &#123; 'color': ['blue', 'green', 'yellow', 'red', 'white'], 'object': ['ball', 'pen', 'pencil', 'paper', 'mug'], 'price': [1.2, 1.0, 0.5, 0.8, 1.5]&#125;df = pd.DataFrame(myDict)print(df.loc[2]) 打印结果： 1234color yellowobject pencilprice 0.5Name: 2, dtype: object 1.2.2.4. 切片DataFrame 也支持切片，不过只支持行切片。 12345678myDict = &#123; 'color': ['blue', 'green', 'yellow', 'red', 'white'], 'object': ['ball', 'pen', 'pencil', 'paper', 'mug'], 'price': [1.2, 1.0, 0.5, 0.8, 1.5]&#125;df = pd.DataFrame(myDict)print(df[1:3]) 打印结果： 123 color object price1 green pen 1.02 yellow pencil 0.5 1.2.2.5. 过滤12345678myDict = &#123; 'color': ['blue', 'green', 'yellow', 'red', 'white'], 'object': ['ball', 'pen', 'pencil', 'paper', 'mug'], 'price': [1.2, 1.0, 0.5, 0.8, 1.5]&#125;df = pd.DataFrame(myDict)print(df[df['price'] &gt; 1]) 打印结果： 123 color object price0 blue ball 1.24 white mug 1.5 1.2.3. 修改 DataFrame1.2.3.1. 增加新列123456789myDict = &#123; 'color': ['blue', 'green', 'yellow', 'red', 'white'], 'object': ['ball', 'pen', 'pencil', 'paper', 'mug'], 'price': [1.2, 1.0, 0.5, 0.8, 1.5]&#125;df = pd.DataFrame(myDict)df['new'] = 12print(df) 打印结果： 123456 color object price new0 blue ball 1.2 121 green pen 1.0 122 yellow pencil 0.5 123 red paper 0.8 124 white mug 1.5 12 也可以使用一个数组作为新的列： 12df['new'] = [3, 4, 5, 6, 7]print(df) 打印结果： 123456 color object price new0 blue ball 1.2 31 green pen 1.0 42 yellow pencil 0.5 53 red paper 0.8 64 white mug 1.5 7 这里的数组还可以换成 Series。 1.2.3.2. 删除新列12del df['price']print(df) 打印结果： 123456 color object new0 blue ball 31 green pen 42 yellow pencil 53 red paper 64 white mug 7 Index ObjectIndex Object 是 Pandas 中表示索引的对象，在前面的 Series 和 DataFrame 中实际上已经接触过这个对象了。 一般不会涉及到这个对象的操作，知道有这么个东西即可。 2. 数据操作2.1. Lambda 表达式首先创建一个三行四列的 DataFrame： 123arr = np.arange(12).reshape((3, 4)) ** 2df = pd.DataFrame(arr)print(df) 打印结果： 1234 0 1 2 30 0 1 4 91 16 25 36 492 64 81 100 121 然后利用 lambda 表达式对其中的每个元素开平方： 12df = df.apply(lambda x: np.sqrt(x))print(df) 打印结果： 1234 0 1 2 30 0.0 1.0 2.0 3.01 4.0 5.0 6.0 7.02 8.0 9.0 10.0 11.0 2.2. 统计信息sum() 123arr = np.arange(12).reshape((3, 4))df = pd.DataFrame(arr, columns=['colA','colB', 'colC', 'colD'])print(df.sum()) 打印结果： 12345colA 12colB 15colC 18colD 21dtype: int64 mean() 123arr = np.arange(12).reshape((3, 4))df = pd.DataFrame(arr, columns=['colA','colB', 'colC', 'colD'])print(df.mean()) 打印结果： 12345colA 4.0colB 5.0colC 6.0colD 7.0dtype: float64 describe() 123arr = np.arange(12).reshape((3, 4))df = pd.DataFrame(arr, columns=['colA','colB', 'colC', 'colD'])print(df.describe()) 打印结果： 123456789 colA colB colC colDcount 3.0 3.0 3.0 3.0mean 4.0 5.0 6.0 7.0std 4.0 4.0 4.0 4.0min 0.0 1.0 2.0 3.025% 2.0 3.0 4.0 5.050% 4.0 5.0 6.0 7.075% 6.0 7.0 8.0 9.0max 8.0 9.0 10.0 11.0 2.3. 排序2.3.1. Series1234ser = pd.Series([1, 5, 2, 8, 3], index=['one','two','three','four','five'])print(ser.sort_index())print(ser.sort_values()) 打印结果： 123456789101112131415# sort by indexfive 3four 8one 1three 2two 5dtype: int64# sort by valuesone 1three 2five 3two 5four 8dtype: int64 2.3.2. DataFrame根据 colA 列的值进行倒序排序。 123arr = np.arange(12).reshape((3, 4))df = pd.DataFrame(arr, columns=['colA','colB', 'colC', 'colD'])print(df.sort_values(by=['colA'], ascending=False)) 打印结果： 1234 colA colB colC colD2 8 9 10 111 4 5 6 70 0 1 2 3 3. 读写数据3.1. CSV读取CSV 1df = pd.read_csv("./somefile.csv") 写入CSV 1pd.to_csv("./somefile.csv") 4. 数据分析4.1. 数据准备4.1.1. Merging根据键进行拼接，有点类似于 SQL 语句中的 join 连接操作。 4.1.1.1. 简单连接1234567891011121314151617import numpy as npimport pandas as pdmyDict1 = &#123; 'id':['ball','pencil','pen','mug','ashtray'], 'price': [12.33,11.44,33.21,13.23,33.62]&#125;myDict2 = &#123; 'id':['pencil','pencil','ball','pen'], 'color': ['white','red','red','black']&#125;df1 = pd.DataFrame(myDict1)df2 = pd.DataFrame(myDict2)print(pd.merge(df1, df2)) 打印结果： 12345 id price color0 ball 12.33 red1 pencil 11.44 white2 pencil 11.44 red3 pen 33.21 black 这两个 DataFrame 通过 ID 这一列进行连接。 4.1.1.2. 使用 on 指定 key 字段123456789101112131415myDict1 = &#123; 'id':['ball','pencil','pen','mug','ashtray'], 'color': ['white','red','red','black','green'], 'brand': ['OMG','ABC','ABC','POD','POD']&#125;myDict2 = &#123; 'id':['pencil','pencil','ball','pen'], 'brand': ['OMG','POD','ABC','POD']&#125;df1 = pd.DataFrame(myDict1)df2 = pd.DataFrame(myDict2)print(pd.merge(df1, df2, on = 'id')) 打印结果： 12345 id color brand_x brand_y0 ball white OMG ABC1 pencil red ABC OMG2 pencil red ABC POD3 pen red ABC POD 4.1.1.3. 指定不同字段连接以上几个实例中连接时是假设两个 DataFrame 作为 key 的字段的列名是相同的，很多时候可能会存在不同的情况，那么应该如何处理？ 123456789101112131415myDict1 = &#123; 'id':['ball','pencil','pen','mug','ashtray'], 'color': ['white','red','red','black','green'], 'brand': ['OMG','ABC','ABC','POD','POD']&#125;myDict2 = &#123; 'sid':['pencil','pencil','ball','pen'], 'brand': ['OMG','POD','ABC','POD']&#125;df1 = pd.DataFrame(myDict1)df2 = pd.DataFrame(myDict2)print(pd.merge(df1, df2, left_on = 'id', right_on = 'sid')) 打印结果： 12345 id color brand_x brand_y0 ball white OMG ABC1 pencil red ABC OMG2 pencil red ABC POD3 pen red ABC POD 4.1.1.4. 连接方式在 SQL 中进行表的连接时，有几种连接方式，包括 inner join, outer join, left join and right join. 在 Pandas 中默认是内连接，也可以通过指定参数采用不同的连接方式。 123456789101112131415161718myDict1 = &#123; 'id':['ball','pencil','pen','mug','ashtray'], 'color': ['white','red','red','black','green'], 'brand': ['OMG','ABC','ABC','POD','POD']&#125;myDict2 = &#123; 'id':['pencil','pencil','ball','pen'], 'brand': ['OMG','POD','ABC','POD']&#125;df1 = pd.DataFrame(myDict1)df2 = pd.DataFrame(myDict2)print(pd.merge(df1, df2, on='id')) # 默认内连接print(pd.merge(df1, df2, on='id', how='outer'))print(pd.merge(df1, df2, on='id', how='left'))print(pd.merge(df1, df2, on='id', how='right')) 打印结果： 12345678910111213141516171819202122232425262728293031# 内连接：两边都有值的记录进行连接 id color brand_x brand_y0 ball white OMG ABC1 pencil red ABC OMG2 pencil red ABC POD3 pen red ABC POD# 外连接：将所有的记录都进行连接 id color brand_x brand_y0 ball white OMG ABC1 pencil red ABC OMG2 pencil red ABC POD3 pen red ABC POD4 mug black POD NaN5 ashtray green POD NaN# 左连接：以第一张表为主进行连接 id color brand_x brand_y0 ball white OMG ABC1 pencil red ABC OMG2 pencil red ABC POD3 pen red ABC POD4 mug black POD NaN5 ashtray green POD NaN# 右连接：以第二张表为主进行连接 id color brand_x brand_y0 ball white OMG ABC1 pencil red ABC OMG2 pencil red ABC POD3 pen red ABC POD 4.1.1.5. 基于索引连接Pandas 还提供了一个 join() 方法，用于基于索引的连接。 4.1.2. Concatenating将多个表拼接成一张表。 1234df1 = pd.DataFrame(np.random.rand(9).reshape(3,3), index=[1,2,3], columns=['A','B','C'])df2 = pd.DataFrame(np.random.rand(9).reshape(3,3), index=[4,5,6], columns=['A','B','C'])print(pd.concat([df1, df2])) 打印结果： 1234567 A B C1 0.110074 0.360846 0.8175332 0.464365 0.804425 0.3726683 0.288729 0.783474 0.6496444 0.941699 0.804263 0.4932435 0.292398 0.573224 0.3078156 0.447318 0.134989 0.667292 4.1.3. Combining如果碰到需要合并的两个 DataFrame 中有重复的索引存在，而又不希望被第二个 DataFrame 覆盖相同索引的记录时，可以使用这个方法合并。 123456ser1 = pd.Series(np.random.rand(5),index=[1,2,3,4,5])ser2 = pd.Series(np.random.rand(4),index=[2,4,5,6])print(ser1)print(ser2)print(ser1.combine_first(ser2)) 打印结果： 12345678910111213141516171819201 0.5154382 0.3716253 0.9277174 0.3217095 0.813122dtype: float642 0.5643504 0.7181795 0.7417266 0.108383dtype: float641 0.5154382 0.3716253 0.9277174 0.3217095 0.8131226 0.108383dtype: float64 4.1.3. 行转列在处理独热编码时，会碰到这样的需求，直接看例子。 12345678910myDict = &#123; 'color':['white','white','white', 'red','red','red', 'black','black','black'], 'item':['ball','pen','mug', 'ball','pen','mug', 'ball','pen','mug'], 'value': np.random.rand(9)&#125;df = pd.DataFrame(myDict)print(df)pdf = df.pivot('color', 'item')print(pdf) 打印结果： 1234567891011121314151617 color item value0 white ball 0.5835821 white pen 0.4136162 white mug 0.3892763 red ball 0.5418634 red pen 0.4135785 red mug 0.9426186 black ball 0.8503457 black pen 0.2305368 black mug 0.612583 value item ball mug pencolor black 0.850345 0.612583 0.230536red 0.541863 0.942618 0.413578white 0.583582 0.389276 0.413616 4.2. 数据转换4.2.1. 去重123456789myDict = &#123; 'color': ['white','white','red','red','white'], 'value': [2,1,3,3,2]&#125;df = pd.DataFrame(myDict)print(df.duplicated())print(df[df.duplicated()]) 打印结果： 123456789100 False1 False2 False3 True4 Truedtype: bool color value3 red 34 white 2 这里需要注意的是，False 表示没有重复，那么后面打印出来的两个就是重复的值，而不是去重后的结果。 4.2.2. mapmap 或者说是字典，利用字典可以进行数据替换、添加等操作。 4.2.2.1. 基于 map 的替换12345678910111213141516myDict = &#123; 'item':['ball','mug','pen','pencil','ashtray'], 'color':['white','rosso','verde','black','yellow'], 'price':[5.56,4.20,1.30,0.56,2.75]&#125;df = pd.DataFrame(myDict)newColor = &#123; 'rosso': 'red', 'verde': 'green'&#125;print(df)print(df.replace(newColor)) 打印结果： 12345678910111213 item color price0 ball white 5.561 mug rosso 4.202 pen verde 1.303 pencil black 0.564 ashtray yellow 2.75 item color price0 ball white 5.561 mug red 4.202 pen green 1.303 pencil black 0.564 ashtray yellow 2.75 4.2.2.2. 基于 map 的添加操作123456789101112131415161718192021myDict = &#123; 'item':['ball','mug','pen','pencil','ashtray'], 'color':['white','rosso','verde','black','yellow']&#125;df = pd.DataFrame(myDict)prices = &#123; 'ball' : 5.56, 'mug' : 4.20, 'bottle' : 1.30, 'scissors' : 3.41, 'pen' : 1.30, 'pencil' : 0.56, 'ashtray' : 2.75&#125;print(df)df['price'] = df['item'].map(prices)print(df) 打印结果： 12345678910111213 item color0 ball white1 mug rosso2 pen verde3 pencil black4 ashtray yellow item color price0 ball white 5.561 mug rosso 4.202 pen verde 1.303 pencil black 0.564 ashtray yellow 2.75 4.2.3. 离散化有时候在处理一个连续型值的列时，需要将值划分成几个区间，然后转换成离散型变量。 4.2.3.1. 指定区间切分123456results = [12, 34, 67, 55, 28, 90, 99, 12, 3, 56, 74, 44, 87, 23, 49, 89, 87]bins = [0, 25, 50, 75, 100]cat = pd.cut(results, bins)print(cat) 打印结果： 123[(0, 25], (25, 50], (50, 75], (50, 75], (25, 50], ..., (75, 100], (0, 25], (25, 50], (75, 100], (75, 100]]Length: 17Categories (4, interval[int64]): [(0, 25] &lt; (25, 50] &lt; (50, 75] &lt; (75, 100]] 这里有三行结果，分别来看每行的含义： 第一行是转换后的结果，即每个值被划分到哪个区间，这里就被替换成这个区间了 第二行是总数，即处理的数据的条数 第三行是划分的区间，这里一共被划分成四个区间 查看所划分的分类： 1print(cat.categories) 打印结果： 123IntervalIndex([(0, 25], (25, 50], (50, 75], (75, 100]] closed='right', dtype='interval[int64]') 查看划分结果对应区间的代码： 1print(cat.codes) 打印结果： 1[0 1 2 2 1 3 3 0 0 2 2 1 3 0 1 3 3] 统计划分后的结果： 1print(pd.value_counts(cat)) 打印结果： 12345(75, 100] 5(50, 75] 4(25, 50] 4(0, 25] 4dtype: int64 可以对区间设置名称： 1234567results = [12, 34, 67, 55, 28, 90, 99, 12, 3, 56, 74, 44, 87, 23, 49, 89, 87]bins = [0, 25, 50, 75, 100]bin_names = ['unlikely','less likely','likely','highly likely']cat = pd.cut(results, bins, labels = bin_names)print(cat) 打印结果： 123[unlikely, less likely, likely, likely, less likely, ..., highly likely, unlikely, less likely, highly likely, highly likely]Length: 17Categories (4, object): [unlikely &lt; less likely &lt; likely &lt; highly likely] 4.2.3.2. 指定区间数量等距切分根据指定区间的数量，以及该列值的最大值和最小值，切割成若干等分，然后进行划分。 123456results = [12, 34, 67, 55, 28, 90, 99, 12, 3, 56, 74, 44, 87, 23, 49, 89, 87]bin_names = ['unlikely','less likely','likely','highly likely']cat = pd.cut(results, 4, labels = bin_names)print(cat) 打印结果： 123[unlikely, less likely, likely, likely, less likely, ..., highly likely, unlikely, less likely, highly likely, highly likely]Length: 17Categories (4, object): [unlikely &lt; less likely &lt; likely &lt; highly likely] 4.2.3.3. 指定区间数量根据密度划分指定区间数量，根据数据散布的特点，确保每个区间最终划分的值的数量相同，至于区间边界，不一定是等分的。 12345678results = [12, 34, 67, 55, 28, 90, 99, 12, 3, 56, 74, 44, 87, 23, 49, 89, 87, 20]bin_names = ['unlikely','likely','highly likely']cat1 = pd.cut(results, 3, labels = bin_names)cat2 = pd.qcut(results, 3, labels = bin_names)print(pd.value_counts(cat1))print(pd.value_counts(cat2)) 打印结果： 1234567891011# pd.cut()unlikely 7highly likely 6likely 5dtype: int64# pd.qcut()highly likely 6likely 6unlikely 6dtype: int64 4.2.4. 随机采样123456789myDict = &#123; 'item':['ball','mug','pen','pencil','ashtray'], 'color':['white','rosso','verde','black','yellow'], 'price':[5.56,4.20,1.30,0.56,2.75]&#125;df = pd.DataFrame(myDict)sample = np.random.randint(0, len(df), size=3)print(df.take(sample)) 打印结果： 1234 item color price2 pen verde 1.300 ball white 5.563 pencil black 0.56 4.3. 数据聚合数据聚合是数据分析中很重要的一部分内容，比如常见的求和、求平均等都是属于聚合。聚合中很重要的一部分就是分组处理。 我们可以把分组处理拆分为三个阶段： 拆分 —— 将数据集拆分成不同的分组 处理 —— 对每个分组分别进行处理 合并 —— 将每个分组处理的结果合并成最终的结果 给定一个 DataFrame： 123456789myDict = &#123; 'color': ['white','red','green','red','green'], 'object': ['pen','pencil','pencil','ashtray','pen'], 'price1': [5.56,4.20,1.30,0.56,2.75], 'price2': [4.75,4.12,1.60,0.75,3.15]&#125;df = pd.DataFrame(myDict)print(df) 打印结果： 123456 color object price1 price20 white pen 5.56 4.751 red pencil 4.20 4.122 green pencil 1.30 1.603 red ashtray 0.56 0.754 green pen 2.75 3.15 现在要求按照颜色一列统计价格1的平均值。 首先按照颜色进行分组： 12group = df['price1'].groupby(df['color'])print(group.groups) 打印结果： 12345&#123; 'green': Int64Index([2, 4], dtype='int64'), 'red': Int64Index([1, 3], dtype='int64'), 'white': Int64Index([0], dtype='int64')&#125; 然后计算每组的平均值： 1print(group.mean()) 打印结果： 12345colorgreen 2.025red 2.380white 5.560Name: price1, dtype: float64 5. 环境设置有时在使用 Pandas 打印内容较多的数据时，会出现显示不完整的情况，可以通过设置以下一些属性值来解决该问题。 123456#显示所有列pd.set_option('display.max_columns', None)#显示所有行pd.set_option('display.max_rows', None)#设置value的显示长度为100，默认为50pd.set_option('max_colwidth',100)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
        <tag>Data Analysis</tag>
        <tag>Series</tag>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy Tutorial]]></title>
    <url>%2F2018%2F11%2F29%2FNumpy-Tutorial%2F</url>
    <content type="text"><![CDATA[摘要NumPy 是 Numeric Python，是 Python 中很重要的一个科学计算类库，在机器学习、数据分析中应用广泛，也是很多相关领域类库的基础。 1. 基本概念NumPy的主要对象是一个均衡的多维数组。也就是一张元素的表格，这些元素通常是数字，所有的元素都是相同类型的，通过下标构成的元组进行索引。在NumPy中的多维数组的维度被成为轴（axes）。轴的数量被称为秩（rank） 例如，在3D空间中的一个点的坐标[1, 2, 1]可以表示为一个秩为1的数组。因为它只有一个轴。这个轴的长度是3.在下面的例子中，这个数组的秩为2（这是一个二维数组）。第一维（轴）的长度为2，第二维的长度为3。 1234[ [1, 0, 0], [0, 1, 2]] NumPy 的数组类被称为 ndarray，其别名为 array。注意，numpy.array 与标准 Python 类库中的 array.array 是不同的，标准类库中的数组只能处理一维数组，并且只提供了极少数的功能。下面列举了一些ndarray 对象的重要属性。 1.1. 维度（ndarray.ndim）表示数组中轴（维度）的数量。在Python中，维度的数量也被称为秩（rank） 一维数组1234import numpy as npmyArray = np.array([1, 0])print(myArray.ndim) # print 1 二维数组1234import numpy as npmyArray = np.array([[1, 0, 3], [2, 3, 5]])print(myArray.ndim) # print 2 三维数组1234567891011121314import numpy as npmyArray = np.array( [ [ [1, 2], [3, 1] ], [ [1, 2], [3, 1] ] ])print(myArray.ndim) # print 3 1.2. 维度的大小（ndarray.shape）是一个表示数组维度大小的元组。元组中的整型数字表示数组中相应维度的大小。例如一个有 n 行 m 列的举证，shape 就是(n, m)。这个 shape 元组的长度也就是秩（rank）或者说是维度的数量。 一维数组1234import numpy as npmyArray = np.array([1, 0])print(myArray.shape) 这个打印的结果是 (2) 。这是个一维数组，一共有两个元素，因此shape是(2)。 二维数组1234import numpy as npmyArray = np.array([[1, 0, 3], [2, 3, 5]])print(myArray.shape) 这个打印的结果是(2, 3)。这是一个二维数组，因此这个shape元组中有两个元素。 第一个元素表示第一个维度中元素的数量，这个数组中第一个维度中有两个元素，分别是[1, 0, 3], [2, 3, 5]。 第二个元素表示第二个维度中元素的数量，也就是[1, 0, 3]和[2, 3, 5]中都有三个元素，因此是3。 三维数组1234567891011121314import numpy as npmyArray = np.array( [ [ [1, 2, 1], [3, 1, 1] ], [ [1, 2, 1], [3, 1, 1] ] ])print(myArray.shape) 这个打印出来的结果是(2, 2, 3)，这个数组是一个三维数组，因此这个shape元组中有三个元素。 第一个维度中有两个元素，第二个维度中也是有两个元素，第三个维度则是最内部的有三个元素的数组。 1.3. 元素数量（ndarray.size）数组中所有元素的数量。这个数字等同于 shape 的元组中所有元素的乘积。 一维数组 1234import numpy as npmyArray = np.array([1, 0])print(myArray.size) 这个打印出来的结果是2。表示这个数组中一个有两个元素。 二维数组 1234import numpy as npmyArray = np.array([[1, 0, 3], [2, 3, 0]])print(myArray.size) 这个数组中有六个元素，因此打印出来的结果是6。 1.4. 元素类型（ndarray.dtype）一个用于描述数组中元素类型的对象。使用标准Python类型可以创建或指定dtype，除此之外，NumPy还提供了其自定义的类型：numpy.int32，numpy.int16和numpy.float64等等 浮点型 1234import numpy as npmyArray = np.array([3.2, 5.1])print(myArray.dtype) 这个打印出来的内容是 float64。表示这里的元素是浮点数。 整型1234import numpy as npmyArray = np.array([3, 5])print(myArray.dtype) 这里打印出来的内容是 int32。表示是32位整型。 布尔型1234import numpy as npmyArray = np.array([False, True])print(myArray.dtype) 这里打印出来的内容是 bool。表示是布尔值。 字符型1234import numpy as npmyArray = np.array(['a', 'cc'])print(myArray.dtype) 这里打印出来的内容是U2。U表示的是数据类型，表示Unicode，后面的2表示数据的长度是2个字符。 2. 创建数组2.1. 将Python数据结构转为NumPy数组2.1.1. 将元组创建为数组array()函数可以接受Python的元组为参数，并将这个元组创建为数组。1234import numpy as npmyArray = np.array((3, 2, 4, 5))print(myArray) 打印内容为：[3 2 4 5]。 2.1.2. 将列表创建为数组array()函数可以接受Python中的列表作为参数，将其转为数组。1234import numpy as npmyArray = np.array([[1, 0, 3], [2, 3, 0]])print(myArray) 2.2. 使用占位符创建指定shape的数组所谓使用占位符创建数组的意思就是创建一个数组，然后使用占位符来填充其中的每个元素。比如创建一个 shape 为 (2,) 的数组，则表示创建一个如下形式的数组：1[占位符, 占位符] 如果要创建一个shape为(2, 3)的数组，则结果为如下形式：1234[ [占位符, 占位符, 占位符], [占位符, 占位符, 占位符]] 2.2.1. 使用0作为占位符创建数组使用 zeros() 函数接受一个 shape 元组可以用来创建占位符为 0 的指定大小的数组。1234import numpy as npmyArray = np.zeros((3, 4))print(myArray) 打印结果为： 123[[ 0. 0. 0. 0.] [ 0. 0. 0. 0.] [ 0. 0. 0. 0.]] 2.2.2. 使用1作为占位符创建数组使用 ones() 函数接受一个 shape 元组可以用来创建占位符为1的指定大小的数组。1234import numpy as npmyArray = np.ones((3, 4))print(myArray) 打印结果：123[[ 1. 1. 1. 1.] [ 1. 1. 1. 1.] [ 1. 1. 1. 1.]] 2.2.3. 使用随机浮点数作为占位符创建数组使用 empty() 函数接受一个shape元组可以用来创建占位符为随机数的指定大小的数组。1234import numpy as npmyArray = np.empty((3, 4))print(myArray) 打印结果：123[[ 8.82769181e+025 7.36662981e+228 7.54894003e+252 2.95479883e+137] [ 1.42800637e+248 2.64686750e+180 1.09936856e+248 6.99481925e+228] [ 7.54894003e+252 7.67109635e+170 2.64686750e+180 3.50784751e-191]] 2.2.4. 创建 0 1 对称数组创建中轴线为1，其余为0填充的数组。 1234import numpy as npmyArray = np.eye(5)print(myArray) 打印结果：12345[[ 1. 0. 0. 0. 0.] [ 0. 1. 0. 0. 0.] [ 0. 0. 1. 0. 0.] [ 0. 0. 0. 1. 0.] [ 0. 0. 0. 0. 1.]] 2.3. 创建数字序列数组2.3.1. arange()使用 arange() 创建数字序列数组，这个函数接受三个参数，前两个参数分别表示序列的首尾，第三个参数表示步长。 1234import numpy as npmyArray = np.arange(1, 20, 2)print(myArray) 该段代码创建的数组为：1[ 1 3 5 7 9 11 13 15 17 19] 使用 reshape() 调整数组维度，刺入将上面的数组调整为 (2, 5)： 1print(myArray.reshape(2, 5)) 打印结果： 12[[ 1 3 5 7 9] [11 13 15 17 19]] 2.3.2. linespace()以线性等分形式创建数组，给定起始值和切分的分数，自动创建等分元素数组。 1234import numpy as npmyArray = np.linspace(0, 5, 11)print(myArray) 打印结果： 1[ 0. 0.5 1. 1.5 2. 2.5 3. 3.5 4. 4.5 5. ] 2.3.3. 创建重复序列数组12345import numpy as npmyArray = np.array([1, 2, 3] * 3)# or myArray = np.repeat([1, 2, 3], 3)print(myArray) 打印结果： 1[1 2 3 1 2 3 1 2 3] 2.3.4. 创建重复元素数组注意与重复序列数组的区别。 1234import numpy as npmyArray = np.repeat([1, 2, 3], 3)print(myArray) 打印结果： 1[1 1 1 2 2 2 3 3 3] 3. 基本运算3.1. 数据类型1234567import numpy as npmyArray = np.array([[4, 5, 6], [14, 25, 36]])print(myArray.dtype)newArray = myArray.astype('f')print(newArray.dtype) 结果为：12int32float32 数组上的算术运算是作用于数组内的元素上的。 3.2. 减法运算123456import numpy as nparray1 = np.array([10, 20, 30, 40])array2 = np.array([1, 2, 3, 4])array3 = array1 - array2print(array3) 打印结果：[9, 18, 27, 36]。观察这个运算后的结果可以发现上面两个数组相减实际上是对应位置上的元素进行了减法运算。 3.3. 平方运算同样的道理，对数组进行平方运算也同样是作用在数组的元素上。 12345import numpy as nparray1 = array([1, 2, 3, 4])array2 = array1 ** 2print(array2) 结果为：[1, 4, 9, 16] 3.4. 矩阵乘法运算首先复习一下矩阵的乘法运算，有如下两个A、B矩阵相乘： 12345a b e fc d g hae+bg af+bhce+dg cf+dh 仔细观察其运算规则实际上新的矩阵的第一行的第一个元素是原矩阵A的第一行分别与矩阵B的第一列元素相乘之后的和。而第一行第二个元素是矩阵A的第一行与矩阵B的第二列乘积的和。 在 NumPy 中矩阵相乘需要使用 dot() 方法，而不是直接使用 “*”。 123456import numpy as nparray1 = ones((2, 2))array2 = ones((2, 2))array3 = array1.dot(array2)print(array3) 结果为：12[[ 2. 2.] [ 2. 2.]] 3.5. 数学函数常用的数学计算，比如求和、求平均、最值等。 1234567891011import numpy as npmyArray = np.array([1, 5, 0, -7, 23, -2])print("sum: " + str(myArray.sum()))print("max: " + str(myArray.max()))print("min: " + str(myArray.min()))print("mean: " + str(myArray.mean()))print("std: " + str(myArray.std()))print("index of max: " + str(myArray.argmax()))print("index of min: " + str(myArray.argmin())) 结果为：1234567sum: 20max: 23min: -7mean: 3.33333333333std: 9.49853789918index of max: 4index of min: 3 4. 数组操作4.1. 合并数组4.1.1. 竖直合并 vstack()12345import numpy as npmyArray = np.ones((2, 3))myArray2 = np.vstack([myArray, myArray*2])print(myArray2) 结果为：1234[[ 1. 1. 1.] [ 1. 1. 1.] [ 2. 2. 2.] [ 2. 2. 2.]] 4.1.2. 水平合并 vstack()12345import numpy as npmyArray = np.ones((2, 3))myArray2 = np.hstack([myArray, myArray*2])print(myArray2) 结果为：12[[ 1. 1. 1. 2. 2. 2.] [ 1. 1. 1. 2. 2. 2.]] 4.2. 转置12345import numpy as npmyArray = np.array([[4, 5, 6], [14, 25, 36]])print(myArray)print(myArray.T) 结果为：12345678# myArray[[ 4 5 6] [14 25 36]]# myArray.T[[ 4 14] [ 5 25] [ 6 36]] 4.3. 索引和切片4.3.1. 一维数组切片12345678910111213141516import numpy as npmyArray = np.arange(13)**2print(myArray) # [ 0 1 4 9 16 25 36 49 64 81 100 121 144]# 数组索引是从 0 开始的。print(myArray[2]) # 4# 提取索引从 1 到 4 的元素print(myArray[1:5]) # [ 1 4 9 16]# 从后往前获取 4 个元素print(myArray[-4:]) # [ 81, 100, 121, 144]# 第二个冒号可以用来表示步长 array [start:stop:step]print(myArray[-5::-2]) # [64, 36, 16, 4, 0] 4.3.2. 二维数组切片二维数组切片的语法： 1array[row, column] 首先创建一个测试用的二维数组： 12345import numpy as npmyArray = np.arange(36)myArray.resize((6, 6))print(myArray) 结果为：123456[[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23] [24 25 26 27 28 29] [30 31 32 33 34 35]] 1234567891011121314# 索引print(myArray[2, 2]) # 4# 索引为 3 的行中的 索引为 3 - 5 的元素print(myArray[3, 3:6]) # [21, 22, 23]# 选择前两行的前五个元素print(myArray[:2, :-1]) # [[ 0, 1, 2, 3, 4], [ 6, 7, 8, 9, 10]]# 条件筛选print(myArray[myArray &gt; 30]) # [31, 32, 33, 34, 35]# 条件筛选重新赋值myArray[myArray &gt; 30] = 30 重新赋值后的数组： 123456[[ 0, 1, 2, 3, 4, 5],[ 6, 7, 8, 9, 10, 11],[12, 13, 14, 15, 16, 17],[18, 19, 20, 21, 22, 23],[24, 25, 26, 27, 28, 29],[30, 30, 30, 30, 30, 30]] 4.4. 复制在 Numpy 中处理复制操作时，需要比较小心，这个涉及到值拷贝和引用拷贝的问题。 仍然创建一个二维数组用于测试。 12345import numpy as npmyArray = np.arange(36)myArray.resize((6, 6))print(myArray) 结果为：123456[[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23] [24 25 26 27 28 29] [30 31 32 33 34 35]] 然后对这个二维数组进行切片： 12myArray2 = myArray[:3, :3]print(myArray2) 打印结果： 123[[ 0, 1, 2], [ 6, 7, 8], [12, 13, 14]] 接下来我们对这个 myArray2 进行修改： 12myArray2[:] = 0print(myArray2) 修改后，myArray2 所有的元素都变成了 0: 123[[0, 0, 0], [0, 0, 0], [0, 0, 0]] 此时，我们再观察一下 myArray，看看它有什么变化： 1print(myArray) 打印结果： 123456[[ 0, 0, 0, 3, 4, 5], [ 0, 0, 0, 9, 10, 11], [ 0, 0, 0, 15, 16, 17], [18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29], [30, 30, 30, 30, 30, 30]] 我们发现这个数组的值也被修改了，这个就是所谓的引用，myArray2 并没有重新创建一个数组。 要避免这个问题，需要使用 copy() 方法来拷贝数组。 12345678myArray_copy = myArray.copy()print(myArray_copy)myArray_copy[:] = 10print(myArray_copy)print(myArray) 对比几次打印结果： 1234567891011121314151617181920212223# 一开始拷贝的 myArray_copy[[ 0, 0, 0, 3, 4, 5], [ 0, 0, 0, 9, 10, 11], [ 0, 0, 0, 15, 16, 17], [18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29], [30, 30, 30, 30, 30, 30]]# 修改后的 myArray_copy[[10 10 10 10 10 10] [10 10 10 10 10 10] [10 10 10 10 10 10] [10 10 10 10 10 10] [10 10 10 10 10 10] [10 10 10 10 10 10]]# 原始的 myArray[[ 0, 0, 0, 3, 4, 5], [ 0, 0, 0, 9, 10, 11], [ 0, 0, 0, 15, 16, 17], [18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29], [30, 30, 30, 30, 30, 30]] 4.5. 遍历数组1234import numpy as npmyArray = np.random.randint(0, 10, (4,3))print(myArray) 结果为：1234[[8 3 8] [4 5 0] [7 4 7] [1 3 5]] 遍历数组： 12for row in myArray: print(row) 打印结果： 1234[8 3 8][4 5 0][7 4 7][1 3 5] 利用索引遍历： 12for i in range(len(myArray)): print(myArray[i]) 打印结果： 1234[8 3 8][4 5 0][7 4 7][1 3 5] 通过索引和行遍历： 12for i, row in enumerate(myArray): print('row', i, 'is', row) 打印结果： 1234row 0 is [8 3 8]row 1 is [4 5 0]row 2 is [7 4 7]row 3 is [1 3 5] 遍历多个数组： 1234myArray2 = myArray * 2 # 创建第二个数组for i, j in zip(myArray, myArray2): print(i,'+',j,'=',i+j) 打印结果为：1234[8 3 8] + [16 6 16] = [24 9 24][4 5 0] + [ 8 10 0] = [12 15 0][7 4 7] + [14 8 14] = [21 12 21][1 3 5] + [ 2 6 10] = [ 3 9 15]]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Numpy</tag>
        <tag>Array</tag>
        <tag>Math Function</tag>
        <tag>Slicing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Kaggle的泰坦尼克生存预测介绍一种数据分析框架]]></title>
    <url>%2F2018%2F11%2F28%2F%E5%9F%BA%E4%BA%8EKaggle%E7%9A%84%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E4%BB%8B%E7%BB%8D%E4%B8%80%E7%A7%8D%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[摘要本文通过一个 Kaggle 的入门级项目泰坦尼克生存预测，介绍一个一般性的数据科学项目工程框架。该框架涵盖一般数据科学问题解决方案的几个主要过程。包括定义问题、收集数据、准备数据、探索性分析、处理模型、验证和优化。 项目介绍项目描述在1912年4月15日发生的泰坦沉船事件中，2224名乘客和船员中，有1502人丧生。在这次海难中导致如此多人员丧生的一个重要原因是没有足够的救生船。不过其中一些乘客的存活率要比其他乘客的存活率大，比如妇女/儿童和上层乘客。 在这个挑战中，我们期望你分析一下什么样的座次有利于提高存活率。并且，我们要求你应用机器学习工具来预测哪些用户在此次海难中存活下来。 评价目标预测在海难中存活的乘客。对给定测试集中的乘客ID，预测一个 0 或 1 的值表示其存活情况。 度量标准准确预测乘客存活情况的百分比。我们使用“准确率”来表示。 提交文件格式提交一份包含 418 条数据和一个标题行的 CSV 文件。如果提交的文件中有多于的行或列，回报错。 文件应该正好只有两行：1、PassengerId：乘客ID，任意顺序2、Survived：存活情况，1表示存活，0表示丧生 12345PassengerId,Survived892,0893,1894,0Etc. 数据数据被分为两组：1、训练集（train.csv）2、测试集（test.csv） 训练集用来构建你的机器学习模型。在这个训练集中，我们为每个乘客ID提供了一个对应的结果，用于表示这位乘客是否生还。你的模型需要基于乘客的特征，如乘客的性别、分类等。你也可以利用特征工程来创建新的特征。 测试集测试集用来检查你的模型对于未知的数据能够有多好的效果。在测试集中，我们并没有提供乘客对应的生还情况。而是由你来预测乘客是否生还。利用你通过训练集训练出来的模型，对测试集中每位乘客的生还情况进行预测。 数据字典 变量 定义 值 survival 是否生还 0=No,1=Yes pclass 坐席类型 1=一等座, 2=二等座, 3=三等座 sex 性别 Age 年龄 sibsp 在船上的兄弟姐妹/配偶数量 parch 在船上的父母/子女数量 ticket 船票编号 fare 票价 cabin 船舱编号 embarked 登船港口 C=Cherbourg, Q=Queenstown, S=Southampton 变量备注pclass：表示社会经济地位 1：一等座上层 2：二等座中层 3：三层座底层 age：如果小于一岁，年龄是分数。如果年龄是估计的，其形式为 XX.5 sibsp/parch：数据集中家庭关系的定义 一些儿童仅仅由保姆陪同出行，因此他们的 parch = 0。 解决过程整体框架1、定义问题在真正决定使用什么样的技术、算法之前，要明确我们要解决的问题是什么。而不是一股脑的就套用最新的技术、工具或算法。 2、收集数据John Naisbitt 在他 1984 的书中写到“我们淹没在数据中，却在寻找知识”。所以，现在我们面临的是数据集已经以某种形式存在于某处。可能是开放的或者需要挖掘的，结构化或非结构化的、静态的或流式数据等等。你只需要知道如何去找到它们，然后将这些“脏数据”转化为“干净数据”。 3、准备数据这一阶段经常被称为数据整理（data wrangling），其中一个必须的流程就是将“杂乱的”数据转换为“易于管理的”数据。数据整理包括以下几部分内容： 实现易于存储和处理的数据架构（data architecture） 开发质量与控制的数据治理（data governance）标准 数据抽取（data extraction） 以识别异常数据、缺失值以及离群值为目标的数据清理（data cleaning） 4、探索性分析如果有过数据工作经验的人可能会知道垃圾输入（garbage-in）和垃圾输出（garbage-out）。因此应该使用描述性统计分析和图示统计分析在数据集中寻找潜在的问题、模式、分类、相关系数和对照等内容。另外，数据分类对于理解和选择合适的假设检验或数据模型是非常重要的。 5、模型数据类似于描述性和推理性统计，数据模型可以总结并预测特征结果。通过数据集特征值以及预期结果分析，可以决定你的算法是否能够使用。需要注意的是算法并不是魔法棒或者银弹，你必须真正懂得如何去选择合适的工具解决对应的问题。 6、验证和实现模型在基于一部分数据训练完成模型之后，需要检验模型。以确保没有对该部分子集数据过拟合。在这个阶段，我们确定模型是过拟合、泛化还是欠拟合。 7、优化接下来就是不断的优化，让你的模型变得更好、更快、更强。 STEP 1. 定义问题在这个项目中，问题的定义非常明确，并且已经在项目介绍中明确的给出了，开发一个算法用于预测泰坦尼克事故中乘客的生还情况。 在1912年4月15日发生的泰坦尼克沉船事件中，2224名乘客和船员中，有1502人丧生。在这次海难中导致如此多人员丧生的一个重要原因是没有足够的救生船。不过其中一些乘客的存活率要比其他乘客的存活率大，比如妇女/儿童和上层乘客。 在这个挑战中，我们期望你分析一下什么样的座次有利于提高存活率。并且，我们要求你应用机器学习工具来预测哪些用户在此次海难中存活下来。 STEP 2. 收集数据在这个项目中，数据已经给定，直接下载即可。 STEP 3. 准备数据TODO 一些描述 STEP 3.1. 准备环境（需要调整，不要一下子导入全部类库）本项目代码基于 Python 3.x 编写。在正式编写代码之前，首先需要导入一些必要的依赖类库。1234567891011121314151617181920212223#load packagesimport sys #access to system parameters https://docs.python.org/3/library/sys.htmlprint("Python version: &#123;&#125;". format(sys.version))import pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like featuresprint("pandas version: &#123;&#125;". format(pd.__version__))import matplotlib #collection of functions for scientific and publication-ready visualizationprint("matplotlib version: &#123;&#125;". format(matplotlib.__version__))import numpy as np #foundational package for scientific computingprint("NumPy version: &#123;&#125;". format(np.__version__))import scipy as sp #collection of functions for scientific computing and advance mathematicsprint("SciPy version: &#123;&#125;". format(sp.__version__)) import sklearn #collection of machine learning algorithmsprint("scikit-learn version: &#123;&#125;". format(sklearn.__version__))#misc librariesimport randomimport time 如果这些类库没有安装的话，需要先安装，除了 Numpy 其他都可以参照以下命令安装：1pip install scikit-learn Numpy安装最好到网站 Python Extension Packages 上下载安装。要先安装 Numpy 再安装其他类库。pip install scikit-learn STEP 3.2. 基本分析接下来要正式开始接触数据了。首先我们需要对数据集有一些基本的了解。比如说数据集看起来是什么样子的，稍微描述一下。其中有哪些特征？ 每个特征大概起到什么样的作用？特征之间的依赖关系？ Survived，是输出变量。1表示生还，0表示丧生。这个变量的所有值都需要我们进行预测。而除了这个变量以外的所有变量都可能是潜在的能够影响预测值的变量。需要注意的是，并不是说用于预测的变量越多，训练的模型就越好，而是正确的变量才对模型有益。 PassengerId 和 Ticket，这两个变量是随机生成的唯一值，因此这两个值对结果变量不会有影响，因此在后续分析时，可以排除这两个变量。 Pclass，是一个有序序列，用于表示社会经济地位，1表示上层人士；2表示中层；3表示底层。 Name，是一个名词类型。可以利用特征工程从中获取性别，利用姓氏可以获得家庭成员数量，以及从称呼中可以分析出经济地位，比如XXX医生。不过这些变量目前已经明确的知道了，因此只需要通过称呼了解这个乘客是不是医生等内容。 Sex 和 Embarked，也是名词变量。在后续的计算中会被转换成数值参与计算。 Age 和 Fare，是连续型数值。 Sibsp 和 Parch，分别表示在船上的兄弟姐妹、配偶以及父母、子女的数量。这两个都是离散型的整数值。这两个值可以用于特征工程，来创建一个家庭成员数量的变量。 Cabin，是一个名词变量，可以用于特征工程中发现当事故发生时，其位于船上的位置，以及距离甲板的距离。然而，这一列很多都是空值，因此也不能用于分析。 我们导入数据，然后使用 sample() 方法来快速的观察一下数据。 123456data_raw = pd.read_csv('./input/train.csv') # 读取训练集data_val = pd.read_csv('./input/test.csv') # 读取测试集print(data_raw.head()) # 获取数据前五条print(data_raw.tail()) # 获取数据后五条print(data_raw.sample(10)) # 随机获取十条 打印结果： df_index PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 660 661 1 1 Frauenthal, Dr. Henry William male 50.0 2 0 PC 17611 133.6500 NaN S 382 383 0 3 Tikkanen, Mr. Juho male 32 0 0 STON/O 2. 3101293 7.925 S 64 65 0 1 Stewart, Mr. Albert A male 0 0 PC 17605 27.7208 C 697 698 1 3 Mullens, Miss. Katherine “Katie” female 0 0 35852 7.7333 Q 345 346 1 2 Brown, Miss. Amelia “Mildred” female 24 0 0 248733 13 F33 S 525 526 0 3 Farrell, Mr. James male 40.5 0 0 367232 7.75 Q 883 884 0 2 Banfield, Mr. Frederick James male 28 0 0 C.A./SOTON 34068 10.5 S 487 488 0 1 Kent, Mr. Edward Austin male 58 0 0 11771 29.7 B37 C 553 554 1 3 Leeni, Mr. Fahim (Philip Zenni) male 22 0 0 2620 7.225 C 241 242 1 3 Murphy, Miss. Katherine “Kate” female 1 0 367230 15.5 Q STEP 3.3. 数据清理在本节中，我们将会对数据进行清理，包括修正异常值；补全缺失值；通过分析创建新的特征；格式转换，将数据转换为便于计算和展示的格式。 1、修正检查数据，看看是否有任何看起来异常的输入。我们发现在年龄和票价两列，似乎存在异常值。不过客观的来说，这些值也是合理的，因此我们等到进行探索性数据分析时，再决定是否要剔除这两列。不过，如果这些值是一些客观上讲完全不可能的值，比如说年龄=800，那么就可能需要立马处理这个值。不过在处理这些值的时候，需要非常谨慎，因为我们需要获得一个尽可能精确的众数。 2、补全在年龄、船舱编号、和登船港口等字段都有空值或缺失值。空值对于某些算法来说是不友好的，因为它们无法处理空值。因为需要利用几种不同的算法计算不同的模型，然后进行比较，因此要在真正开始模型训练之前，处理这些空值。有两种方法处理缺失值：1）删除缺失值对应的这条记录；2）利用一些可靠的输入填充缺失值。 一般而言，不建议删除缺失值记录，特别是当缺失值占比重很大时。一种比较基础的方法是利用平均数、中位数或者平均数加上一个随机标准偏差来填充缺失值。 稍微高级一些的方法是基于特定条件的缺失值填充。比如基于座位的等次划分不同的年龄平均数进行年龄缺失值的填充。基于票价和社会经济地位来进行登船港口的填充。当然还有一些更复杂的方法，不过在此之前，应该首先建立基线模型，然后将复杂方法填充的数据训练的模型与其进行比较，以决定是否使用复杂的填充方法。 在这个数据集中，我们使用中位数来填充年龄的缺失值，而船舱这个特征会被丢弃。我们会使用一个模型来填充登船港口的缺失值，通过迭代来决定填充的值是否对模型的精度有所改善。 3、创建所谓特征工程，就是利用已有的特征来创建新的特征，并且判断这个新的特征对于模型的构建是否有促进作用。在这个数据集中，我们利用特征工程来创建一个“头衔”特征，判断角色对于生还情况是否有影响。 4、转换最后，我们要处理格式化的问题。虽然没有诸如日期、货币等字段需要格式化，但是我们有一些数据类型需要转换。比如我们的分类数据是以分类的名称表示的，无法进行数学运算。我们需要将这个数据集中的一些数据转换为可以用于数据运算的变量类型。 STEP 3.3.1. 观察数据查看训练集中数据的空值情况：1print('Train columns with null values:\n', data_raw.isnull().sum()) 结果如下。可以看到其中的年龄和船舱分别都有较多空值，登船港口存在两个空值，其余字段基本没有空值。12345678910111213Train columns with null values:PassengerId 0Survived 0Pclass 0Name 0Sex 0Age 177SibSp 0Parch 0Ticket 0Fare 0Cabin 687Embarked 2 以同样的方法查看测试集中的数据：1print('Train columns with null values:\n', data_val.isnull().sum()) 结果如下。同样是年龄和船舱两个字段存在较多空值。123456789101112Test/Validation columns with null values:PassengerId 0Pclass 0Name 0Sex 0Age 86SibSp 0Parch 0Ticket 0Fare 1Cabin 327Embarked 0 查看训练集整体数据情况。从这个统计数据中我们可以看到一些数据的分布情况，比如最大值、最小值、中位数、数据出现的频度等数据。1print(data_raw.describe(include = 'all')) Type PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked count 891.000000 891.000000 891.000000 891 891 714.000000 891.000000 891.000000 891 891.000000 204 889 unique NaN NaN NaN 891 2 NaN NaN NaN 681 NaN 147 3 top NaN NaN NaN Hirvonen, Miss. Hildur E male NaN NaN NaN CA. 2343 NaN G6 S freq NaN NaN NaN 1 577 NaN NaN NaN 7 NaN 4 644 mean 446.000000 0.383838 2.308642 NaN NaN 29.699118 0.523008 0.381594 NaN 32.204208 NaN NaN std 257.353842 0.486592 0.836071 NaN NaN 14.526497 1.102743 0.806057 NaN 49.693429 NaN NaN min 1.000000 0.000000 1.000000 NaN NaN 0.420000 0.000000 0.000000 NaN 0.000000 NaN NaN 25% 223.500000 0.000000 2.000000 NaN NaN 20.125000 0.000000 0.000000 NaN 7.910400 NaN NaN 50% 446.000000 0.000000 3.000000 NaN NaN 28.000000 0.000000 0.000000 NaN 14.454200 NaN NaN 75% 668.500000 1.000000 3.000000 NaN NaN 38.000000 1.000000 0.000000 NaN 31.000000 NaN NaN max 891.000000 1.000000 3.000000 NaN NaN 80.000000 8.000000 6.000000 NaN 512.329200 NaN NaN STEP 3.3.2. 修正将训练集拷贝一个副本，然将副本与测试集合并形成一个新的集合。12data_raw2 = data_raw.copy(deep = True)data_cleaner = [data_raw2, data_val] 后续的操作基于这个拷贝的副本进行。 在这个数据集中，通过上面的观察，并没有发现明显不符合客观事实的值，因此不需要进行处理。不过其中存在一个无用的列，包括乘客ID，船舱、船票编号，需要将这几个列删除。12drop_column = ['PassengerId','Cabin', 'Ticket']data_raw2.drop(drop_column, axis=1, inplace = True) 观察一下删除后的空值情况：12345print('Train columns with null values:\n', data_raw2.isnull().sum())print("-" * 25)print('Test/Validation columns with null values:\n', data_val.isnull().sum())print("-" * 25) 结果：123456789101112131415161718192021222324252627-------------------------Train columns with null values:Survived 0Pclass 0Name 0Sex 0Age 177SibSp 0Parch 0Fare 0Embarked 2dtype: int64-------------------------Test/Validation columns with null values:PassengerId 0Pclass 0Name 0Sex 0Age 86SibSp 0Parch 0Ticket 0Fare 1Cabin 327Embarked 0dtype: int64------------------------- STEP 3.3.3. 补全这里的 data_cleaner 只是一个引用，对其修改，会直接反映到 data_raw2 和 data_val 上。123456789101112131415for dataset in data_cleaner: # 使用中位数填充所有空值 dataset['Age'].fillna(dataset['Age'].median(), inplace = True) # 使用众数填充登船港口 dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True) # 使用中位数填充票价 dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)print('Train columns with null values:\n', data_raw2.isnull().sum())print("-" * 25)print('Test/Validation columns with null values:\n', data_val.isnull().sum())print("-" * 25) 同样的将处理后的结果打印出来看看：1234567891011121314151617181920212223242526Train columns with null values:Survived 0Pclass 0Name 0Sex 0Age 0SibSp 0Parch 0Fare 0Embarked 0dtype: int64-------------------------Test/Validation columns with null values:PassengerId 0Pclass 0Name 0Sex 0Age 0SibSp 0Parch 0Ticket 0Fare 0Cabin 327Embarked 0dtype: int64------------------------- STEP 3.3.4. 创建利用特征工程将训练集和测试集中的数据创建新的特征。这里我们根据现有的 特征分别创建几个新的特征。1、根据兄弟姐妹、配偶、父母和子女数量的两个字段来创建一个表示家庭成员数量的字段。利用家庭成员数量可以得到一个新的字段表示是否是孤身一人。2、观察姓名一列，可以看到其中有关于头衔之类的信息，从中可以看到是先生、女士、硕士、博士等等信息。3、将船票费用和年龄划分为几个不同的区间。 1234567891011for dataset in data_cleaner: dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1 dataset['IsAlone'] = 1 dataset['IsAlone'].loc[dataset['FamilySize'] &gt; 1] = 0 dataset['Title'] = dataset['Name'].str.split(", ", expand=True)[1].str.split(".", expand=True)[0] dataset['FareBin'] = pd.qcut(dataset['Fare'], 4) dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5) 查看增加了几个特征之后的数据： df_index Survived Pclass Name Sex Age SibSp Parch Fare Embarked FamilySize IsAlone Title FareBin AgeBin 205 0 3 Strom, Miss. Telma Matilda female 2.0 0 1 10.4625 S 2 0 Miss (7.91, 14.454] (-0.08, 16.0] 879 1 1 Potter, Mrs. Thomas Jr (Lily Alexenia Wilson) female 56.0 0 1 83.1583 C 2 0 Mrs (31.0, 512.329] (48.0, 64.0] 329 1 1 Hippach, Miss. Jean Gertrude female 16.0 0 1 57.9792 C 2 0 Miss (31.0, 512.329] (-0.08, 16.0] 136 1 1 Newsom, Miss. Helen Monypeny female 19.0 0 2 26.2833 S 3 0 Miss (14.454, 31.0] (16.0, 32.0] 57 0 3 Novel, Mr. Mansouer male 28.5 0 0 7.2292 C 1 1 Mr (-0.001, 7.91] (16.0, 32.0] 848 0 2 Harper, Rev. John male 28.0 0 1 33.0 S 2 0 Rev (31.0, 512.329] (16.0, 32.0] 151 1 1 Pears, Mrs. Thomas (Edith Wearne) female 22.0 1 0 66.6 S 2 0 Mrs (31.0, 512.329] (16.0, 32.0] 659 0 1 Newell, Mr. Arthur Webster male 58.0 0 2 113.275 C 3 0 Mr (31.0, 512.329] (48.0, 64.0] 228 0 2 Fahlstrom, Mr. Arne Jonas male 18.0 0 0 13.0 S 1 1 Mr (7.91, 14.454] (16.0, 32.0] 371 0 3 Wiklund, Mr. Jakob Alfred male 18.0 1 0 6.4958 S 2 0 Mr (-0.001, 7.91] (16.0, 32.0] 对于头衔这一列，由于是拆分字符串，因此对于结果需要着重检查一下，打印这一列的统计信息：1print(data_raw2['Title'].value_counts()) 观察结果：1234567891011121314151617Mr 517Miss 182Mrs 125Master 40Dr 7Rev 6Mlle 2Col 2Major 2the Countess 1Jonkheer 1Capt 1Mme 1Ms 1Sir 1Lady 1Don 1 我们发现，拆分出来的头衔有很多种，不过其中占比比较高的就是前面四个，后面的一些数量都太少，因此我们打算将后面的一些都合并成一个。1234stat_min = 10title_names = (data_raw2['Title'].value_counts() &lt; stat_min)data_raw2['Title'] = data_raw2['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)print(data_raw2['Title'].value_counts()) 再次观察处理后的结果：12345Mr 517Miss 182Mrs 125Master 40Misc 27 发现头衔这一列已经全部处理为这几类，对于数量比较少的，都归入到 Misc 中。 STEP 3.3.5. 转换接下来我们需要将分类数据转换为离散特征编码用于数学分析。另外，在本环节中，我们还会定义用于训练模型的 x （输入）和 y （输出）变量。 首先利用标签编码将几个离散型的列转换为标签。1234567label = LabelEncoder()for dataset in data_cleaner: dataset['Sex_Code'] = label.fit_transform(dataset['Sex']) dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked']) dataset['Title_Code'] = label.fit_transform(dataset['Title']) dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin']) dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin']) df_index Other Columns Sex_Code Embarked_Code Title_Code AgeBin_Code FareBin_Code 312 … 0 2 4 1 2 394 … 0 2 4 1 2 75 … 1 2 3 1 0 794 … 1 2 3 1 0 348 … 1 2 0 0 2 330 … 0 1 2 1 2 517 … 1 1 3 1 2 203 … 1 0 3 2 0 573 … 0 1 2 1 0 176 … 1 2 0 1 2 将其中的一些离散型变量转为独热编码：123456789101112131415161718192021# 定义输出变量 yTarget = ['Survived']# 定义输入变量 xdata_raw2_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone']data_raw2_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare']data_raw2_xy = Target + data_raw2_xprint('Original X Y: ', data_raw2_xy, '\n')# 定义离散输入变量 xdata_raw2_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']data_raw2_xy_bin = Target + data_raw2_x_binprint('Bin X Y: ', data_raw2_xy_bin, '\n')# 将离散型变量转为独热编码data_raw2_dummy = pd.get_dummies(data_raw2[data_raw2_x])data_raw2_x_dummy = data_raw2_dummy.columns.tolist()data_raw2_xy_dummy = Target + data_raw2_x_dummyprint('Dummy X Y: ', data_raw2_xy_dummy, '\n')print(data_raw2_dummy.sample(10)) 输出结果： df_index Pclass SibSp Parch Age Fare FamilySize IsAlone Sex_female Sex_male Embarked_C Embarked_Q Embarked_S Title_Master Title_Misc Title_Miss Title_Mr Title_Mrs 154 3 0 0 28.0 7.3125 1 1 0 1 0 0 1 0 0 0 1 0 317 2 0 0 54.0 14.0 1 1 0 1 0 0 1 0 1 0 0 0 216 3 0 0 27.0 7.925 1 1 1 0 0 0 1 0 0 1 0 0 177 1 0 0 50.0 28.7125 1 1 1 0 1 0 0 0 0 1 0 0 514 3 0 0 24.0 7.4958 1 1 0 1 0 0 1 0 0 0 1 0 552 3 0 0 28.0 7.8292 1 1 0 1 0 1 0 0 0 0 1 0 785 3 0 0 25.0 7.25 1 1 0 1 0 0 1 0 0 0 1 0 324 3 8 2 28.0 69.55 11 0 0 1 0 0 1 0 0 0 1 0 135 2 0 0 23.0 15.0458 1 1 0 1 1 0 0 0 0 0 1 0 79 3 0 0 30.0 12.475 1 1 1 0 0 0 1 0 0 1 0 0 STEP 3.3.6. 拆分训练集前面提到项目给定了两个数据集，一个是训练集，一个是测试集。但是在真正开始进行模型训练时，我们需要将这个训练集拆分为两部分，使用其中一部分进行模型训练，另一部分进行验证，以防止过拟合。 1234567train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data_raw2[data_raw2_x_calc], data_raw2[Target], random_state = 0)train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data_raw2[data_raw2_x_bin], data_raw2[Target] , random_state = 0)train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data_raw2_dummy[data_raw2_x_dummy], data_raw2[Target], random_state = 0)print("data_raw2 Shape: &#123;&#125;".format(data_raw2.shape))print("Train1 Shape: &#123;&#125;".format(train1_x.shape))print("Test1 Shape: &#123;&#125;".format(test1_x.shape)) STEP 4. 探索性数据分析现在已经完成了数据清理，我们需要利用统计和图示工具来找出分类特征，已经判断其与输出或其他特征之间的相关性。 STEP 4.1. 单变量相关性分析STEP 4.1.1. 基于数值统计的分析对离散型特征分别进行分组统计，分析不同类型的值与生还情况之间的关系。 12345for x in data_raw2_x: if data_raw2[x].dtype != 'float64' : print('Survival Correlation by:', x) print(data_raw2[[x, Target[0]]].groupby(x, as_index=False).mean()) print('-'*10, '\n') 生还情况的关系*生还者程：将数据按照性别分为两组，分别求每组 Survived 的平均值，由于生还者标志为 1，丧生为 0，则该平均数正好可以表示每个分组中生还者占的百分比。1234Survival Correlation by: Sex Sex Survived0 female 0.7420381 male 0.188908 **根据该统计结果，发现女性中，生还者占 74% 左右，而男性中的生还者占比只有 19% 左右。 座位等级与生还情况与座次关系：一等座中有 62% 的生还者；二等座则有 47% 的生还者；三等座只有 24% 的生还者。因此坐席等级越高，其存活可能性就相对越高。12345Survival Correlation by: Pclass Pclass Survived0 1 0.6296301 2 0.4728262 3 0.242363 生还率与登船港口关系：从 Cherbourg 登船的乘客生还率要显著高于其他两个港口登船的乘客。12345Survival Correlation by: Embarked Embarked Survived0 C 0.5535711 Q 0.3896102 S 0.339009 生还率与头衔之间的关系：1234567Survival Correlation by: Title Title Survived0 Master 0.5750001 Misc 0.4444442 Miss 0.6978023 Mr 0.1566734 Mrs 0.792000 生还率与兄弟姐妹和配偶数量关系：我们发现在船上拥有 1 到 2 名兄弟姐妹或配偶的人员生存率比较高，达到 53% 和 46%，其余都很低。123456789Survival Correlation by: SibSp SibSp Survived0 0 0.3453951 1 0.5358852 2 0.4642863 3 0.2500004 4 0.1666675 5 0.0000006 8 0.000000 生还率与父母子女数量关系：通船上父母与子女的数量与生存情况关系，有1-3人的生存率最高，在 50~60%之间。123456789Survival Correlation by: Parch Parch Survived0 0 0.3436581 1 0.5508472 2 0.5000003 3 0.6000004 4 0.0000005 5 0.2000006 6 0.000000 生还率与家庭成员数量关系：总的家庭成员在 4 人左右的存活率较高，可能是什么原因？或许可以分析一下这些存货下来的人员对应的家庭成员，特别是男性家庭成员的存货情况。1234567891011Survival Correlation by: FamilySize FamilySize Survived0 1 0.3035381 2 0.5527952 3 0.5784313 4 0.7241384 5 0.2000005 6 0.1363646 7 0.3333337 8 0.0000008 11 0.000000 生还率与是否独身一人的关系：1234Survival Correlation by: IsAlone IsAlone Survived0 0 0.5056501 1 0.303538 STEP 4.1.2. 基于箱线图和直方图的分析票价分布情况与生还情况关系123456789101112plt.figure(figsize=[16,12])plt.subplot(121)plt.boxplot(x=data_raw2['Fare'], showmeans = True, meanline = True)plt.title('Fare Boxplot')plt.ylabel('Fare ($)')plt.subplot()pla_e'], ')plt.label(')plt.show() 结果： 观察票价分布的箱线图，可以看出来票价的主要分布区间大概在 5 ~ 40 之间，上限在 80 左右，票价中存在部分离群点，应该不是异常值，是属于比较高档的票价。 观察票价与生还情况直方图，可以看出来在 0 ~ 50 票价的乘客存活率相对较低，大概只有 30% 左右，而票价越高，其存活率基本上是不断上升的。 年龄分布情况与生还情况关系 12345678910111213plt.figure(figsize=[16,12])plt.subplot(233121)plt.boxplot(data_raw2['e'], showmeans = True, meanline = True)plt.title('e Boxplot')plt.ylabel('Age (Years)')plt.subplot()plt.hist(x = [data_raw2[data_raw2['Survived']==1]['FarAge'], data_raw2[data_raw2['Survived']==0]['FarAge']], stacked=True, color = ['g','r'],label = ['Survived','Dead'])plt.title('FarAge Histogram by Survival')plt.xlabel('Fare ($Age (Years)')plt.ylabel('# of Passengers')plt.legend() 观察年龄分布的箱线图，可以看出来年龄的主要分布区间大概在 20 ~ 35 之间，大部分应该属于青壮年。存在部分离群点，可能是年龄较大的老年人和幼儿。 观察年龄与生还情况直方图，可以看出来在 30 岁区间的乘客存活率相对较低，低于30% ，其中很多人可能是牺牲了自己把求生的机会让给别人了。其余年龄分布的存活率也都相对不高，只有幼儿，青少年和50岁以上的老人存活率较高。 家庭成员数量分布情况与生还情况关系1234567891011121314151617plt.figure(figsize=[16,12])plt.subplot(235121)plt.hist(x = [data_raw2[data_raw2['Survived']==1]['Age'], data_raw2[data_raw2['Survived']==0]['Agboxplot(data_raw2['FamilySize']], stacked=True, color = ['g','r'],label = ['Survived','Dead'])plt.title('Age Histogram by Survivalshowmeans = True, meanline = True)plt.title('Family Size Boxplot')plt.xylabel('Age (Years)')plt.ylabel('# of Passengers')plt.legend(Family Size (#)')plt.subplot(236122)plt.hist(x = [data_raw2[data_raw2['Survived']==1]['FamilySize'], data_raw2[data_raw2['Survived']==0]['FamilySize']], stacked=True, color = ['g','r'],label = ['Survived','Dead'])plt.title('Family Size Histogram by Survival')plt.xlabel('Family Size (#)')plt.ylabel('# of Passengers')plt.legend() 观察家庭成员数量分布的箱线图，可以看出来家庭成员数量的主要分布区间大概在 1 ~ 2 之间。 观察家庭成员数量与生还情况直方图，可以看出来在 2 ~ 5 区间的乘客存活率相对较高，高于50% ，可能这个数量的家庭成员相对比较集中，并且能够相互帮助求生。 STEP 4.1.3. 基于直方图的分析以图形化的方式展示一开始采用的数学统计的分析结果，按照分组展示每个特征不同值对应的生还率。 12345fig, saxis = plt.subplots(, 3,figsize=(16,12))sns.barplot(x = 'Embarked', y = 'Survived', data=data_raw2, ax = saxis[])sns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data_raw2, ax = saxis[1])sns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data_raw2, ax = saxis[0,2]) STEP 4.1.4. 基于折线图的分析分析票价区间、年龄区间与家庭成员数量与生还率之间的关系，通过折线图比较直观的看出彼此之间的区别。基本上票价越高的，存活率越高；而年龄主要是16所以下的青少年和 48 ~ 64 岁之间的老年存活率较高。1234fig, saxis = plt.subplots(1, 3,figsize=(16,12))sns.pointplot(x = 'FareBin', y = 'Survived', data=data_raw2, ax = saxis[0])sns.pointplot(x = 'AgeBin', y = 'Survived', data=data_raw2, ax = saxis[1])sns.pointplot(x = 'FamilySize', y = 'Survived', data=data_raw2, ax = saxis[2]) STEP 4.2. 双变量相关性分析STEP 4.2.1. 座位等次与票价、年龄家庭规员数量与生还情况的关系12345678910fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))sns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data_raw2, ax = axis1)axis1.set_title('Pclass vs Fare Survival Comparison')sns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data_raw2, split = True, ax = axis2)axis2.set_title('Pclass vs Age Survival Comparison')sns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = data_raw2, ax = axis3)axis3.set_title('Pclass vs Family Size Survival Comparison') 从图中我们可以发下以下一些内容： 一等座中票价较高的乘客存活率较高，而二等座和三等座中票价对存活率没有太大影响； 一等座中存活下来的基本上是年龄较大的，而二等座青少年存活率较高，三等座也是类似； 一等座中家庭成员数量基本在 2 人左右，存活率相仿，二等座和三等座家庭成员较多，存活率较高的基本都分布在 2 ~ 3 人左右； STEP 4.2.2. 性别和登船港口、座位等次、是否独自一人与生还情况的关系12345678910fig, qaxis = plt.subplots(1,3,figsize=(14,12))sns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data_raw2, ax = qaxis[0])axis1.set_title('Sex vs Embarked Survival Comparison')sns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data_raw2, ax = qaxis[1])axis1.set_title('Sex vs Pclass Survival Comparison')sns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data_raw2, ax = qaxis[2])axis1.set_title('Sex vs IsAlone Survival Comparison') 利用折线图：1234567891011fig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))#how does family size factor with sex &amp; survival comparesns.pointplot(x="FamilySize", y="Survived", hue="Sex", data=data_raw2, palette=&#123;"male": "blue", "female": "pink"&#125;, markers=["*", "o"], linestyles=["-", "--"], ax = maxis1)#how does class factor with sex &amp; survival comparesns.pointplot(x="Pclass", y="Survived", hue="Sex", data=data_raw2, palette=&#123;"male": "blue", "female": "pink"&#125;, markers=["*", "o"], linestyles=["-", "--"], ax = maxis2) 1234a = sns.FacetGrid( data_raw2, hue = 'Survived', aspect=4 )a.map(sns.kdeplot, 'Age', shade= True )a.set(xlim=(0 , data_raw2['Age'].max()))a.add_legend() STEP 4.2.3. 相关热力图123456789101112131415161718def correlation_heatmap(df): _ , ax = plt.subplots(figsize =(14, 12)) colormap = sns.diverging_palette(220, 10, as_cmap = True) _ = sns.heatmap( df.corr(), cmap = colormap, square=True, cbar_kws=&#123;'shrink':.9 &#125;, ax=ax, annot=True, linewidths=0.1,vmax=1.0, linecolor='white', annot_kws=&#123;'fontsize':12 &#125; ) plt.title('Pearson Correlation of Features', y=1.05, size=15)correlation_heatmap(data_raw2) STEP 5. 模型数据接下来到了我们使用算法结合数据训练模型的阶段了。首先我们要面临的一个问题就是如何选择合适的算法。 STEP 5.1. 如何选择一个机器学习算法一般来说在选择机器学习算法中有一个很著名的理论，No Free Lunch Theorem，没有免费的午餐定理。就是说没有一个算法能够适用于所有问题，因此针对不同的问题需要使用不同的算法处理。 下面使用一些主流算法分别训练模型，然后观察结果。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283MLA = [ # Ensemble Methods ensemble.AdaBoostClassifier(), ensemble.BaggingClassifier(), ensemble.ExtraTreesClassifier(), ensemble.GradientBoostingClassifier(), ensemble.RandomForestClassifier(), # Gaussian Processes gaussian_process.GaussianProcessClassifier(), # GLM linear_model.LogisticRegressionCV(), linear_model.PassiveAggressiveClassifier(), linear_model.RidgeClassifierCV(), linear_model.SGDClassifier(), linear_model.Perceptron(), # Navies Bayes naive_bayes.BernoulliNB(), naive_bayes.GaussianNB(), # Nearest Neighbor neighbors.KNeighborsClassifier(), # SVM svm.SVC(probability=True), svm.NuSVC(probability=True), svm.LinearSVC(), # Trees tree.DecisionTreeClassifier(), tree.ExtraTreeClassifier(), # Discriminant Analysis discriminant_analysis.LinearDiscriminantAnalysis(), discriminant_analysis.QuadraticDiscriminantAnalysis(), XGBClassifier() ]#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit#note: this is an alternative to train_test_splitcv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%#create table to compare MLA metricsMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']MLA_compare = pd.DataFrame(columns = MLA_columns)#create table to compare MLA predictionsMLA_predict = data_raw2[Target]#index through MLA and save performance to tablerow_index = 0for alg in MLA: #set name and parameters MLA_name = alg.__class__.__name__ MLA_compare.loc[row_index, 'MLA Name'] = MLA_name MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params()) #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate cv_results = model_selection.cross_validate(alg, data_raw2[data_raw2_x_bin], data_raw2[Target], cv = cv_split) MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean() MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean() MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean() #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3 #let's know the worst that can happen! #save MLA predictions - see section 6 for usage alg.fit(data_raw2[data_raw2_x_bin], data_raw2[Target]) MLA_predict[MLA_name] = alg.predict(data_raw2[data_raw2_x_bin]) row_index+=1 #print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.htmlMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)print(MLA_compare) 结果按照测试准确率从高到低排序： # MLA Name MLA Train Accuracy Mean MLA Test Accuracy Mean MLA Test Accuracy 3*STD MLA Time 21 XGBClassifier 0.856367041 0.829477612 0.052754649 0.039702296 14 SVC 0.837265918 0.826119403 0.045387616 0.041802382 2 ExtraTreesClassifier 0.895131086 0.823134328 0.070477953 0.019001102 3 GradientBoostingClassifier 0.866666667 0.822761194 0.04987314 0.076404333 15 NuSVC 0.83576779 0.822761194 0.049368083 0.053803039 17 DecisionTreeClassifier 0.895131086 0.820895522 0.05321581 0.003500175 4 RandomForestClassifier 0.8917603 0.820522388 0.077465598 0.021201277 1 BaggingClassifier 0.89082397 0.817910448 0.07621811 0.016200852 13 KNeighborsClassifier 0.850374532 0.81380597 0.069086302 0.005700326 18 ExtraTreeClassifier 0.895131086 0.813432836 0.047227664 0.003000212 0 AdaBoostClassifier 0.820411985 0.811940299 0.049860576 0.058903384 5 GaussianProcessClassifier 0.871722846 0.810447761 0.049253731 0.156008911 20 QuadraticDiscriminantAnalysis 0.821535581 0.807089552 0.081038901 0.005000305 8 RidgeClassifierCV 0.796629213 0.794029851 0.036030172 0.008900499 19 LinearDiscriminantAnalysis 0.796816479 0.794029851 0.036030172 0.007000422 16 LinearSVC 0.797565543 0.792910448 0.041053256 0.036402035 6 LogisticRegressionCV 0.797003745 0.790671642 0.065358182 0.127807283 12 GaussianNB 0.794756554 0.781343284 0.087456828 0.005600309 11 BernoulliNB 0.78576779 0.775373134 0.057034653 0.005900288 9 SGDClassifier 0.73670412 0.739179104 0.123788965 0.009600472 10 Perceptron 0.740074906 0.728731343 0.162220766 0.009200573 7 PassiveAggressiveClassifier 0.699812734 0.682462687 0.352227936 0.010100603 以图形化的方式将结果展示出来：123456789plt.figure(figsize=[16,12])#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.htmlsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')#prettify using pyplot: https://matplotlib.org/api/pyplot_api.htmlplt.title('Machine Learning Algorithm Accuracy Score \n')plt.xlabel('Accuracy Score (%)')plt.ylabel('Algorithm') STEP 5.2. 选择基线在进行后续的模型优化之前，我们首先需要判断目前所拥有的模型是否值得继续优化，因此我们要确定一个基线。 我们知道这是一个二分类问题，因此无论如何，就算是猜，最差也应该能够有 50% 的准确率。当然，这个基线是在对于具体的项目和数据信息一无所知的情况下确定的，但事实上我们对这个数据集有一定的了解。 我们知道在这个事故中，2224 人中有 1502 人丧生，也就是 67.5%。如果我们就单纯的猜测所有人全部丧生，那么也会有 67.5% 的准确率。因此我们的准确率要大于这个值才有价值，我们将这个基线设置为 68%。 参考[1] https://www.kaggle.com/c/titanic[2] https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy/notebook]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Matplotlib</tag>
        <tag>Python</tag>
        <tag>Pandas</tag>
        <tag>Kaggle</tag>
        <tag>Data Science</tag>
        <tag>SKLearn</tag>
        <tag>Feature Selection</tag>
        <tag>Cross Validation</tag>
        <tag>Seaborn</tag>
        <tag>Titanic</tag>
        <tag>Data wrangling</tag>
        <tag>Explorator Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于生产者消费者模式实现数据从 MongoDB 导入 HBase]]></title>
    <url>%2F2018%2F11%2F22%2F%E5%9F%BA%E4%BA%8E%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%BC%8F%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E4%BB%8E-MongoDB-%E5%AF%BC%E5%85%A5-HBase%2F</url>
    <content type="text"><![CDATA[摘要目前有数千万的数据存储在 MongoDB 中，单台机器、单库单表。要使用 Spark 处理，所以打算先转到 HBase 中。使用生产者消费者模式，利用多线程实现该功能。 主要内容数据是爬虫爬取的一些问答数据，存储在 MongoDB 中，大概六七千万条，一百五十多G。单库单表。 由于数据读取和写入肯定是有速度差的，因此使用生产者消费者模式，利用线程进行均衡。采用三段式，分给读取数据ID、数据内容、写入。 读取 ID由于数据量较大，肯定不能一次读取，因此需要采用类似分页的方式读取。MongoDB 中分页有两种方式，一种是 skip，直接跳过一定数量的数据，但这种方式如果要跳过较多数据量时性能很差。 因此采取设置起始行的方式进行分页。也就是读取 N 行数据，然后以第一批数据的最后一行为起点，读取下一批 N 行数据，这种方式的效率较高。 将所有读取的数据放入一个队列中，供后续线程处理。 读取内容另外开辟线程读取 ID 后从 MongoDB 中读取数据内容。当初这样考虑的原因是考虑到读取数据和解析可能会耗费一些时间，因此可以多开两个线程处理这部分功能。不过实际测试发现，这部分处理的速度仍然非常快，基本上一万条数据在两百毫秒左右。 这部分读取的内容封装成对象之后，再放入一个队列中，供写入线程使用。 写入写入是一个比较耗费时间的过程，一开始使用生产中消费者模式也是出于这个考虑。不过实际测试下来，基本上读取 ID 和内容都只要一个线程即可，而写入数据则需要多开辟几个线程，其速度是相当慢。 并且对于提高 HBase 写入性能还需要专门研究，我觉得这里还有很大的提升空间。 性能分析通过日志记录对数据读写进行分析。其中读取部分主要是读取完整内容，单单读取ID的速度非常快，可以忽略。 读写数据分别在两组不同的机器上测试：1、MongoDB 所在 I5-8500 16G，HBase 是三台 1C 4G KVM。2、MongoDB 所在 I7-8700K 64G，HBase 是五台 1C 16G KVM。 第一组机器以 250W 条数据进行测试。 可以看到在 60W 左右，读取速度骤然下降： 推测这里应该是与 MongoDB 数据存储机制相关。MongDB 会将一部分数据缓存在内存中，写不下的会放到文件中。前面速度快的部分应该就是从内存中读取，后面速度骤然下降，就是从文件中读取了。 调研过 MongoDB 的数据缓存策略，是通过数据访问频繁程度控制的，无法进行手动调整，因此优化这部分性能的方法就在于提高物理内存了。该猜想可以在第二组机器测试结果里面得到验证。第二组机器内存较大，全程读取速度都非常快。 至于写入速度，就相对较慢了。主要瓶颈也在于此。开了三个线程，如果开更多，就会有部分写入线程被挂起，速度没有提升。HBase 的写入机制需要研究下，难道是每个 RegionServer 支持一个写入？如果是这样，写入的性能也不是很高啊。 第二组机器以 150W 条数据进行测试。 可以看到，其读取速度一直很稳定，相当快。 写入速度也要快一些，是因为多了两台 RegionServer 的原因么？ 分析脚本绘制分析折线图采用的是 Python 的 Seaborn 类库，需要安装 Python 环境。 脚本代码12345678910111213141516171819import pandas as pddf = pd.read_csv("250W_read_70W.csv")ds = df[df['data/W'] % 50000 == 0]ds['data/W'] = ds['data/W'] / 10000ds['time/Min'] = ds['time/Min'] / 60000import seaborn as snsimport matplotlib.pyplot as pltplt.rcParams['font.sans-serif'] =['Microsoft YaHei']f, ax= plt.subplots(figsize = (14, 8))ax.set_title('Read-70W Speed')sns.set_style("darkgrid") sns.pointplot(x="data/W",y="time/Min",data=ds)plt.show() 项目介绍项目采用基于 Maven 的 Java 控制台项目。 依赖项目基于 JDK 1.8，另外主要还用到了 MongoDB 驱动、HBase Client、日志等类库。其 Maven 依赖如下： 1234567891011121314151617181920212223242526272829303132333435&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mongodb&lt;/groupId&gt; &lt;artifactId&gt;mongo-java-driver&lt;/artifactId&gt; &lt;version&gt;3.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.25&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-core&lt;/artifactId&gt; &lt;version&gt;1.2.3&lt;/version&gt;&lt;/dependency&gt; Resources 目录core-site.xml &amp; hbase-site.xml为了能够访问 HBase，需要在项目的 classpath 中加入目标集群的这两个文件。分别从 Hadoop 和 HBase 部署的配置目录中拷贝这两个文件到项目的 resources 目录中。 log4j.properties日志配置文件 1234567891011121314151617# Set root logger level to DEBUG and its only appender to A1.log4j.rootLogger=INFO, STO, F# A1 is set to be a ConsoleAppender.log4j.appender.STO=org.apache.log4j.ConsoleAppender# A1 uses PatternLayout.log4j.appender.STO.layout=org.apache.log4j.PatternLayoutlog4j.appender.STO.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss&#125; [%t] %-5p %c %x - %m%nlog4j.appender.F=org.apache.log4j.DailyRollingFileAppenderlog4j.appender.F.File=./log.loglog4j.appender.F.layout=org.apache.log4j.PatternLayoutlog4j.appender.F.layout.ConversionPattern=%d&#123;yyyy-MM-dd HH:mm:ss&#125; [%t] %-5p %c %x - %m%nlog4j.logger.com.oolong=DEBUG#log4j.logger.org.mongodb=ERROR logback.xml日志配置文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!--debug="true" : 打印logback内部状态（默认当logback运行出错时才会打印内部状态 ），配置该属性后打印条件如下（同时满足）： 1、找到配置文件 2、配置文件是一个格式正确的xml文件 也可编程实现打印内部状态，例如： LoggerContext lc = (LoggerContext) LoggerFactory.getILoggerFactory(); StatusPrinter.print(lc); --&gt;&lt;!-- scan="true" ： 自动扫描该配置文件，若有修改则重新加载该配置文件 --&gt;&lt;!-- scanPeriod="30 seconds" : 配置自动扫面时间间隔（单位可以是：milliseconds, seconds, minutes or hours，默认为：milliseconds）， 默认为1分钟，scan="true"时该配置才会生效 --&gt;&lt;configuration debug="false" scan="true" scanPeriod="30 seconds" packagingData="true"&gt; &lt;!-- 设置 logger context 名称,一旦设置不可改变，默认为default --&gt; &lt;contextName&gt;MedicalQATransfer&lt;/contextName&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;!-- encoders are by default assigned the type ch.qos.logback.classic.encoder.PatternLayoutEncoder --&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- 当前活动日志文件名 --&gt; &lt;file&gt;./medicalqatransfer.log&lt;/file&gt; &lt;!-- 文件滚动策略根据%d&#123;patter&#125;中的“patter”而定，此处为每天产生一个文件 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;!-- 归档文件名“.zip或.gz结尾”,表示归档文件自动压缩 --&gt; &lt;FileNamePattern&gt;./medicalqatransfer%d&#123;yyyyMMdd&#125;.log.zip&lt;/FileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;[%-5level][%thread]%logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;!-- &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125;[%-5level][%thread] - %msg%n&lt;/pattern&gt; --&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;logger name="com.oolong" level="DEBUG"&gt; &lt;appender-ref ref="STDOUT" /&gt; &lt;appender-ref ref="FILE" /&gt; &lt;/logger&gt; &lt;!--&lt;logger name="org.mongodb" level="ERROR"&gt;--&gt; &lt;!--&lt;appender-ref ref="STDOUT" /&gt;--&gt; &lt;!--&lt;appender-ref ref="FILE" /&gt;--&gt; &lt;!--&lt;/logger&gt;--&gt; &lt;!-- 至多只能配置一个root --&gt; &lt;root level="debug"&gt; &lt;appender-ref ref="STDOUT" /&gt; &lt;appender-ref ref="FILE" /&gt; &lt;/root&gt;&lt;/configuration&gt; 程序入口App12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.oolong;import com.oolong.model.QAModel;import com.oolong.read.ReadDocumentsTask;import com.oolong.read.ReadIdsTask;import com.oolong.write.WriteTask;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.File;import java.io.IOException;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.atomic.AtomicInteger;import java.util.concurrent.atomic.AtomicLong;public class App &#123; public static void main( String[] args ) throws IOException &#123; // 以下四行代码解决该错误，原因未知 // ERROR org.apache.hadoop.util.Shell - Failed to locate the winutils binary in the hadoop binary path String path = new File(".").getCanonicalPath(); System.getProperties().put("hadoop.home.dir", path); new File("./bin").mkdirs(); new File("./bin/winutils.exe").createNewFile(); Logger logger = LoggerFactory.getLogger(App.class); LinkedBlockingQueue&lt;String&gt; idPool = new LinkedBlockingQueue&lt;&gt;(); AtomicInteger count = new AtomicInteger(0); AtomicInteger savedCount = new AtomicInteger(0); AtomicLong start = new AtomicLong(System.currentTimeMillis()); logger.info("Create Thread: Load Ids from MongoDB."); Thread idReadTask = new Thread(new ReadIdsTask(idPool, 1000000)); idReadTask.start(); logger.info("Starting Thread: Load Ids from MongoDB..."); logger.info("Create Thread: Load QAData from MongoDB."); LinkedBlockingQueue&lt;QAModel&gt; questionPool = new LinkedBlockingQueue&lt;&gt;(); ExecutorService docReadTask = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 1; i++) &#123; docReadTask.execute(new ReadDocumentsTask(idPool, questionPool , 100, count, start)); &#125; logger.info("Starting Thread: Load QAData from MongoDB..."); ExecutorService docSaveTask = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 3; i++) &#123; docReadTask.execute(new WriteTask(questionPool , 1000, savedCount, start)); &#125; logger.info("Starting Thread: Save QAData to HBase..."); &#125;&#125; 主要逻辑ReadIdsTask 读取 ID123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081package com.oolong.read;import com.mongodb.client.FindIterable;import com.mongodb.client.MongoCollection;import com.mongodb.client.MongoCursor;import com.oolong.utils.MongoUtil;import org.bson.Document;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.ArrayList;import java.util.List;import java.util.concurrent.LinkedBlockingQueue;/** * 从 MongoDB 中读取 ID，并加入队列中。 * 单线程工作，不支持多线程。 */public class ReadIdsTask implements Runnable &#123; private LinkedBlockingQueue&lt;String&gt; idPool; private int capacity; private int batchNum; private String start; private Logger logger; public ReadIdsTask(LinkedBlockingQueue&lt;String&gt; idPool, int capacity) &#123; this.idPool = idPool; this.capacity = capacity; this.start = ""; this.batchNum = 10000; this.logger = LoggerFactory.getLogger(ReadIdsTask.class); &#125; @Override public void run() &#123; MongoCollection&lt;Document&gt; collection = MongoUtil.getCollection(); while (true) &#123; try &#123; if (capacity - idPool.size() &lt; batchNum) &#123; Thread.sleep(1000); continue; &#125; List&lt;String&gt; ids = this.readBatchIds(collection, start, batchNum); if (ids.size() &gt; 0) &#123; this.start = ids.get(ids.size() - 1); idPool.addAll(ids); logger.info("当前待处理ID: " + idPool.size()); &#125; else &#123; logger.info("完成所有Id加载，结束当前任务。"); break; &#125; &#125; catch (InterruptedException e) &#123; logger.debug(e.getMessage()); &#125; &#125; &#125; private List&lt;String&gt; readBatchIds(MongoCollection&lt;Document&gt; collection, String start, int num) &#123; FindIterable&lt;Document&gt; findIterable = null; Document fieldFilter = new Document("_id", 1); if (!start.isEmpty()) &#123; Document condition = new Document("_id", new Document("$gt", start)); findIterable = collection.find(condition).projection(fieldFilter).limit(num); &#125; else &#123; findIterable = collection.find().projection(fieldFilter).limit(num); &#125; MongoCursor&lt;Document&gt; mongoCursor = findIterable.iterator(); List&lt;String&gt; res = new ArrayList&lt;&gt;(); while(mongoCursor.hasNext())&#123; res.add(mongoCursor.next().getString("_id")); &#125; return res; &#125;&#125; ReadDocumentsTask 读取内容123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105package com.oolong.read;import com.mongodb.client.FindIterable;import com.mongodb.client.MongoCollection;import com.mongodb.client.MongoCursor;import com.oolong.model.Answer;import com.oolong.model.QAModel;import com.oolong.utils.MongoUtil;import org.bson.Document;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.util.ArrayList;import java.util.List;import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.atomic.AtomicInteger;import java.util.concurrent.atomic.AtomicLong;public class ReadDocumentsTask implements Runnable &#123; private LinkedBlockingQueue&lt;String&gt; idPool; private LinkedBlockingQueue&lt;QAModel&gt; questionPool; private int batchNum; private AtomicInteger count; private AtomicLong start; private Logger logger; public ReadDocumentsTask(LinkedBlockingQueue&lt;String&gt; idPool , LinkedBlockingQueue&lt;QAModel&gt; questionPool, int batchNum , AtomicInteger count, AtomicLong start) &#123; this.idPool = idPool; this.questionPool = questionPool; this.batchNum = batchNum; this.count = count; this.start = start; this.logger = LoggerFactory.getLogger(ReadDocumentsTask.class + Thread.currentThread().getName()); &#125; @Override public void run() &#123; MongoCollection&lt;Document&gt; collection = MongoUtil.getCollection(); List&lt;String&gt; ids = null; while (true) &#123; try &#123; if (idPool.size() &lt;= 0) &#123; Thread.sleep(500); continue; &#125; ids = new ArrayList&lt;&gt;(); int num = Math.min(idPool.size(), batchNum); for (int i = 0; i &lt;num; i++) &#123; ids.add(idPool.poll()); &#125; List&lt;QAModel&gt; qas = this.readDocument(collection, ids); questionPool.addAll(qas); count.addAndGet(qas.size()); logger.info("读取数据: " + count + " 耗费时间: " + (System.currentTimeMillis() - start.get()) + "ms"); &#125; catch (InterruptedException e) &#123; logger.debug(e.getMessage()); &#125; &#125; &#125; private List&lt;QAModel&gt; readDocument(MongoCollection&lt;Document&gt; collection, List&lt;String&gt; ids) &#123; Document condition = new Document("_id", new Document("$in", ids)); FindIterable&lt;Document&gt; findIterable = collection.find(condition); MongoCursor&lt;Document&gt; mongoCursor = findIterable.iterator(); List&lt;QAModel&gt; res = new ArrayList&lt;&gt;(); QAModel model = null; Answer answer = null; Document document = null; while(mongoCursor.hasNext())&#123; model = new QAModel(); document = mongoCursor.next(); model.setId(document.getString("_id")); // 省略 List&lt;Document&gt; ans = (ArrayList&lt;Document&gt;)document.get("answers"); for (Document a : ans) &#123; answer = new Answer(); answer.setAnswer(a.getString("answer")); // 省略 model.getAnswers().add(answer); &#125; res.add(model); &#125; return res; &#125;&#125; WriteTask 写入 HBase1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798package com.oolong.write;import com.oolong.model.Answer;import com.oolong.model.QAModel;import com.oolong.utils.HBaseUtil;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Table;import org.apache.hadoop.hbase.util.Bytes;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.util.ArrayList;import java.util.List;import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.atomic.AtomicInteger;import java.util.concurrent.atomic.AtomicLong;public class WriteTask implements Runnable &#123; private LinkedBlockingQueue&lt;QAModel&gt; questionPool; private int batchNum; private AtomicInteger savedCount; private AtomicLong start; private Logger logger; public WriteTask(LinkedBlockingQueue&lt;QAModel&gt; questionPool, int batchNum , AtomicInteger savedCount, AtomicLong start) &#123; this.questionPool = questionPool; this.batchNum = batchNum; this.savedCount = savedCount; this.start = start; this.logger = LoggerFactory.getLogger(WriteTask.class + Thread.currentThread().getName()); &#125; @Override public void run() &#123; Table table = null; while (true) &#123; try &#123; if (table == null) &#123; table = HBaseUtil.getTable(); Thread.sleep(1000); &#125; else &#123; break; &#125; &#125; catch (InterruptedException e) &#123; logger.debug(e.getMessage()); &#125; &#125; while (true) &#123; try &#123; if (questionPool.size() &lt;= 0) &#123; Thread.sleep(1000); continue; &#125; List&lt;QAModel&gt; questions = new ArrayList&lt;&gt;(); int num = Math.min(questionPool.size(), batchNum); for (int i = 0; i &lt; num; i++) &#123; questions.add(questionPool.poll()); &#125; writeToHBase(table, questions); savedCount.addAndGet(questions.size()); logger.info("保存数据: " + savedCount + " 耗费时间: " + (System.currentTimeMillis() - start.get()) + "ms"); &#125; catch (InterruptedException e) &#123; logger.debug(e.getMessage()); &#125; &#125; &#125; private void writeToHBase(Table table, List&lt;QAModel&gt; questions) &#123; try &#123; List&lt;Put&gt; actions = new ArrayList&lt;&gt;(); for (QAModel model : questions) &#123; Put put = new Put(Bytes.toBytes(model.getId())); put.addColumn(Bytes.toBytes("http"), Bytes.toBytes("url"), Bytes.toBytes(model.getUrl())); // 省略 actions.add(put); &#125; table.put(actions); &#125; catch (IOException e) &#123; logger.debug(e.getMessage()); &#125; &#125;&#125; 工具类Constant 常量12345678910package com.oolong.utils;public class Constant &#123; public static String MONGO_IP = "192.168.0.125"; public static int MONGO_PORT = 27017; public static String MONGO_DB = "medicalqa_8"; public static String MONGO_COLLECTION = "questions_2"; public static String HB_TABLE = "medicalqa";&#125; MongoUtil MongoDB 连接工具123456789101112131415161718192021222324252627package com.oolong.utils;import com.mongodb.MongoClient;import com.mongodb.client.MongoCollection;import com.mongodb.client.MongoDatabase;import org.bson.Document;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class MongoUtil &#123; private static Logger logger; static &#123; logger = LoggerFactory.getLogger(MongoUtil.class); &#125; public static MongoCollection&lt;Document&gt; getCollection() &#123; MongoClient mongo = new MongoClient(Constant.MONGO_IP, Constant.MONGO_PORT); MongoDatabase database = mongo.getDatabase(Constant.MONGO_DB); logger.info("Connect to MongoDB successfully"); MongoCollection&lt;Document&gt; collection = database.getCollection(Constant.MONGO_COLLECTION); return collection; &#125;&#125; HBaseUtil HBase 连接工具1234567891011121314151617181920212223242526272829303132333435363738394041package com.oolong.utils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.hadoop.hbase.client.Table;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import java.io.IOException;import java.net.URISyntaxException;public class HBaseUtil &#123; private static Logger logger; static &#123; logger = LoggerFactory.getLogger(HBaseUtil.class); &#125; public static Table getTable() &#123; Table table = null; try &#123; Configuration config = HBaseConfiguration.create(); config.addResource(new Path(ClassLoader.getSystemResource("hbase-site.xml").toURI())); config.addResource(new Path(ClassLoader.getSystemResource("core-site.xml").toURI())); Connection connection = ConnectionFactory.createConnection(config); table = connection.getTable(TableName.valueOf(Constant.HB_TABLE)); logger.info("Connect to HBase successfully"); &#125; catch (URISyntaxException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return table; &#125;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Maven</tag>
        <tag>HBase</tag>
        <tag>生产者消费者模式</tag>
        <tag>多线程</tag>
        <tag>MongoDB</tag>
        <tag>Log4J</tag>
        <tag>SLF4J</tag>
        <tag>LogBack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[npm install Error EACCES permission denied]]></title>
    <url>%2F2018%2F11%2F21%2Fnpm-install-Error-EACCES-permission-denied%2F</url>
    <content type="text"><![CDATA[摘要使用 npm install 安装项目时，遇到的一个错误 Error: EACCES: permission denied。记录一下解决方法。 npm install Error: EACCES: permission denied在使用 npm 安装模块时碰到的问题，异常信息如下。 这里简单记录一下解决方法，给命令增加如下的一些参数，大概就是一些允许权限之类的，没有具体研究：1npm install --unsafe-perm=true --allow-root 错误信息1234567891011121314151617181920212223242526272829[root@weilu125 adminMongo-master]# npm installnpm WARN deprecated to-iso-string@0.0.2: to-iso-string has been deprecated, use @segment/to-iso-string instead.npm WARN deprecated jade@0.26.3: Jade has been renamed to pug, please install the latest version of pug instead of jadenpm WARN deprecated electron-prebuilt@1.4.13: electron-prebuilt has been renamed to electron. For more details, see http://electron.atom.io/blog/2016/08/16/npm-install-electronnpm WARN deprecated sprintf@0.1.5: The sprintf package is deprecated in favor of sprintf-js.npm WARN deprecated mongodb@2.2.16: Please upgrade to 2.2.19 or highernpm WARN deprecated minimatch@0.3.0: Please update to minimatch 3.0.2 or higher to avoid a RegExp DoS issuenpm WARN deprecated circular-json@0.3.3: CircularJSON is in maintenance only, flatted is its successor.&gt; electron-prebuilt@1.4.13 postinstall /usr/local/adminMongo-master/node_modules/electron-prebuilt&gt; node install.js/usr/local/adminMongo-master/node_modules/electron-prebuilt/install.js:22 throw err ^Error: EACCES: permission denied, mkdir &apos;/usr/local/adminMongo-master/node_modules/electron-prebuilt/.electron&apos;npm WARN mocha-jsdom@1.2.0 requires a peer of jsdom@&gt;10.0.0 but none is installed. You must install peer dependencies yourself.npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! electron-prebuilt@1.4.13 postinstall: `node install.js`npm ERR! Exit status 1npm ERR! npm ERR! Failed at the electron-prebuilt@1.4.13 postinstall script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.npm ERR! A complete log of this run can be found in:npm ERR! /root/.npm/_logs/2018-11-21T01_22_13_662Z-debug.log]]></content>
      <categories>
        <category>Nodejs</category>
      </categories>
      <tags>
        <tag>npm</tag>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Scala 编写的 Spark 程序操作 HBase]]></title>
    <url>%2F2018%2F11%2F20%2F%E5%9F%BA%E4%BA%8E-Scala-%E7%BC%96%E5%86%99%E7%9A%84-Spark-%E7%A8%8B%E5%BA%8F%E6%93%8D%E4%BD%9C-HBase%2F</url>
    <content type="text"><![CDATA[摘要使用 Scala 编写 Spark 程序操作 HBase 中的数据。 环境准备集群环境Hadoop环境参考：Hadoop 集群部署方案 Zookeeper&amp;HBase环境参考：基于 Hadoop 集群部署 ZooKeeper 和 HBase 集群 Spark环境参考：基于 Hadoop 和 Yarn 集群部署 Spark 集群 运行环境整个过程是在本地 Idea 中使用 Scala 编写 Spark 程序，使用 SBT 打包后，通过 spark-submit 提交到 Yarn 中运行。 之前在部署 Spark 环境时，有设置过一个变量：1spark.yarn.jars hdfs://weilu131:9000/spark_jars/* 并且已经将 Spark 的相关 JAR 包上传到 HDFS 上的这个目录中。这是 Spark 程序的运行环境。由于需要连接 HBase，因此程序的运行环境还需要有 HBase 相关的 JAR 包。 将 HBase 根目录下的以下 JAR 包上传到 HDFS 的这个目录中：123456789/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/hbase*.jar /spark_jars/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/guava-12.0.1.jar /spark_jars/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/htrace-core-3.2.0-incubating.jar /spark_jars/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/protobuf-java-2.5.0.jar /spark_jars/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/metrics-core-2.2.0.jar /spark_jars 这里需要注意，由于使用的 HBase 和 Spark 各自有对一些 JAR 包的不同版本的依赖，因此需要都上传上去，如果没有可能会报找不到类的错误。在末尾会有相关示例。 数据准备进入 HBase shell 环境：1/usr/local/hbase-1.2.0-cdh5.15.0/bin/hbase shell 建表创建一张名为 student 的表，包含一个 info 列族。1create &apos;student&apos;,&apos;info&apos; 添加数据HBase 中添加数据是按照单元格添加，通过行键、列族、列名确定一个单元格。1234567put &apos;student&apos;,&apos;e018565c-ebaa-11e8-b4ec-3c970e0087f3&apos;,&apos;info:name&apos;,&apos;wcwang&apos;put &apos;student&apos;,&apos;e018565c-ebaa-11e8-b4ec-3c970e0087f3&apos;,&apos;info:gender&apos;,&apos;F&apos;put &apos;student&apos;,&apos;e018565c-ebaa-11e8-b4ec-3c970e0087f3&apos;,&apos;info:age&apos;,&apos;22&apos;put &apos;student&apos;,&apos;e0182f4a-ebaa-11e8-a353-3c970e0087f3&apos;,&apos;info:name&apos;,&apos;lx&apos;put &apos;student&apos;,&apos;e0182f4a-ebaa-11e8-a353-3c970e0087f3&apos;,&apos;info:gender&apos;,&apos;M&apos;put &apos;student&apos;,&apos;e0182f4a-ebaa-11e8-a353-3c970e0087f3&apos;,&apos;info:age&apos;,&apos;21&apos; 编写运行程序创建一个 Scala 项目，参考配置 Intellij Idea 和 Sbt 开发、打包、运行 Spark 程序。 代码然后在 src/main/scala/ 目录下创建一个 Scala 脚本，填充以下代码： 12345678910111213141516171819202122232425262728293031323334import org.apache.spark._ import org.apache.hadoop.hbase.HBaseConfiguration import org.apache.hadoop.hbase.mapreduce.TableInputFormat import org.apache.hadoop.hbase.util.Bytes object DataImport &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster("yarn").set("spark.app.name", "MedicalQA") sparkConf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer") val sc = new SparkContext(sparkConf) val hbaseConf = HBaseConfiguration.create() hbaseConf.set("hbase.zookeeper.property.clientPort", "2181") hbaseConf.set("hbase.zookeeper.quorum", "192.168.0.131,192.168.0.132,192.168.0.133,192.168.0.151,192.168.0.152") hbaseConf.set("hbase.master", "192.168.0.131") hbaseConf.set(TableInputFormat.INPUT_TABLE, "student") val studRDD = sc.newAPIHadoopRDD(hbaseConf, classOf[TableInputFormat], classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable], classOf[org.apache.hadoop.hbase.client.Result]) val count = studRDD.count() println(count) studRDD.cache() studRDD.collect().foreach(&#123; row =&gt; &#123; val result = row._2 val key = Bytes.toString(result.getRow) val name = Bytes.toString(result.getValue("info".getBytes(), "name".getBytes())) val gender = Bytes.toString(result.getValue("info".getBytes(), "gender".getBytes())) val age = Bytes.toString(result.getValue("info".getBytes(), "age".getBytes())) println(key + " " + name + " " + gender + " " + age) &#125; &#125;) &#125; &#125; 在这段代码中首先统计了 student 表中数据的条数，然后将两条数据分别打印出来。 项目配置将 Hbase 下面的 hbase-site.xml 文件拷贝到项目中的 src/main/scala 目录中。 打包运行打包好的 JAR 包上传到服务器中，然后提交到 Spark 中运行：1/usr/local/spark-2.4.0-bin-hadoop2.6/bin/spark-submit --deploy-mode cluster /home/workspace/MedicalQAImport.jar 如果命令行和 Yarn 中没有抛异常，那就OK。然后检查 Yarn 中的日志。1232e0182f4a-ebaa-11e8-a353-3c970e0087f3 lx M 21e018565c-ebaa-11e8-b4ec-3c970e0087f3 wcwang F 22 注意，这里输出的日志不是在控制台输出，而是在所运行的那台服务器的日志中，可以通过 Yarn 的界面查看。 遇到问题java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge这里就是之前所说的 HBase 和 Spark 依赖于同一个类库的不同版本的问题。在上传 Spark 的 JAR 包时，已经将 metrics-core-3.1.5.jar 上传了。但实际上 HBase 依赖的是 metrics-core-2.2.0.jar 的版本，因此还需要将这个包也上传。 部分异常1234567891011org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:Tue Nov 20 08:55:43 CST 2018, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0Caused by: java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0 Caused by: java.io.IOException: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/GaugeCaused by: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/GaugeCaused by: java.lang.NoClassDefFoundError: com/yammer/metrics/core/GaugeCaused by: java.lang.ClassNotFoundException: com.yammer.metrics.core.Gauge 完整异常信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413518/11/20 08:55:44 ERROR ApplicationMaster: User class threw exception: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:Tue Nov 20 08:55:43 CST 2018, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:Tue Nov 20 08:55:43 CST 2018, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0 at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.throwEnrichedException(RpcRetryingCallerWithReadReplicas.java:320) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:247) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:62) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210) at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:327) at org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:302) at org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:167) at org.apache.hadoop.hbase.client.ClientScanner.&lt;init&gt;(ClientScanner.java:162) at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:867) at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:193) at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:89) at org.apache.hadoop.hbase.client.MetaScanner.allTableRegions(MetaScanner.java:324) at org.apache.hadoop.hbase.client.HRegionLocator.getAllRegionLocations(HRegionLocator.java:88) at org.apache.hadoop.hbase.util.RegionSizeCalculator.init(RegionSizeCalculator.java:94) at org.apache.hadoop.hbase.util.RegionSizeCalculator.&lt;init&gt;(RegionSizeCalculator.java:81) at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:256) at org.apache.hadoop.hbase.mapreduce.TableInputFormat.getSplits(TableInputFormat.java:240) at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126) at org.apache.spark.rdd.RDD.count(RDD.scala:1168) at DataImport$.main(DataImport.scala:18) at DataImport.main(DataImport.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)Caused by: java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0 at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:169) at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:80) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.IOException: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:334) at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:408) at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:204) at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:65) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:397) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:371) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:136) ... 4 moreCaused by: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:240) at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:336) at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:34094) at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:400) ... 10 moreCaused by: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:225) ... 13 moreCaused by: java.lang.ClassNotFoundException: com.yammer.metrics.core.Gauge at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 14 more18/11/20 08:55:44 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:Tue Nov 20 08:55:43 CST 2018, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0 at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.throwEnrichedException(RpcRetryingCallerWithReadReplicas.java:320) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:247) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:62) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210) at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:327) at org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:302) at org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:167) at org.apache.hadoop.hbase.client.ClientScanner.&lt;init&gt;(ClientScanner.java:162) at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:867) at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:193) at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:89) at org.apache.hadoop.hbase.client.MetaScanner.allTableRegions(MetaScanner.java:324) at org.apache.hadoop.hbase.client.HRegionLocator.getAllRegionLocations(HRegionLocator.java:88) at org.apache.hadoop.hbase.util.RegionSizeCalculator.init(RegionSizeCalculator.java:94) at org.apache.hadoop.hbase.util.RegionSizeCalculator.&lt;init&gt;(RegionSizeCalculator.java:81) at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:256) at org.apache.hadoop.hbase.mapreduce.TableInputFormat.getSplits(TableInputFormat.java:240) at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126) at org.apache.spark.rdd.RDD.count(RDD.scala:1168) at DataImport$.main(DataImport.scala:18) at DataImport.main(DataImport.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)Caused by: java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0 at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:169) at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:80) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.IOException: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:334) at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:408) at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:204) at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:65) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:397) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:371) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:136) ... 4 moreCaused by: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:240) at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:336) at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:34094) at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:400) ... 10 moreCaused by: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:225) ... 13 moreCaused by: java.lang.ClassNotFoundException: com.yammer.metrics.core.Gauge at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 14 more)18/11/20 08:55:44 INFO SparkContext: Invoking stop() from shutdown hook had a not serializable result解决在 Spark 的环境中配置：1sparkConf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer") 异常信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110218/11/20 09:55:25 ERROR TaskSetManager: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritableSerialization stack: - object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33) - field (class: scala.Tuple2, name: _1, type: class java.lang.Object) - object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;)) - element of array (index: 0) - array (class [Lscala.Tuple2;, size 2); not retrying18/11/20 09:55:25 INFO YarnClusterScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 18/11/20 09:55:25 INFO YarnClusterScheduler: Cancelling stage 118/11/20 09:55:25 INFO YarnClusterScheduler: Killing all running tasks in stage 1: Stage cancelled18/11/20 09:55:25 INFO DAGScheduler: ResultStage 1 (collect at DataImport.scala:23) failed in 0.338 s due to Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritableSerialization stack: - object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33) - field (class: scala.Tuple2, name: _1, type: class java.lang.Object) - object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;)) - element of array (index: 0) - array (class [Lscala.Tuple2;, size 2)18/11/20 09:55:25 INFO DAGScheduler: Job 1 failed: collect at DataImport.scala:23, took 0.346124 s18/11/20 09:55:25 ERROR ApplicationMaster: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritableSerialization stack: - object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33) - field (class: scala.Tuple2, name: _1, type: class java.lang.Object) - object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;)) - element of array (index: 0) - array (class [Lscala.Tuple2;, size 2)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritableSerialization stack: - object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33) - field (class: scala.Tuple2, name: _1, type: class java.lang.Object) - object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;)) - element of array (index: 0) - array (class [Lscala.Tuple2;, size 2) at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926) at scala.Option.foreach(Option.scala:257) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:363) at org.apache.spark.rdd.RDD.collect(RDD.scala:944) at DataImport$.main(DataImport.scala:23) at DataImport.main(DataImport.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)18/11/20 09:55:25 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritableSerialization stack: - object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33) - field (class: scala.Tuple2, name: _1, type: class java.lang.Object) - object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;)) - element of array (index: 0) - array (class [Lscala.Tuple2;, size 2) at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926) at scala.Option.foreach(Option.scala:257) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:363) at org.apache.spark.rdd.RDD.collect(RDD.scala:944) at DataImport$.main(DataImport.scala:23) at DataImport.main(DataImport.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)) 参考：https://segmentfault.com/q/1010000007041500]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>HBase</tag>
        <tag>Scala</tag>
        <tag>Spark</tag>
        <tag>Scala操作HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 关键概念备忘]]></title>
    <url>%2F2018%2F11%2F20%2FSpark-%E5%85%B3%E9%94%AE%E6%A6%82%E5%BF%B5%E5%A4%87%E5%BF%98%2F</url>
    <content type="text"><![CDATA[摘要学习 Spark 过程中记录的一些比较重要的概念。填充了一部分内容，另有一部分留空的，后续理解逐步加深后进一步补全和拓展。 基本概念和架构基本概念1. RDDResillient Distributed Dataset 弹性分布式数据集，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型 2. DAGDirected Acyclic Graph 有向无环图，反应 RDD 之间的依赖关系 3. Executor运行在工作节点（WorkerNode）上的一个进程，负责运行 Task 4. Application用户编写的 Spark 应用程序 5. Task运行在 Execotr 上的工作单元 6. Job一个 Job 包含多个 RDD 及作用于相应 RDD 上的各种操作 7. Stage是 Job 的基本调度单位，一个 Job 会分为多组 Task，每组 Task 成为 Stage，或者 TaskSet，代表一组关联的，相互之间没有 Shuffle 依赖关系的任务组成的任务集 运行架构 Spark有点：1、利用多线程执行具体的任务，减少任务启动开销2、Executor 中有一个 BlockManager 存储模块，结合内存和磁盘作为存储设备，减少 IO 开销 Spark 运行流程 STEP 1 为应用构建基本的运行环境，由 Driver 创建一个 SparkContext 进行资源的申请、任务的分配和监控。 STEP 2 资源管理器为 Executor 分配资源，并启动 Executor 进程。 STEP 3 SparkContext 根据 RDD 的依赖关系构建 DAG 图，DAG 图提交给 DAGScheduler 解析成 Stage，然后把一个个 TaskSet 提交给底层调度器 TaskSchedule 处理。 Executor 向 SprakContext 申请 Task。 TaskScheduler 将 Task 分发给 Executor ，并提供应用程序代码 STEP 4 Task 在 Excutor 上运行，把执行结果反馈给 TaskScheduler，然后反馈给 DAGScheduler，运行完成之后写入数据并释放资源。 Spark 运行特点1、每个 Application 都有自己专属的 Executor 进程，并且该进程在 Application 运行期间一直驻留。Executor进程以多线程的方式运行 Task。2、Spark 运行过程与资源管理器无关，只要能够获取 Executor 进程并保持通讯即可3、Task 采用了数据本地性和推测执行优化机制 RDDRDD 执行过程1、RDD 读取外部数据进行创建 2、经过一系列转换（Transformation）操作，每次都会产生新的 RDD，提供给下一次转换操作使用 3、最后一个 RDD 经过“动作”操作进行转换，并输出到外部数据源 一般讲一个 DAG 的一系列处理成为一个 Lineage（血缘关系） RDD 的依赖关系窄依赖1、一个父亲 RDD 的一个分区，转换得到一个儿子 RDD 的一个分区2、多个父亲 RDD 的若干个分区，转换得到一个儿子 RDD 的一个分区 宽依赖1、一个父亲 RDD 的一个分区，转换得到多个儿子 RDD 的若干个分区 Stage 划分DAG 中进行反向解析，遇到宽依赖就断开，遇到债依赖就把当前 RDD 加入到 Stage 中。将窄依赖尽量划分在同一个 Stage 中，实现流水线计算。 RDD 基本操作RDD 创建从本地文件系统加载数据 1val lines = sc.textFile("file:///home/spark/mydata/word.txt") 从分布式文件系统 HDFS 中加载数据1val lines = sc.textFile("hdfs://weilu131:9000/mydata/word.txt") 从集合中创建 RDD使用 sc.parallelize() 方法可以将数组转换为 RDD12val array = Array(1, 2, 3, 4, 5)val rdd = sc.parallelize(array) 对于列表 List 同上。 RDD 操作转换操作（Transformation）filter(func)筛选出能够满足函数 func 的元素，并返回一个新的数据集。12val lines = sc.textFile("hdfs://weilu131:9000/mydata/word.txt")val res = lines.filter(line =&gt; line.contains("Spark")).count() map(func)将每个元素传递到函数func中，并将结果返回为一个新的数据集12val lines = sc.textFile("hdfs://weilu131:9000/mydata/word.txt")val res = lines.map(line =&gt; line.split(" ").size).reduce((a,b) =&gt; if (a &gt; b) a else b) flatMap(func)与 map 类似，但每个输入元素都可以映射到0或多个输出结果 groupByKey()应用于(K,V)键值对数据集时，返回一个新的 (K, Iterable) 形式的数据集 reduceByKey(func)应用于 (K, V) 键值对的数据集时，返回一个新的 (K, V) 形式的数据集，其中的每个值是将每个 key 传递到函数 func 中进行聚合。 行动操作（Action）count()返回数据集中的元素个数 collect()以数组的形式返回数据集中的所有元素 first()返回数据集中第一个元素 take(n)以数组的形式返回数据集中的前 n 个元素 reduce(func)通过函数 func（输入两个参数并返回一个值）聚合函数集中的元素 foreach(func)将数据集中的每个元素传递到函数 func 中运行 惰性机制对于 RDD 而言，每一次转换都会形成新的 RDD，但是在转换操作过程中，只会记录轨迹，只有程序运行到行动操作时，才会真正的进行计算，这个被称为惰性求值。 持久化12345678val list = List("Hadoop", "Spark", "Hive")val rdd = sc.parallelize(list)// 行动操作，触发一次计算rdd.count()// 行动操作，再次触发一次计算rdd.collect().mkString(",") 在两次行动操作中每次触发的转换操作都是相同的，为了避免重复计算，可以对第一次转换的过程进行持久化。 persist(MEMORY_ONLY)将 RDD 作为反序列化对象存储于 JVM 中，如果内存不足，按照 LRU 原则替换缓存中内容。 persist(MEMORY_AND_DISK)将 RDD 作为反序列化的对象存储在 JVM 中，如果内存不足，超出部分会被存储在硬盘上 cache()persist(MEMORY_ONLY) 的快捷方式 unpersist()手动把持久化的 RDD 从缓存中删除 1234567891011val list = List("Hadoop", "Spark", "Hive")val rdd = sc.parallelize(list)// 标记为持久化rdd.cache()// 行动操作，触发一次计算，并缓存转换操作结果rdd.count()// 行动操作，直接使用缓存的转换操作结果rdd.collect().mkString(",") 分区RDD 分区的一个分区原则是使得分区的个数尽量等于整个集群中的CPU核心数目对于不同的 spark 部署模式而言，都可以使用 spark.default.parallelism 这个参数设置 在调用 textFile 和 parallelize 方法时候手动指定分区个数即可。 对于 parallelize 而言，如果没有在方法中指定分区数，则默认为 spake.deafault.parallelism。 对于textFile 而言，如果没有在方法中指定分区数，则默认为 min(defaultParallelism, 2)，其中，defaultParallelism 对应的就是 spark.default.parallelism 如果时从 HDFS 中读取文件，则分区数为文件分片数（比如，128MB/片） textFile 1sc.textFile(path, partitionNum) parallelize 1sc.parallelize(array, 2) // 设置两个分区 通过转换操作得到新的 RDD 时，直接调用 reparation 方法 自定义分区12345678910111213141516171819202122import org.apache.spark.&#123;Partitioner, SparkContext, SparkConf&#125; class UserPartitioner(numParts: Int) extends Partitioner &#123; override def numPartitions: Int = numParts override def getPartition(key: Any): Int = &#123; key.toString.toInt % 10 &#125; &#125; object ManualPartition &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() val sc = new SparkContext(conf) val data = sc.parallelize(1 to 5, 5) val data2 = data.map((_, 1)) val data3 = data2.partitionBy(new UserPartitioner(10)) val data4 = data3.map(_._1) data4.saveAsTextFile("hdfs://weilu131:9000/test/output") &#125; &#125; Pair RDD（键值对 RDD）创建 PairRDD12345678val list = List("Hadoop", "Hive", "HBase", "Spark", "Sqoop", "Spark")val rdd = sc.parallelize(list) // 创建 RDDval pairRDD = rdd.map(word =&gt; (word, 1))// 如果是在集群上运行 Spark 程序，那么这段代码不会打印任何内容pairRDD.foreach(println)// 需要先收集之后再打印pairRDD.collect().foreach(println) 打印内容：123456(Hadoop,1)(Hive,1)(HBase,1)(Spark,1)(Sqoop,1)(Spark,1) reduceByKey(func)key 相同，将值按照传入逻辑计算12345678910111213val list = List("Hadoop 2", "Spark 3", "HBase 5", "Spark 6", "Hadoop 1")val rdd = sc.parallelize(list)val split = (line : String) =&gt; &#123; val res = line.split(" ") (res(0), res(1).toInt)&#125;val pairRdd = rdd.map(split)pairRdd.collect().foreach(println) // 打印测试:1val res = pairRdd.reduceByKey((a,b) =&gt; a+b)res.collect().foreach(println) // 打印测试：2 打印结果：1234567891011// 第一次(Hadoop,2)(Spark,3)(HBase,5)(Spark,6)(Hadoop,1)// 第二次(Spark,9)(HBase,5)(Hadoop,3) groupByKey()key 相同，将值生成一个列表12345678910111213val list = List("Hadoop 2", "Spark 3", "HBase 5", "Spark 6", "Hadoop 1")val rdd = sc.parallelize(list)val split = (line : String) =&gt; &#123; val res = line.split(" ") (res(0), res(1).toInt)&#125;val pairRdd = rdd.map(split)pairRdd.collect().foreach(println) // 打印测试:1val res = pairRdd.groupByKey()res.collect().foreach(println) // 打印测试：2 打印结果：1234567891011// 第一次(Hadoop,2)(Spark,3)(HBase,5)(Spark,6)(Hadoop,1)// 第二次(Spark,CompactBuffer(6, 3))(HBase,CompactBuffer(5))(Hadoop,CompactBuffer(1, 2)) keys、values仅仅把 PairRDD 中的键或者值单独取出来形成一个 RDD sortByKey()123456789101112131415val list = List("Hadoop 2", "Spark 3", "HBase 5", "Spark 6", "Hadoop 1")val rdd = sc.parallelize(list)val split = (line : String) =&gt; &#123; val res = line.split(" ") (res(0), res(1).toInt)&#125;val pairRdd = rdd.map(split)val res = pairRdd.sortByKey(true)res.collect().foreach(println) // 打印测试：1val res = pairRdd.sortByKey(false)res.collect().foreach(println) // 打印测试：2 打印结果：12345678910111213# 1(HBase,5)(Hadoop,2)(Hadoop,1)(Spark,6)(Spark,3)# 2(Spark,3)(Spark,6)(Hadoop,1)(Hadoop,2)(HBase,5) mapValues(func)对 PairRDD 中的每个值进行处理，不影响 key. join将两个 PairRDD 根据 key 进行连接操作 combineByKey共享变量主要用于节省传输开销。当Spark在集群的多个节点上的多个任务上并行运行一个函数时，它会吧函数中涉及到的每个变量在每个任务中生成一个副本。但是，有时需要在多个任务之间共享变量，或者在任务和任务控制节点之间共享变量。 为满足这种需求，Spark提供了两种类型的变量：广播变量（broadcast variables）和累加器（accumulators）。广播变量用来把变量在所有节点的内存之间进行共享；累加器则支持在所有不同节点之间进行累加计算（比如计数、求和等） 广播变量允许程序开发人员在每个机器上缓存一个只读变量，而不是在每个机器上的每个任务都生成一个副本。Spark的“行动”操作会跨越多个阶段（Stage），对每个阶段内的所有任务所需要的公共数据，Spark会自动进行广播。 可以使用 broadcast() 方法封装广播变量123val broadcastVar = sc.broadcast(Array(1, 2, 3))println(broadcastVar.value) 累加器Spark 原生支持数值型累加器，可以通过自定义开发对新类型支持的累加器。 longAccumulator &amp; doubleAccumulatorSpark 自带长整型和双精度数值累加器，可以通过以上两个方法创建。创建完成之后可以使用 add 方法进行累加操作，但在每个节点上只能进行累加操作，不能读取。只有任务控制节点可以使用 value 方法读取累加器的值。 123val accum = sc.longAccumulator("OneAccumulator")sc.parallelize(Array(1, 2, 3)).foreach(x =&gt; accum.add(x))accum.value 数据读写文件系统数据读写读写本地文件1234val aFile = sc.textFile("file:///home/spark/somewords.txt")// 保存时会生成一个目录，内容被跌倒这个目录中aFile.saveAsTextFile("file:///home/spark/something.txt") 读写HDFS文件1234val aFile = sc.textFile("hdfs://weilu131:9000/home/spark/somewords.txt")// 保存时会生成一个目录，内容被跌倒这个目录中aFile.saveAsTextFile("hdfs://weilu131:9000/home/spark/something.txt")]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>RDD</tag>
        <tag>DAG</tag>
        <tag>Exector</tag>
        <tag>Job</tag>
        <tag>Stage</tag>
        <tag>PairRDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置 Intellij Idea 和 Sbt 开发、打包、运行 Spark 程序]]></title>
    <url>%2F2018%2F11%2F16%2F%E9%85%8D%E7%BD%AE-Intellij-Idea-%E5%92%8C-Sbt-%E5%BC%80%E5%8F%91%E3%80%81%E6%89%93%E5%8C%85%E3%80%81%E8%BF%90%E8%A1%8C-Spark-%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[摘要使用 Idea 集成 Sbt 配置 Scala 开发环境，开发 Spark 程序。 配置基本环境首先需要几个基本环境，一个是 JDK，还有 Scala，另外安装 Idea，以及在 Idea 中安装 Scala 插件，这里就不赘述了。 创建项目首先创建一个 Scala - sbt 项目： 填写项目名称，项目路径。另外需要注意的是选择合适的 sbt 和 scala 版本。 创建完成之后，会从远处服务器拉取项目结构信息，可能会耗费一些时间。 完成之后会生成如图所示的一个项目结构： 导入依赖编辑 build.sbt 文件，填写项目依赖，这里是 Spark 程序，需要添加 Spark-core依赖。另外这里还声明了 Scala 依赖。 1234567name := "MedicalQA"version := "0.1"scalaVersion := "2.11.12"libraryDependencies += "org.apache.spark" %% "spark-core" % "2.4.0" 在编辑了 build.sbt 文件后，会提示是否导入这些依赖，选择自动导入即可。 如果是第一次添加某个依赖，会从远程服务器下载，根据网络情况，耗费一定时间，需耐心等待。 导入之后，可以在 External Libraries 中看到这些依赖。 添加 SBT 依赖如果 External Libraries 中没有 sbt 相关的 JAR 包，可以手动添加。 打开 Project Structure，在 File 或者界面右上角的文件图标打开。 点击上方的小加号，选择 Scala SDK，图片中的实例是添加后的效果，一开始应该是空的。 在弹出的如下界面中选择 Browse，浏览本地目录，选中本地 Scala 的根目录即可。 编写测试程序创建一个 Scala 文件。 这里面写了一个小李子，是在hdfs上写入一个文本文件。完成之后就需要将项目编译成一个JAR包，上传到spark中执行。 1234567891011121314151617181920212223import java.io.BufferedOutputStreamimport org.apache.hadoop.fs.&#123;FileSystem, Path&#125;import org.apache.spark._object DataImport &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() conf.setMaster("yarn") conf.set("spark.app.name", "MedicalQA") val sc = new SparkContext(conf) val sentence = "some words" val fs = FileSystem.get(sc.hadoopConfiguration) val output = fs.create(new Path("/test/somewords.txt")) val ops = new BufferedOutputStream(output) ops.write(sentence.getBytes("UTF-8")) ops.close() output.close() fs.close() &#125;&#125; 编译打包再次打开 Project Structure，选择 Artifacts，点击小加号 选择 Module，也就是项目，以及程序入口 Main Class，也就是主函数所在的类。 然后会看到如下的界面： 接下来我们要把jar包中引用的jar包去除掉，不打包在里面。选中这些JAR包，用上面的减号删除掉。 最后只剩下项目的代码 完成之后会在项目目录下看到 META-INF 目录。 然后进行编译。 编译打包完成之后，会在项目目录中看到打包的结果 运行测试接下来我们把这个JAR包提交到spark集群中跑跑看。 把 JAR 包放到 /home/workspace 目录下，执行命令： 1/usr/local/spark-2.4.0-bin-hadoop2.6/bin/spark-submit --class &quot;DataImport&quot; --deploy-mode cluster /home/workspace/MedicalQA.jar 跑完之后如果没有报什么错的话，用HDFS命令检查一下 123[root@weilu131 bin]# ./hdfs dfs -ls /testFound 1 items-rw-r--r-- 3 root supergroup 10 2018-11-16 15:52 /test/somewords.txt 发现文件确实写进去了，当然如果不放心还可以看看文件内容 12[root@weilu131 bin]# ./hdfs dfs -cat /test/somewords.txtsome words 还可以看看 Yarn 上面提交的任务执行情况]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Idea</tag>
        <tag>Scala</tag>
        <tag>Spark</tag>
        <tag>Sbt</tag>
        <tag>sbt package</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Hadoop 和 Yarn 集群部署 Spark 集群]]></title>
    <url>%2F2018%2F11%2F15%2F%E5%9F%BA%E4%BA%8E-Hadoop-%E5%92%8C-Yarn-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2-Spark-%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[摘要基于 Hadoop 和 Yarn 集群部署 Spark 集群。 部署 Scala下载 Scala，放置到 /usr/local/ 下，并解压。1tar -zxvf scala-2.11.12.tgz 配置环境变量：1vim /etc/profile 中增加以下内容：12export SCALA_HOME=/usr/local/scala-2.11.12export PATH=$PATH:$SCALA_HOME/bin 重新载入：1source /etc/profile 验证：12345[root@weilu131 local]# scalaWelcome to Scala 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_192).Type in expressions for evaluation. Or try :help.scala&gt; 部署 Spark下载&amp;解压从 Apache Spark download page 下载安装包。在选择安装包类型时，如果是针对某个版本的 Hadoop 的话，可以选择 Pre-build for Apache Hadoop 2.6，或 Pre-build for Apache Hadoop 2.7 and later。分别是针对 2.6 和 2.7 版本的。或者也可以选择 Pre-build with user-provided Apache Hadoop，表示适用于所有版本 Hadoop。 下载后解压到 /usr/local/ 目录。1tar -zxvf spark-2.4.0-bin-hadoop2.6.tgz 开放端口8080：Master节点上的端口，提供Web UI8081：Worker节点上的端口，提供Web UI 1234567firewall-cmd --add-port=8080/tcp --permanentfirewall-cmd --add-port=7077/tcp --permanentfirewall-cmd --add-port=8081/tcp --permanentfirewall-cmd --add-port=8030/tcp --permanentfirewall-cmd --add-port=30000-50000/tcp --permanentfirewall-cmd --reload 配置环境变量在 Spark 的 conf 目录下拷贝一份 spark-env.sh 文件：1cp spark-env.sh.template spark-env.sh 在文件最后添加以下内容：12345678910export JAVA_HOME=/usr/local/jdk1.8.0_181export SCALA_HOME=/usr/local/scala-2.11.12export HADOOP_HOME=/usr/local/hadoop-2.6.0-cdh5.15.0export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport SPARK_WORKING_MEMORY=1g #每一个worker节点上可用的最大内存export SPARK_MASTER_IP=weilu131 #驱动器节点IPexport SPARK_DIST_CLASSPATH=$(/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hadoop classpath) HADOOP_CONF_DIR要让 Spark 与 YARN 资源管理器通信的话，需要将 Hadoop 的配置信息告诉 Spark，通过配置 HADOOP_CONF_DIR 环境变量实现。 设置 Yarn 为资源管理器将 conf 目录下的配置文件模板拷贝一份：1cp spark-defaults.conf.template spark-defaults.conf 修改其中的 master 配置：1spark.master yarn 配置公共 JAR 包配置 Spark 的 jar 包。在配合 Hadoop 集群下提交任务时，会将 jar 包提交到 HDFS 上，为防止每次提交任务时都提交，所以在 HDFS 上上传一份公共的。 在 HDFS 上创建存放 jar 包的目录：1/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -mkdir /spark_jars 检查目录是否创建：1/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -ls / 将 spark 下的 jar 包上传到该目录下：1/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/spark-2.4.0-bin-hadoop2.6/jars/* /spark_jars 使用如下命令检查是否上传成功：1/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -ls /spark_jars 在 spark-defaults.conf 中增加以下内容：1spark.yarn.jars hdfs://weilu131:9000/spark_jars/* 配置 slaves配置从节点主机名（或者IP），在 Spark 的 conf 目录下拷贝一份 slaves 文件：1cp slaves.template slaves 在其中添加以下内容：123weilu132weilu135weilu151 启动 Spark 集群1/usr/local/spark-2.4.0-bin-hadoop2.6/sbin/start-all.sh 验证jps 命令在 Master 和 Worker 节点上分别使用 jps 命令，可以分别看到 Master 和 Worker 进程。 Web UI访问Master：http://192.168.0.131:8080/可以看到当前集群的状况。 启动 spark-shellSpark-shell 是 spark 提供的一个交互式编程环境，基于 Scala。123456789101112131415161718192021222324[root@weilu131 conf]# /usr/local/spark-2.4.0-bin-hadoop2.6/bin/spark-shell SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/spark-2.4.0-bin-hadoop2.6/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.6.0-cdh5.15.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]2018-11-15 19:57:20 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).Spark context Web UI available at http://weilu131:4040Spark context available as &apos;sc&apos; (master = yarn, app id = application_1542209821671_0001).Spark session available as &apos;spark&apos;.Welcome to ____ __ / __/__ ___ _____/ /__ _\ \/ _ \/ _ `/ __/ &apos;_/ /___/ .__/\_,_/_/ /_/\_\ version 2.4.0 /_/ Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)Type in expressions to have them evaluated.Type :help for more information.scala&gt; 如果能够正常进入 scala 交互命令行，说明部署没有问题。 对于Yarn而言，实际上 spark-shell 也是一个应用程序，因此可以在Yarn上看到这个启动的 shell： 部署History Server（不完整）该部分内容可以暂时不用，没有完成部署验证，留待之后补全。开放Web UI 端口12firewall-cmd --add-port=18080/tcp --permanentfirewall-cmd --reload 创建日志目录：1/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -mkdir /spark_logs spark-defaults.conf这部分配置的是 Spark 写入日志的信息。123spark.eventLog.enabled true spark.eventLog.dir hdfs://weilu131:9000/spark_logsspark.eventLog.compress true spark-env.sh1export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://weilu131:9000/spark_logs&quot; 启动日志服务器1/usr/local/spark-2.4.0-bin-hadoop2.6/sbin/start-history-server.sh 访问日志服务器：http://192.168.0.131:18080 参考[1] http://spark.apache.org/docs/latest/monitoring.html[2] https://www.fwqtg.net/%E3%80%90spark%E5%8D%81%E5%85%AB%E3%80%91spark-history-server.html[3] https://my.oschina.net/u/3754001/blog/1811243]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>Spark on Yarn</tag>
        <tag>Spark-shell</tag>
        <tag>History-Server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 常用配置 - 禁止系统休眠]]></title>
    <url>%2F2018%2F11%2F14%2FCentOS-7-%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE-%E7%A6%81%E6%AD%A2%E7%B3%BB%E7%BB%9F%E4%BC%91%E7%9C%A0%2F</url>
    <content type="text"><![CDATA[摘要CentOS 常用配置 修改文件：1vi /etc/default/grub 在文件末尾增加：1pcie_aspm=off]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>休眠</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 语法之走马观花]]></title>
    <url>%2F2018%2F11%2F14%2FScala-%E8%AF%AD%E6%B3%95%E4%B9%8B%E8%B5%B0%E9%A9%AC%E8%A7%82%E8%8A%B1%2F</url>
    <content type="text"><![CDATA[摘要走马观花的学习了一下 Scala 的基本语法，以下示例基于 Scala 2.11.12 版本编写。 变量可变和不可变量Scala 中变量有两种不同类型：1）val：不可变，在声明时必须初始化，且之后不能再复制2）var：可变，声明时需要初始化，之后可以再次赋值 数据类型scala 可以根据赋值推断数据类型1val myName = "dcwang" 也可以指定类型1val myName2 : String = "dcwang" 还可以利用全限定名指定类型，Scala中使用的是Java类型1val myName3 : java.lang.String = "dcwang" 所有 scala 文件默认会导入 java.lang 下面所有的包，在 scala中表示为：1import java.lang._ 用下划线表示所有 12var price = 1.2var price2 : Double = 2.2 在 scala 中所有的基本类型都是 类 基本运算在 scala 中所有运算都是调用函数1val sum1 = 5 + 3 相当于1val sum2 = (5).+(3) 富包装类Range121 to 5res1: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5) 1 to 5 相当于 1.to(5)12scala&gt; 1 until 5res2: scala.collection.immutable.Range = Range(1, 2, 3, 4)\ 12scala&gt; 1 to 10 by 2res3: scala.collection.immutable.Range = Range(1, 3, 5, 7, 9) 控制结构判断123456789101112131415val x = 6if (x &gt; 0) &#123; // do something&#125; else &#123; // do something&#125;if (x &gt; 0) &#123; //&#125; else if (x == 0) &#123; //&#125; else &#123; //&#125; scala 中可以将 if 中判断的值赋值给变量 while 循环123456var i = 9while (i &gt; 0) &#123; i -= 1 printf("i is %d \n", i)&#125; for 循环123456789101112for (i &lt;- 1 to 5) println(i)for (i &lt;- 1 to 5 by 2) println(i)// 守卫（guard）表达式for (i &lt;- 1 to 5 if i % 2 == 0) println(i)for (i &lt;- 1 to 5; j &lt;- 1 to 3) println(i * j) 文件操作文本文件读写Scala 需要调用 java.io.PrintWriter 实现文件写入12345678import java.io.PrintWriterval out = new PrintWriter("output.txt")for (i &lt;- 1 to 5) out.println(i)out.close() 使用 Scala.io.Source 的 getLines 方法实现对文件中所有行的读取12345678import scala.io.Sourceval inputFile = Source.fromFile("output.txt")val lines = inputFile.getLinesfor (line &lt;- lines) println(line) 异常捕获Scala 不支持 Java 中的 checked exception，将所有异常当做运行时异常Scala 仍然使用 try-catch 捕获异常 1234567891011121314import java.io.FileReaderimport java.io.FileNotFoundExceptionimport java.io.IOExceptiontry &#123; val file = new FileReader("NotExistFile.txt")&#125; catch &#123; case ex: FileNotFoundException =&gt; // do something case ex: IOException =&gt; // do something&#125; finally &#123; file.close()&#125; 容器 CollectionsScala 提供了一套丰富的容器库，包括列表、数组、集合、映射等Scala 用三个包来组织容器，分别是123scala.collectionscala.collection.mntable scala.collection.immutable 列表 List共享相同类型的不可变的对象序列，定义在 scala.collection.immutable 中List 在声明时必须初始化123456789101112131415var strList = List("BigData", "Hadoop", "Spark")// 返回头部第一个元素strList.head// 返回除了第一个之外的其他元素（一个新的List）strList.tail// 连接操作（从右侧开始）var oneList = List(1, 2, 3)var otherList = List(4, 5)var newList = oneList::otherList// Nil 是一个空列表对象var intList = 1::2::3::Nil 集合 Set集合中的元素插入式无序的，以哈希方法对元素的值进行组织，便于快速查找 集和包括可变和不可变集和，分别位于 scala.collection.mntable 和 scala.collection.immutable 包中，默认情况下是不可变集和。 12var mySet = Set("Hadoop", "Spark")mySet += "Scala" 变量是可变的，集合不可变，加操作导致生成了一个新的集合。 可变集合，在原集合中增加一个元素。1234import scala.collection.mutable.Setval myMutableSet = Set("BigData", "Spark")myMutableSet += "Scala" 映射 Map映射时一系列键值对的容器。也有可变和不可变两个版本，默认情况下是不可变的。 1val university = Map("XMU" -&gt; "Xiamen University", "THU" -&gt; "Tsinghua University", "PKU" -&gt; "Peking University") 可变映射123456import scala.collection.mutable.Mapval university = Map("XMU" -&gt; "Xiamen University", "THU" -&gt; "Tsinghua University", "PKU" -&gt; "Peking University")university("XMU") = "Ximan University" // updateuniversity("FZU") = "Fuzhou University" // add 遍历映射1234567891011for ( (k,v) &lt;- university) &#123; // do something&#125;for ( k &lt;- university.keys ) &#123; // do something&#125;for ( v &lt;- university.values ) &#123; // do something&#125; 迭代器 Iterator迭代器不是一个集合，而是一种访问集合的方法。 迭代器有两个基本操作：next 和 hasNext。 123456789val iter = Iterator("Hadoop", "Spark", "Scala")while (iter.hasNext) &#123; println(iter.next())&#125;for (elem &lt;- iter) &#123; println(elem)&#125; grouped &amp; slidinggrouped 返回元素的增量分块1234567891011scala&gt; val xs = List(1,2,3,4,5)xs: List[Int] = List(1, 2, 3, 4, 5)scala&gt; val git = xs grouped 3git: Iterator[List[Int]] = non-empty iteratorscala&gt; git.next()res0: List[Int] = List(1, 2, 3)scala&gt; git.next()res1: List[Int] = List(4, 5) sliding 生成一个滑动元素的窗口1234567891011scala&gt; val sit = xs sliding 3sit: Iterator[List[Int]] = non-empty iteratorscala&gt; sit.next()res3: List[Int] = List(1, 2, 3)scala&gt; sit.next()res4: List[Int] = List(2, 3, 4)scala&gt; sit.next()res5: List[Int] = List(3, 4, 5) 数组 Array是一种可变的、可索引的、元素具有相同类型的数据集合，Scala提供了类似于Java中泛型的机制指定数组类型。也可以不指定类型。 123456val intValueArr = new Array[Int](3)intValueArr(0) = 23intValueArr(1) = 34intValueArr(2) = 45val strArr = Array("BigData", "Hadoop", "Spark") 多维数组定义多维数组使用 ofDim() 方法1val myMatrix = Array.ofDim[Int](3,4) // 三行四列 访问元素1myMatrix(0)(1) 不定长数组1234567891011121314import scala.collection.mutable.ArrayBufferval aMutableArr = ArrayBuffer(10, 20, 30)aMutableArr += 40println(aMutableArr) // ArrayBuffer(10, 20, 30, 40)aMutableArr.insert(2, 60, 40)println(aMutableArr) // ArrayBuffer(10, 20, 60, 40, 30, 40)aMutableArr -= 40println(aMutableArr) // ArrayBuffer(10, 20, 60, 30, 40)var temp = aMutableArr.remove(2)println(aMutableArr) // ArrayBuffer(10, 20, 30, 40) 元组 Tuple不同类型的值的集合。123val tuple = ("BigData", 2015, 45.0)println(tuple._1)println(tuple._2) 面向对象类基本类结构简单类123456789class Counter &#123; private var value = 0 def increment(): Unit = &#123; value += 1 &#125; def current(): Int = &#123; value &#125;&#125; 定义方法可以省略返回类型：123456789class Counter &#123; private var value = 0 def increment() &#123; value += 1 &#125; def current(): Int = &#123; value &#125;&#125; 方法传参123456789class Counter &#123; private var value = 0 def increment(step: Int) &#123; value += step &#125; def current(): Int = &#123; value &#125;&#125; 创建对象12345val myCounter = new CountermyCounter.increment()println(myCounter.current())val myCounter2 = new Counter() 编译&amp;执行如下定义的这样一个类在执行时，直接使用 scala 解释器执行即可，不需要编译。123456789class Counter &#123; private var value = 0 def increment() &#123; value += 1 &#125; def current(): Int = &#123; value &#125;&#125; 如果要编译，需要创建一个单例对象：1234567891011121314151617class Counter &#123; private var value = 0 def increment() &#123; value += 1 &#125; def current(): Int = &#123; value &#125;&#125;object MyCounter &#123; def main(args:Array[String]) &#123; val myCounter = new Counter myCounter.increment() println(myCounter.current) &#125;&#125; 编译（后面跟的是文件名）：1scalac counter.scala 编译之后会产生一些文件：123Counter.classMyCounter.classMyCounter$.class 执行执行时后面跟的是包含 main 方法的对象名称1scala -classpath . MyCounter getter &amp; setter1234567891011121314151617181920212223242526272829303132333435class Person &#123; private var privateName = "dcwang" private var privateAge = 25 // def name = privateName def setName(newName : String) &#123; privateName = newName &#125; def age = privateAge def setAge(newAge : Int) &#123; privateAge = newAge &#125; def grown(step : Int) &#123; privateAge += step &#125;&#125;object SomeOne &#123; def main(args:Array[String]) &#123; val someOne = new Person println(someOne.name) someOne.setName("yz") println(someOne.name) println(someOne.age) someOne.grown(2) println(someOne.age) &#125;&#125; 构造器Scala构造器包含一个主构造器和若干个辅构造器辅助构造器的名称为 this ，每个辅助构造器都必须调用一个已有的构造器 1234567891011121314151617181920212223242526272829303132333435363738class Person &#123; private var privateName = "dcwang" private var privateAge = 25 def this(name : String) &#123; this() // invoke main constructor this.privateName = name &#125; def this(name : String, age : Int) &#123; this(name) this.privateAge = age &#125; def name = privateName def setName(newName : String) &#123; privateName = newName &#125; def age = privateAge def setAge(newAge : Int) &#123; privateAge = newAge &#125; def grown(step : Int) &#123; privateAge += step &#125;&#125;object SomeOne &#123; def main(args:Array[String]) &#123; val someOne = new Person("yz", 18) println(someOne.name) println(someOne.age) &#125;&#125; 对象单例对象123456789101112object Person &#123; private var id = 0 def newId() = &#123; id += 1 id &#125;&#125;println(Person.newId())println(Person.newId())println(Person.newId()) 伴生对象在 Java 中经常用到同时包含实例方法和静态方法的类，在 Scala 中可以用伴生对象来实现类和它的伴生对象必须在同一个文件中，并且可以相互访问私有成员当单例对象与某个类具有相同的名称时，它被成为这个类的伴生对象 12345678910111213141516171819202122232425262728class Person &#123; private val id = Person.newPersonId() private var name = "" def this(name : String) &#123; this() this.name = name &#125; def info() &#123; println("The id of %s is %d. \n".format(name, id)) &#125;&#125;object Person &#123; private var lastId = 0 private def newPersonId() = &#123; lastId += 1 lastId &#125; def main(args : Array[String]) &#123; val person1 = new Person("yz") val person2 = new Person("yj") person1.info() person2.info() &#125;&#125; applay 方法update 方法继承在子类中重写超类抽象方法时不需要使用 override 关键字重写一个非抽象方法必须使用 override 修饰符只有主构造器可以调用超类的主构造器可以重写超类中的子段 抽象类12345678// 抽象类，不能直接实例化abstract class Car &#123; // 抽象子段，不需要初始化 val carBrand : String // 抽象方法，不需要使用 abstract 关键字 def info() def greeting() &#123; println("Welcome to my car!") &#125;&#125; 继承抽象类123456789101112class BMWCar extends Car &#123; override val carBrand = "BMW" // 重写抽象方法不用加 override def info() &#123; printf("This is a car") &#125; // 重写非抽象方法必须使用 override override def greeting() &#123; printf("something") &#125;&#125; 特质（trait）在 Scala 中没有接口的概念，而是提供了 trait，它实现了接口的功能，以及许多其他特性trait 是 Scala 中代码重用的基本单元，可以同时拥有抽象方法和具体方法在 Scala 中一个类只能继承一个超类，但是可以实现多个 trait，从而拥有 trait 中的方法和字段，实现多重继承。 123456789101112trait CarId &#123; var id : Int def currentId() : Int&#125;class BYDCarId extends CarId &#123; // 使用 extends 关键字继承 trait override var id = 10000 def currentId() : Int = &#123; id += 1; id &#125;&#125; 混入多个trait123456789101112131415161718trait CarId &#123; var id : Int def currentId() : Int&#125;trait CarGreeting &#123; def greeting(msg : String) &#123; println(msg) &#125;&#125;// 使用 extends 关键字继承 trait// 后面混入的多个 trait 可以反复使用 with 关键字class BYDCarId extends CarId with CarGreeting &#123; override var id = 10000 def currentId() : Int = &#123; id += 1; id &#125;&#125; 模式匹配简单匹配类似于 Java 中的 switch12345678910val colorNum = 1val colorStr = colorNum match &#123; case 1 =&gt; "red" case 2 =&gt; "green" case 3 =&gt; "yellow" case _ =&gt; "Not Allowed"&#125;println(colorStr) 获取匹配值可以声明一个变量 unexpected，用于获取进行匹配的值，然后在分支中进行操作。12345678910val colorNum = 4val colorStr = colorNum match &#123; case 1 =&gt; "red" case 2 =&gt; "green" case 3 =&gt; "yellow" case unexpected =&gt; unexpected + " is Not Allowed"&#125;println(colorStr) 类型模式可以匹配元素的类型，根据类型选择不同操作。1234567891011for (elem &lt;- List(9, 12.3, "Spark", "Hello")) &#123; val str = elem match &#123; case i : Int =&gt; i + " is an int value." case d : Double =&gt; d + " is a double value." case s : String =&gt; s + " is a string value." case "Hello" =&gt; "Hello is here." case _ =&gt; "unexpected value" &#125; println(str)&#125; 守卫（guard）语句将 case 的选项设置为全匹配，然后用 if 判断进行处理。那为什么不直接用判断？ case类的匹配一次匹配多个值，或者对比对象？12345678910111213case class Car(brand: String, price: Int)val myBYDCar = new Car("BYD", 89000)val myBMWCar = new Car("BMW", 1200000)val myBenzCar = new Car("Benz", 1500000)for (car &lt;- List(myBYDCar, myBMWCar, myBenzCar)) &#123; car match &#123; case Car("BYD", 89000) =&gt; println("BYD") case Car("BMW", 1200000) =&gt; println("BMW") case Car(brand, price) =&gt; println(brand + price) &#125;&#125; Option类型处理 None 返回值1234567val someMap = Map("spark" -&gt; 123, "hadoop" -&gt; 234)var someValue = someMap.get("hive")println(someValue.getOrElse("No such value")) // No such valuesomeValue = someMap.get("spark")println(someValue.getOrElse("No such value")) // 123 Option[T] 类中的 T 可以是各种数据类型，如果一个 Option 对象中包含值，那么这个对象就是 Some 类型，否则就是 None 类型。 如果返回值是一个集合，就可以对其使用 map 、foreach 或 filter 等方法1someMap.get("spark").foreach(println) 函数式编程函数定义函数字面量每个函数本身是一个值，可以被传递，类似于 JavaScript 中函数的概念。 函数的类型和值函数的类型是指传入参数和返回值的类型。1def counter(value: Int): Int = &#123; value += 1 &#125; 上面这个函数的类型就是： (Int) =&gt; Int如果有多个参数，使用逗号隔开；否则括号可以省略。 函数的值是指去掉了参数类型和返回值类型之后剩下的参数和函数体：1(value) =&gt; &#123; value += 1 &#125; 对比声明一个基本类型来声明一个函数：1val num : Int = 5 基本类型，声明一个变量 num，指定变量类型为 Int，然后给变量赋值为 5。 12var counter = (value : Int) =&gt; &#123; value + 1 &#125; : Intprintln(counter(2)) 匿名函数、Lamda表达式与闭包 Lamda 表达式123(参数) =&gt; 表达式(num: Int) =&gt; num * 2 闭包在一个函数内部可以访问外部变量的形式。123456var more = 1var addMore = (x: Int) =&gt; x + moreprintln(addMore(10))more = 5println(addMore(10)) 占位符语法 为了让函数字面量更简洁，可以使用下划线作为一个或多个参数的占位符，每个参数仅可以在函数字面量中出现一次。 1234567val numList = List(1, 2, 3, 4, 5)val res = numList.filter(x =&gt; x &gt; 3)print(res)val res2 = numList.filter(_ &gt; 3)print(res2) 针对集合的操作mapmap操作是针对集合的典型变换操作，将函数应用到集合中的每一个元素上，并产生一个新的结果集合。 12345val books = List("Hadoop", "Hive", "Spark")println(books) // List(Hadoop, Hive, Spark)val newbooks = books.map(s =&gt; s.toUpperCase)println(newbooks) // List(HADOOP, HIVE, SPARK) flatMap调用一个函数，将一个集合中的每个元素处理后形成的多个集合，合并成一个集合。123val books = List("Hadoop", "Hive", "Spark")val letters = books.flatMap(s =&gt; s.toList)println(letters) // List(H, a, d, o, o, p, H, i, v, e, S, p, a, r, k) filter遍历一个集合，过滤其中的元素形成一个新的集合。 1234val books = List("Hadoop", "Hive", "Spark")val newBooks = books.filter(&#123;value =&gt; value.contains("a")&#125;)println(newBooks) // List(Hadoop, Spark) reduce对给定集合中的两两元素，指定某给定函数的操作。1234567val numList = List(1, 3, 5)var res = numList.reduceLeft(&#123;_ - _&#125;)println(res) // -7res = numList.reduceRight(&#123;_ - _&#125;)println(res) // 3// 1 - (3 - 5) fold带初始值的规约12val numList = List(1, 3, 5)val res = numList.fold(10)(&#123;_ * _&#125;)]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>Syntax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7 安装 GeForce RTX 2080 Ti 驱动]]></title>
    <url>%2F2018%2F11%2F13%2FCentOS7-%E5%AE%89%E8%A3%85-GeForce-RTX-2080-Ti-%E9%A9%B1%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[摘要CentOS7 采用手动方式安装 RTX 2080 ti 驱动。 下载NVIDIA官方驱动到NVIDIA的官方驱动网站下载对应显卡的驱动程序，下载后的文件格式为run。 https://www.geforce.cn/drivers bios禁用禁用secure boot，也就是设置为disable如果没有禁用secure boot,会导致NVIDIA驱动安装失败，或者不正常。 https://www.technorms.com/45538/disable-enable-secure-boot-asus-motherboard-uefi-bios-utility 禁用nouveau(未验证)12345678910111213[root@iflysse123 ~]# lshw -numeric -C display *-display UNCLAIMED description: VGA compatible controller product: NVIDIA Corporation [10DE:1E07] vendor: NVIDIA Corporation [10DE] physical id: 0 bus info: pci@0000:01:00.0 version: a1 width: 64 bits clock: 33MHz capabilities: pm msi pciexpress vga_controller bus_master cap_list configuration: latency=0 resources: memory:f6000000-f6ffffff memory:e0000000-efffffff memory:f0000000-f1ffffff ioport:e000(size=128) memory:f7000000-f707ffff 观察 configuration 中是否有一项 driver=nouveau，如果有，就需要禁用掉 nouveau 打开编辑配置文件： /etc/modprobe.d/blacklist.conf 在最后一行添加： blacklist nouveau 这一条的含义是禁用nouveau第三方驱动，之后也不需要改回来。 由于nouveau是构建在内核中的，所以要执行下面命令生效: update-initramfs -u mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bakdracut -v /boot/initramfs-$(uname -r).img $(uname -r) 停止可视化桌面为了安装新的Nvidia驱动程序，我们需要停止当前的显示服务器。最简单的方法是使用telinit命令更改为运行级别3。执行以下linux命令后，显示服务器将停止，因此请确保在继续之前保存所有当前工作（如果有）： telinit 3 验证 12345678910111213[root@iflysse123 home]# lshw -numeric -C display *-display description: VGA compatible controller product: NVIDIA Corporation [10DE:1E07] vendor: NVIDIA Corporation [10DE] physical id: 0 bus info: pci@0000:01:00.0 version: a1 width: 64 bits clock: 33MHz capabilities: pm msi pciexpress vga_controller bus_master cap_list rom configuration: driver=nvidia latency=0 resources: irq:16 memory:f6000000-f6ffffff memory:e0000000-efffffff memory:f0000000-f1ffffff ioport:e000(size=128) memory:f7000000-f707ffff #123456789101112131415161718[root@iflysse123 home]# nvidia-smiFri Nov 9 17:14:38 2018 +-----------------------------------------------------------------------------+| NVIDIA-SMI 410.73 Driver Version: 410.73 CUDA Version: 10.0 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce RTX 208... Off | 00000000:01:00.0 Off | N/A || 32% 40C P0 56W / 280W | 0MiB / 10989MiB | 0% Default |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 参考https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-centos-7-linuxhttps://blog.csdn.net/wf19930209/article/details/81877822]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>NVIDIA</tag>
        <tag>2080ti</tag>
        <tag>Driver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Scrapyd 部署、管理 Scrapy 爬虫]]></title>
    <url>%2F2018%2F11%2F07%2F%E5%9F%BA%E4%BA%8E-Scrapyd-%E9%83%A8%E7%BD%B2%E3%80%81%E7%AE%A1%E7%90%86-Scrapy-%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[摘要Scrapy 是 Python 下一款非常好的爬虫框架，使用帮助快速实现爬虫。当爬虫数量较少时可以直接通过命令行的方式进行管理。但随着爬虫数量越来越多，版本不断更新，这时候就需要一些工具帮助我们进行爬虫管理了。而 Scrapyd 就是这样一个工具，其提供了一些基于 HTTP 的接口，帮助我们管理爬虫项目以及查看任务情况。 构建爬虫首先使用 scrapy 构建一个简单的爬虫，示例中随便使用了一个地址，只是请求页面，不做具体的内容解析和处理。 安装 Scrapy1pip install scrapy 如果是在 Windowns 上安装的话，最好先安装以下两块内容： Twisted-18.7.0-cp36-cp36m-win_amd64.whl 安装 twisted 编译好的版本，这个可以在如下网站下载： https://www.lfd.uci.edu/~gohlke/pythonlibs/ 安装，进入下载文件所在目录，使用如下命令：1pip install Twisted-18.7.0-cp36-cp36m-win_amd64.whl pywin32-223.win-amd64-py3.6.exe 这个直接双击安装即可。 然后使用如下命令安装 scrapy：1pip install scrapy 初始化 Scrapy 项目scrapy 提供了一个非常方便的命令直接创建好一个爬虫项目的框架，进入一个你想放置爬虫项目的目录，执行以下命令：1scrapy startproject IpSourceXici 命令执行完成后，会创建如下的一个目录结构： 创建爬虫在 spiders 目录中创建一个 py 文件，命名为 XiciSpider.py，其中添加以下内容： 123456789101112131415161718import scrapyclass XiciSpider(scrapy.Spider): name = &quot;XiciSpider&quot; def start_requests(self): for index in range(1, 7): url = &quot;http://www.89ip.cn/index_&quot; + str(index) + &quot;.html&quot; yield scrapy.Request(url=url, callback=self.parse) def parse(self, response): try: print(response.url) yield item except Exception as e: pass 这是一个非常简单的爬虫，仅仅访问了 URL，然后输出URL，没有进行任何处理。 配置爬虫根据需要在 settings.py 文件中设置一些配置。由于我们只是作示例，因此对爬取速度、数量等要求都不高，因此我们最好不要对目标服务器产生过高的负载。 USER_AGENT 伪装浏览器 1USER_AGENT = &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36&apos; CONCURRENT_REQUESTS 设置最大的并行数量，这里仅设置一个即可。默认不设置是16个。 1CONCURRENT_REQUESTS = 1 DOWNLOAD_DELAY 设置随机停顿，即在爬取同一个源头的网页时，设置停顿时间 DOWNLOAD_DELAY 乘以一个随机种子（通过 RANDOMIZE_DOWNLOAD_DELAY 开启）。单位是秒。 12RANDOMIZE_DOWNLOAD_DELAY = TrueDOWNLOAD_DELAY = 20 测试爬虫进入爬虫项目根目录，在这个示例中就是进入 IpSourceXici 目录，爬虫项目会构建两个 IpSourceXici 目录，进入最外层的一个即可，然后执行命令： 1scrapy crawl XiciSpider 之后会输出如下内容，说明爬虫没有问题： 配置 Scrapyd 服务安装 Scrapyd直接使用命令安装： 1pip install scrapyd 安装完成之后在命令行中输入命令启动 scrapyd 服务： 1scrapyd 服务默认监听 6800 端口。 测试 Scrapyd 服务直接在浏览器中访问地址： http://localhost:6800/ 可以查看到如下页面： 说明服务启动正常。当然，需要注意当前的 Available projects 后面应该是空的，因为我们还没有部署项目，这里是部署后的结果。 部署爬虫部署爬虫可以使用专门的部署工具 scrapyd-deploy。 安装 Scrapyd-clientscrapyd-deploy 是 scrapyd-client 中的一个命令。 如果是在 linux 平台下安装，可以直接使用命令： 1pip install scrapyd-client 不过这种方式在 windows 平台下安装会有问题，应该使用源码直接安装。 从 github 上下载： https://github.com/scrapy/scrapyd-client/releases 解压后进入源码的根目录，使用命令安装： 1python setup.py install 配置 scrapy.cfg修改爬虫项目根目录下文件 scrapy.cfg，参考以下内容： 123456[settings]default = IpSourceXici.settings[deploy:XiciProxy]url = http://localhost:6800/project = IpSourceXici 主要是取消 url 的注释，以及设置 deploy 的 Name。 部署进入爬虫所在目录使用命令：1scrapyd-deploy XiciProxy -p IpSourceXici -v r1 如果得到如下反馈，应该就是部署成功了1234Packing version r1Deploying to project &quot;IpSourceXici&quot; in http://localhost:6800/addversion.jsonServer response (200):&#123;&quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;project&quot;: &quot;IpSourceXici&quot;, &quot;version&quot;:&quot;r1&quot;, &quot;spiders&quot;: 1&#125; 验证可以直接访问：http://localhost:6800/ 查看以有效的项目，同时可以使用如下接口对项目进行简单的管理。 参考接口123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#列出所有工程http://localhost:6800/listprojects.json&#123; &quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;projects&quot;: [ &quot;IpSourceXici&quot; ]&#125;#查看爬虫http://localhost:6800/listspiders.json?project=IpSourceXici&#123; &quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;spiders&quot;: [ &quot;XiciSpider&quot; ]&#125;#列出版本http://localhost:6800/listversions.json?project=IpSourceXici&#123; &quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;versions&quot;: [ &quot;r1&quot;, &quot;r2&quot; ]&#125;#删除版本(POST)http://localhost:6800/delversion.jsonproject=IpSourceXiciversion=r2&#123; &quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;&#125;#执行爬虫(POST)http://localhost:6800/schedule.jsonproject=IpSourceXicispider=XiciSpider&#123; &quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;jobid&quot;: &quot;86a34534e23811e8b1413c970e0087f3&quot;&#125;#查看爬虫的执行状态http://localhost:6800/listjobs.json?project=IpSourceXici&#123; &quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;pending&quot;: [], &quot;running&quot;: [ &#123; &quot;id&quot;: &quot;86a34534e23811e8b1413c970e0087f3&quot;, &quot;spider&quot;: &quot;XiciSpider&quot;, &quot;pid&quot;: 5260, &quot;start_time&quot;: &quot;2018-11-07 10:55:06.534457&quot; &#125; ], &quot;finished&quot;: []&#125; 参考资料：[1] https://scrapyd.readthedocs.io/en/latest/index.html]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
        <tag>Scrapyd-client</tag>
        <tag>Scrapyd-deploy</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Hadoop 集群部署 ZooKeeper 和 HBase 集群]]></title>
    <url>%2F2018%2F11%2F05%2F%E5%9F%BA%E4%BA%8E-Hadoop-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2-ZooKeeper-%E5%92%8C-HBase-%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[摘要之前的环境中配置了 Hadoop 集群以及 Yarm。现在基于 HDFS 部署 ZooKeeper 集群和 HBase 集群。 三台机器中，将 131 作为 Master、其余两台作为 Region。另外设置 132 为 backup-master。三台机器都部署 zookeeper。 部署 Zookeeper在 weilu131/weilu132/weilu135三台上部署 zookeeper。 开放端口12345678910111213141516firewall-cmd --add-port=2888/tcp --permanentfirewall-cmd --add-port=3888/tcp --permanentfirewall-cmd --add-port=2181/tcp --permanentfirewall-cmd --add-port=16000/tcp --permanentfirewall-cmd --add-port=16010/tcp --permanentfirewall-cmd --add-port=16020/tcp --permanentfirewall-cmd --add-port=60000/tcp --permanentfirewall-cmd --add-port=50010/tcp --permanentfirewall-cmd --add-port=60020/tcp --permanentfirewall-cmd --add-port=60010/tcp --permanentfirewall-cmd --add-port=60030/tcp --permanentfirewall-cmd --reload 将安装包拷贝到 /usr/local/ 目录下，解压：1tar -zxvf zookeeper-3.4.5-cdh5.15.0.tar.gz zoo.cfg在 zookeeper 包的根目录创建一个文件夹，随意命名：1/usr/local/zookeeper-3.4.5-cdh5.15.0/zookeeperDataDir 进入 /usr/local/zookeeper-3.4.5-cdh5.15.0/conf 目录，拷贝一份 zoo_sample.cfg 文件，命名为 zoo.cfg：1cp zoo_sample.cfg zoo.cfg 在 zoo.cfg 中配置以下内容：123456789101112131415161718# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial synchronization phase can takeinitLimit=10# The number of ticks that can pass between sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.dataDir=/usr/local/zookeeper-3.4.5-cdh5.15.0/zookeeperDataDir# the port at which the clients will connectclientPort=2181server.1=weilu131:2888:3888server.2=weilu132:2888:3888server.3=weilu135:2888:3888 myid在每台 ZooKeeper 节点的数据目录（dataDir目录）下存放一个 myid 文件，文件中存放一个集群中唯一的ID，表明这台机器的ID，ID范围是 1~255 分发配置将 zoo.cfg 和 myid 分发到其他及台机器上，对应修改 myid。 1scp zoo.cfg root@weilu135:/usr/local/zookeeper-3.4.5-cdh5.15.0/conf 分发目录时增加参数 -r1scp -r zookeeperDataDir/ root@weilu151:/usr/local/zookeeper-3.4.5-cdh5.15.0/zookeeperDataDir 启动 zookeeper启动时需要分别在每个节点使用 bin/zkServer.sh 脚本启动：1/usr/local/zookeeper-3.4.5-cdh5.15.0/bin/zkServer.sh start 在所有节点启动完成之前，如果使用 ./zkServer.sh status 查看状态时，会提示123JMX enabled by defaultUsing config: /usr/local/zookeeper-3.4.5-cdh5.15.0/bin/../conf/zoo.cfgError contacting service. It is probably not running. 如果都启动之后，再次查看会得到：123JMX enabled by defaultUsing config: /usr/local/zookeeper-3.4.5-cdh5.15.0/bin/../conf/zoo.cfgMode: follower 部署 HBase将安装包拷贝到 /usr/local/ 目录下，解压 Hbase：1tar -zxvf hbase-1.2.0-cdh5.15.0.tar.gz hbase-env.sh配置 JDK 环境：1export JAVA_HOME=/usr/local/jdk1.8.0_181 配置不使用 HBase 默认自带的 ZooKeeper：1export HBASE_MANAGES_ZK=false hbase-site.xml创建临时文件目录：1mkdir -p /home/hbase/tmp 配置123456789101112131415161718192021222324252627282930&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hbase/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://weilu131:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;weilu131,weilu132,weilu135&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;60010&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.regionserver.info.port&lt;/name&gt; &lt;value&gt;60030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; regionservers配置 Region 服务器列表，在 regionservers 文件中配置以下内容，类似于 Hadoop 集群的 slaves 列表：12weilu132weilu135 backup-masters在 conf/backup-masters 中用来配置备份 master 服务器。1weilu132 分发配置1234567891011# hbase-env.sh 文件scp hbase-env.sh root@weilu132:/usr/local/hbase-1.2.0-cdh5.15.0/conf/# hbase-site.xmlscp hbase-site.xml root@weilu132:/usr/local/hbase-1.2.0-cdh5.15.0/conf/# regionserversscp regionservers root@weilu132:/usr/local/hbase-1.2.0-cdh5.15.0/conf/#backup-mastersscp backup-masters root@weilu132:/usr/local/hbase-1.2.0-cdh5.15.0/conf/ 启动服务1/usr/local/hbase-1.2.0-cdh5.15.0/bin/start-hbase.sh 验证对于 Master 可以访问以下地址，查看集群情况。 访问Master：http://192.168.0.131:60010 对于 Region 可以访问以下地址，查看情况。 访问Master：http://192.168.0.132:60030 问题zookeeper.MetaTableLocator: Failed verification of hbase:meta1234567892018-11-05 22:27:04,735 INFO [weilu131:60000.activeMasterManager] zookeeper.MetaTableLocator: Failed verification of hbase:meta,,1 at address=weilu135,60020,1541427295657, exception=org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on weilu135,60020,1541428018822 at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2997) at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegion(RSRpcServices.java:1069) at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegionInfo(RSRpcServices.java:1349) at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$2.callBlockingMethod(AdminProtos.java:22233) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2191) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:112) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:183) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:163) 这个问题是 zookeeper 的数据出错导致的，将 zookeeper 集群都停掉，然后将 Datadir目录中除了配置的 myid 以外的文件都删掉，然后启动 zookeeper，应该就可以了。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
        <tag>HBase</tag>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群部署方案]]></title>
    <url>%2F2018%2F10%2F30%2FHadoop%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[摘要记录一下 Hadoop 集群部署过程，简单的一拖三，包括 Hadoop 和 Yarn。 环境说明配置环境准备四台机器，四台机器环境是 CentOS 7.5，IP和主机名配置如下：1234192.168.0.131 weilu131192.168.0.132 weilu132192.168.0.135 weilu135192.168.0.151 weilu151 注意：主机名不可以有下划线 前置配置免密登录生成密钥：1ssh-keygen 这个会生成在 /root/.ssh/ 目录下，然后进入该目录，将公钥拷贝到其它两台机器上：1ssh-copy-id -i id_rsa.pub root@weilu132 具体可以参考：https://weilu2.github.io/2018/10/08/CentOS-7-4-%E9%85%8D%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/ JDK环境从本地将JDK包拷贝到机器上：123[root@weilu_125 packages]# scp jdk-8u181-linux-x64.tar.gz root@192.168.0.131:/usr/local/root@192.168.0.131&apos;s password: jdk-8u181-linux-x64.tar.gz 100% 177MB 11.1MB/s 00:15 解压到当前目录：1tar -xvzf jdk-8u181-linux-x64.tar.gz 配置环境变量：1vim /etc/profile 在其中末尾添加以下内容：123export JAVA_HOME=/usr/local/jdk1.8.0_181 export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin 然后重新加载配置文件：1source /etc/profile 检查配置结果：1234[root@weilu_132 local]# java -versionjava version &quot;1.8.0_181&quot;Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) 用相同的方法将其余两台机器也配置好。 防火墙配置如果开启了防火墙，那么就需要开启以下端口 9000这个端口是 Hadoop 集群中 NameNode 与 DataNode 通信的端口。 8031这个端口是 Yarn 的 ResourceManager 与 NodeManager 通信的端口。 配置 Hadoop下载：http://archive.cloudera.com/cdh5/cdh/5/ 文件放置在 /usr/local/ 下，解压缩：1tar -zxvf hadoop-2.6.0-cdh5.15.0.tar.gz hadoop-env.sh修改 hadoop 环境配置中的 JDK 配置：1vim /usr/local/hadoop-2.6.0-cdh5.15.0/etc/hadoop/hadoop-env.sh 修改其中的 JAVA_HOME1export JAVA_HOME=/usr/local/jdk1.8.0_181 core-site.xml配置 NameNode 的URI：123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://weilu131:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml配置 NameNode配置 NameNode 在本地文件系统中存放内容的位置。首先创建目录：1mkdir -p /home/hadoop/tmp/dfs/name 配置 DataNode配置 DataNode 在本地文件系统中存放内容的位置。首先创建目录：1mkdir -p /home/hadoop/tmp/dfs/data 这个目录是自己定义的。12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置 Yarnyarn-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;weilu131&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置 MapReducemapred-site.xml在每个节点上配置以下内容。 从模板中拷贝一份 mapred-site.xml 文件：1cp mapred-site.xml.template mapred-site.xml 在新文件中添加以下内容：123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置从节点slaves在 slaves 文件中列出从节点的主机名或IP： 123weilu132weilu135weilu151 配置内存分配默认的内存分配策略是基于至少 8G 内存的分配，如果所使用的机器内存少于 8G，就需要进行手动配置。 分配策略一个 YARN 的 job 执行需要两类资源： 一个 Application Master(AM)，负责监控应用以及在集群上协调分布的 executor； 若干 executor，由 AM 创建，实际 job 的执行者。 他们都运行在从节点的容器中。每个从节点运行一个 NodeManager 守护进程，负责在节点上创建容器。整个集群在一个 ResourceManager 的管理下，RM 负责在从节点上调度容器等。 整个集群正常工作，需要配置四类资源：1、每个节点允许所有 YARN 容器占用的总的内存。 这个内存应该比其他所有的都大，否则应用程序无法正常执行。 这个值通过 yarn-site.xml 文件中的 yarn.nodemanager.resource.memory-mb 配置。 2、单个容器允许占用的内存 每个节点上可以有多个容器，单个容器需要配置允许的最大内存和最小内存。 通过 yarn-site.xml 文件中的 yarn.scheduler.maximum-allocation-mb 和 yarn.scheduler.minimum-allocation-mb 配置。 3、ApplicationMaster 允许的内存 这个值是在容器最大内存限制内的一个常数值。 通过 mapred-site.xml 文件中的 yarn.app.mapreduce.am.resource.mb 配置。 4、每个 map 和 reduce 操作允许的最大内存 通过 mapred-site.xml 文件中的 mapreduce.map.memory.mb 和 mapreduce.reduce.memory.mb 配置。 这些配置之间的关系可以用下图表示： 内存分配 属性 内存大小 yarn.nodemanager.resource.memory-mb 3000 yarn.scheduler.maximum-allocation-mb 3000 yarn.scheduler.minimum-allocation-mb 256 yarn.app.mapreduce.am.resource.mb 512 mapreduce.map.memory.mb 512 mapreduce.reduce.memory.mb 512 yarn-site.xml在 yarn-site.xml 文件中添加以下内容： 12345678910111213141516&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;3000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;3000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;256&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; mapred-site.xml在 mapred-site.xml 文件中添加以下内容： 123456789101112&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt;&lt;/property&gt; 分发配置文件注意，以上所有配置都是在 weilu131 上配置的，完成之后，将配置文件分发到其他几台机器上，每台机器的配置文件都是一模一样的，下面使用 scp 命令分发，以从节点文件为例： 123scp slaves root@weilu_132:/usr/local/hadoop-2.6.0-cdh5.15.0/etc/hadoopscp slaves root@weilu_135:/usr/local/hadoop-2.6.0-cdh5.15.0/etc/hadoopscp slaves root@weilu_151:/usr/local/hadoop-2.6.0-cdh5.15.0/etc/hadoop 格式化文件系统123[root@weilu_131 bin]# /usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs namenode -format...18/10/30 21:12:08 INFO common.Storage: Storage directory /home/hadoop/tmp/dfs/name has been successfully formatted. 看到有这行输出表明格式化成功。 启动12345678910111213141516[root@weilu_131 sbin]# /usr/local/hadoop-2.6.0-cdh5.15.0/sbin/start-all.sh This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh18/10/30 22:17:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableStarting namenodes on [weilu131]weilu131: starting namenode, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/hadoop-root-namenode-weilu131.outweilu132: starting datanode, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/hadoop-root-datanode-weilu_132.outweilu135: starting datanode, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/hadoop-root-datanode-weilu_135.outweilu151: starting datanode, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/hadoop-root-datanode-weilu_151.outStarting secondary namenodes [0.0.0.0]0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/hadoop-root-secondarynamenode-weilu131.out18/10/30 22:17:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicablestarting yarn daemonsstarting resourcemanager, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/yarn-root-resourcemanager-weilu_131.outweilu132: starting nodemanager, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/yarn-root-nodemanager-weilu_132.outweilu135: starting nodemanager, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/yarn-root-nodemanager-weilu_135.outweilu151: starting nodemanager, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/yarn-root-nodemanager-weilu_151.out 验证验证进程在 weilu131上：12345[root@weilu131 sbin]# jps3927 NameNode4520 Jps4254 ResourceManager4111 SecondaryNameNode 因为在 weilu131 上部署了 NameNode 和 ResourceManager，因此使用 jps 命令应该能够看到这几个进程。 在其余几个节点上：1234[root@weilu132 hadoop]# jps1290 DataNode1531 Jps1391 NodeManager 因为其余几个节点上面只部署了 DataNode 和 NodeManager，因此使用 jps 命令应该能够看到这几个进程。 Hadoop Web 端验证访问：http://192.168.0.131:50070 可以看到以下页面： 打开其中的 Live Node，可以看到目前存活的节点： Yarn Web 端验证 打开其中的 Active Node，可以看到目前存活的节点： 问题集群正常启动 50070 页面显示没有 Live NodeNameNode 和 DataNode 都正常启动，但是访问 50070 页面发现检测不到任何节点。查看 DataNode 的日志，发现如下内容：12342018-10-30 23:02:04,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: weilu131/192.168.0.131:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)2018-10-30 23:02:05,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: weilu131/192.168.0.131:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)2018-10-30 23:02:06,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: weilu131/192.168.0.131:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)2018-10-30 23:02:07,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: weilu131/192.168.0.131:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 一种可能性是由于 NameNode 的防火墙开着，并且不允许访问 9000 端口，这样 DataNode 就没法向 NameNode 报告状态，将9000端口开启后，该问题马上解决了，验证了该猜想。 集群正常启动 Yarn 页面上看不到活动节点查看 NodeManager 节点上的 Yarn 启动日志，可以看到以下错误信息：123org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from weilu132/192.168.0.132 to weilu131:8031 failed on socket timeout exception: java.net.NoRouteToHostException: 没有到主机的路由; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.serviceStart(NodeStatusUpdaterImpl.java:215)... 问题还是一样的问题，无法和 ResourceManager 节点进行通讯，yarn 使用的是 8031 端口，设置一下，然后重启集群。 参考[1] https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>集群</tag>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 设置防火墙开放指定端口]]></title>
    <url>%2F2018%2F10%2F30%2FCentOS-7-%E8%AE%BE%E7%BD%AE%E9%98%B2%E7%81%AB%E5%A2%99%E5%BC%80%E6%94%BE%E6%8C%87%E5%AE%9A%E7%AB%AF%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[摘要记录一下 Linux 常用的关于防火墙端口操作的命令。 查看已打开的端口1netstat -anp 查看想开的端口是否已开1firewall-cmd --query-port=666/tcp 若此提示 FirewallD is not running表示为不可知的防火墙 需要查看状态并开启防火墙 查看防火墙状态1systemctl status firewalld running 状态即防火墙已经开启dead 状态即防火墙未开启 开启防火墙1systemctl start firewalld 没有任何提示即开启成功 或者： 1service firewalld start 关闭防火墙1systemctl stop firewalld centos7.3 上述方式可能无法开启，可以先1systemctl unmask firewalld.service 然后 1systemctl start firewalld.service 查看想开的端口是否已开1firewall-cmd --query-port=666/tcp 提示no表示未开 开永久端口号1firewall-cmd --add-port=666/tcp --permanent 提示 success 表示成功 重新载入配置1firewall-cmd --reload 比如添加规则之后，需要执行此命令 移除端口1firewall-cmd --permanent --remove-port=666/tcp]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>firewall</tag>
        <tag>防火墙</tag>
        <tag>port</tag>
        <tag>端口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 网络配置之初始配置、静态IP以及网桥]]></title>
    <url>%2F2018%2F10%2F27%2FCentOS-7-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E4%B9%8B%E5%88%9D%E5%A7%8B%E9%85%8D%E7%BD%AE%E3%80%81%E9%9D%99%E6%80%81IP%E4%BB%A5%E5%8F%8A%E7%BD%91%E6%A1%A5%2F</url>
    <content type="text"><![CDATA[摘要记录一下 CentOS 7 的网络配置，一开始是安装完的初始化配置，动态分配IP；然后改成了静态IP；之后又修改为配置网桥。 初始配置123456789101112131415TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=dhcpDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s31f6UUID=98eb9b94-878c-4430-8ce5-13471bb46997DEVICE=enp0s31f6ONBOOT=no 静态IP12345678910111213141516171819202122TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s31f6UUID=98eb9b94-878c-4430-8ce5-13471bb46997DEVICE=enp0s31f6ONBOOT=yesIPADDR=192.168.0.123GATEWAY=192.168.0.1NETMASK=255.255.255.0NM_CONTROLLED=noDNS1=8.8.8.8DNS2=8.8.4.4 网桥ifcfg-br0文件： 123456789TYPE=BridgeNAME=br0DEVICE=br0ONBOOT=yesBOOTPROTO=staticIPADDR=192.168.0.123GATEWAY=192.168.0.1NETMASK=255.255.255.0DNS1=192.168.0.1 ifcfg-enp0s31f6文件： 123456TYPE=EthernetBRIDGE=br0NAME=enp0s31f6UUID=99244a4d-8cac-4023-9a09-8e50c547cd3aDEVICE=enp0s31f6ONBOOT=yes]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Bridge</tag>
        <tag>Network</tag>
        <tag>Static IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用MAT对Java程序进行内存分析之小试牛刀]]></title>
    <url>%2F2018%2F10%2F26%2F%E4%BD%BF%E7%94%A8MAT%E5%AF%B9Java%E7%A8%8B%E5%BA%8F%E8%BF%9B%E8%A1%8C%E5%86%85%E5%AD%98%E5%88%86%E6%9E%90%E4%B9%8B%E5%B0%8F%E8%AF%95%E7%89%9B%E5%88%80%2F</url>
    <content type="text"><![CDATA[摘要今天在写爬虫的时候发现数据抓到五十万左右时候，进程占用内存1.5G左右，程序完全卡死了。爬虫是用 WebMagic 框架写的，逻辑也不复杂，看不出什么问题；阅读了 WebMagic 的源码，看着也没有什么问题。就尝试通过内存分析的方式看看能否找出问题的原因。 启动程序，打开 %JAVA_HOME%/bin/jconsole.exe 工具，根据进程ID连接到运行中到Java程序 可以看到程序在执行过程中不断的占用内容，然后被GC回收内存。 程序初始化的时候读取了大概五十万条左右数据，所以一开始就占用了500M左右的内存，后续GC回收时，一般在500M左右徘徊 随着程序执行时间不断累积，可以看到内存占用越来越大了，GC回收后相对于初始化时，仍然有一百多M没有回收掉 运行了大概半个小时之后程序挂，在运行期间使用 jmap 工具保存了若干个时间节点下的内存快照。 12345678910111213C:\Java\jdk1.8.0_151\bin&gt;jmap -dump:format=b,file=D:/jvmdump/heap.bin 4344Dumping heap to D:\jvmdump\heap.bin ...Heap dump file createdC:\Java\jdk1.8.0_151\bin&gt;jmap -dump:format=b,file=D:/jvmdump/heap.bin 4344Dumping heap to D:\jvmdump\heap.bin ...File exists...C:\Java\jdk1.8.0_151\bin&gt;jmap -dump:format=b,file=D:/jvmdump/heap10.bin 4344Dumping heap to D:\jvmdump\heap10.bin ...Heap dump file created jmp 工具生成的是内存快照，因此每个文件的大小就是当前程序所使用内存的大小。 接下来使用 MAT（Memory Analyzer Tool）工具对内存进行分析。 MAT工具下载地址：http://www.eclipse.org/mat/downloads.php 对比查看不同时间段占用内存最大的对象情况。 通过这个对比分析，可以很明显看到内存占用最大的对象是数据库连接对象，结合我的程序，能够判断出来应该是使用的数据连接池有问题。更换一个连接池测试一下发现，确实是这个问题。这样，就未完成了一次简单的内存分析。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>MAT</tag>
        <tag>内存分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无法启动此程序，因为计算机中丢失 api-ms-win-crt-runtime-l1-1-0.dll]]></title>
    <url>%2F2018%2F10%2F26%2F%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E6%AD%A4%E7%A8%8B%E5%BA%8F%EF%BC%8C%E5%9B%A0%E4%B8%BA%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E4%B8%A2%E5%A4%B1-api-ms-win-crt-runtime-l1-1-0-dll%2F</url>
    <content type="text"><![CDATA[摘要我这边碰到的错误是在安装 MySQL Workbench 时出现的，这个文件是 Visual C++ Redistributable 下的一个文件。我按照其官网上标注的安装前提，安装了 Visual C++ 2015 Redistributable for Visual Studio 2015，重新安装 Workbench 仍然报该错误。 问题描述 解析其主要原因是在安装 Visual C++ 2015 Redistributable for Visual Studio 2015 时，系统中已经存在了某个版本了，并且文件 api-ms-win-crt-runtime-l1-1-0.dll 可能正在被某个程序所引用，在重新安装时，并没有将之前的 api-ms-win-crt-runtime-l1-1-0.dll 文件更新覆盖，从而导致没有真正解决该问题。 解决方法解决方法就是先找到该文件，将其删除，删除的过程中涉及到某个程序引用这个文件的，先将那个程序退出再删除。 在目录 C:\Windows\System32和C:\Windows\SysWOW64 下查找 api-ms-win-crt-runtime-l1-1-0.dll 文件，如果有，就将其删除。 然后重新安装 Visual C++ 2015 Redistributable for Visual Studio 2015，安装之后可能要重启，之后再安装 Workbench 即可正常运行。 对应到在安装其他程序过程中碰到该问题的情况，只需要按照相同的方法，安装对应版本的 Visual C++ 即可。]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Visual C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL启动错误：在本地计算机上的MySQL57服务启动后停止。某些服务在未由其他服务或程序使用时将自动停止]]></title>
    <url>%2F2018%2F10%2F26%2FMySQL%E5%90%AF%E5%8A%A8%E9%94%99%E8%AF%AF%EF%BC%9A%E5%9C%A8%E6%9C%AC%E5%9C%B0%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8A%E7%9A%84MySQL57%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8%E5%90%8E%E5%81%9C%E6%AD%A2%E3%80%82%E6%9F%90%E4%BA%9B%E6%9C%8D%E5%8A%A1%E5%9C%A8%E6%9C%AA%E7%94%B1%E5%85%B6%E4%BB%96%E6%9C%8D%E5%8A%A1%E6%88%96%E7%A8%8B%E5%BA%8F%E4%BD%BF%E7%94%A8%E6%97%B6%E5%B0%86%E8%87%AA%E5%8A%A8%E5%81%9C%E6%AD%A2%2F</url>
    <content type="text"><![CDATA[摘要在 MySQL 服务启动时，出现错误“在本地计算机上的MySQL57服务启动后停止。某些服务在未由其他服务或程序使用时将自动停止”，服务无法正常启动。 问题描述这个问题是在MySQL服务停掉之后，重新启动时出现的，情形如下： 问题原因和解决方法一开始是有点蒙的，后来想到刚刚停掉服务时，是为了修改配置文件 my.ini 的： 我在配置文件中配置了服务器的字符集，实际上应该写作utf8，多了一个横杠之后就会报错，去掉后重启就正常了。 这个配置文件错误，是导致这个问题的一个原因。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>my.ini</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Configure Maven Java Project in Idea on CentOS 7]]></title>
    <url>%2F2018%2F10%2F20%2FConfigure-Maven-Java-Project-in-Idea-on-CentOS-7%2F</url>
    <content type="text"><![CDATA[摘要介绍如何在 CentOS 7 环境下安装配置 JDK、Maven、Idea。以及如何创建一个可以运行的 Maven Java 项目。 卸载 OpenJDK使用命令查看当前系统的 jdk 版本：1234[root@localhost ~]# java -versionopenjdk version &quot;1.8.0_161&quot;OpenJDK Runtime Environment (build 1.8.0_161-b14)OpenJDK 64-Bit Server VM (build 25.161-b14, mixed mode) 确实是 openjdk，然后使用 yum 命令查看有多少个包：12345678[root@localhost ~]# yum list installed | grep javajava-1.7.0-openjdk.x86_64 1:1.7.0.171-2.6.13.2.el7 @anacondajava-1.7.0-openjdk-headless.x86_64 1:1.7.0.171-2.6.13.2.el7 @anacondajava-1.8.0-openjdk.x86_64 1:1.8.0.161-2.b14.el7 @anacondajava-1.8.0-openjdk-headless.x86_64 1:1.8.0.161-2.b14.el7 @anacondajavapackages-tools.noarch 3.4.1-11.el7 @anacondapython-javapackages.noarch 3.4.1-11.el7 @anacondatzdata-java.noarch 2018c-1.el7 @anaconda 可以看到，系统中有两个版本的 openjdk，我们先删除 1.7 的：1234567[root@localhost ~]# yum -y remove java-1.7.0-openjdk*...删除: java-1.7.0-openjdk.x86_64 1:1.7.0.171-2.6.13.2.el7 java-1.7.0-openjdk-headless.x86_64 1:1.7.0.171-2.6.13.2.el7 完毕！ 用同样的方法将 1.8 的也删除即可。12yum -y remove java-1.8.0-openjdk*... 安装 JDK下载 JDKhttp://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 这里下载源码版本的。 将包移动到 /usr/local/ 目录下，并解压：1tar -xvzf jdk-8u181-linux-x64.tar.gz 配置环境变量打开配置文件：1vim /etc/profile 在文件最后添加如下内容：123export JAVA_HOME=/usr/local/jdk1.8.0_181export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/bin 效果如下： 保存后退出，然后重新加载配置文件：1source /etc/profile 测试1234[root@localhost local]# java -versionjava version &quot;1.8.0_181&quot;Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) 安装 Idea下载：https://www.jetbrains.com/idea/download/#section=linux得到文件：ideaIC-2018.2.4.tar.gz 将该文件拷贝到 /usr/local 目录下，然后解压：1tar -zxvf ideaIC-2018.2.4.tar.gz 进入该目录，其中有一个 Install-Linux-tar.txt 文件，其中说明了如何安装。 进入该文件夹的 bin 目录中，执行：1./idea.sh 第一次执行时，会生成一些配置文件，以及要求你进行一些设置。 安装 Maven下载：1wget http://mirror.bit.edu.cn/apache/maven/maven-3/3.5.4/binaries/apache-maven-3.5.4-bin.tar.gz 将下载文件拷贝到 /usr/local/ 中然后解压：1tar -zxvf /usr/local/apache-maven-3.5.4-bin.tar.gz 创建 Java 项目在 IDEA 的顶部的菜单中 File -&gt; New -&gt; Project 打开如下界面，选择 Maven，设置好项目所使用的 SDK，勾选 Create from archetype，然后从下面选择一个 Maven 的QuickStart 项目模板。 填写项目的 Maven 相关信息： 这里注意，使用的 Maven Home 目录选择你所下载的 Maven 版本，同时将Maven的设置文件和仓库目录修改为你自定义的。 这里基本不用修改，如果需要可以更改项目名称和存放路径。 之后，项目的创建可能需要一些时间来下载和引入相关的模板文件。 完成之后，你会在信息框中看到 BUILD SUCCESS的字样，不过这个时候这个Java项目是没有办法直接运行的。需要点击右下角的方框中的 Enable Auto Import才行。 之后，项目就变成一个可执行的项目了，在 Main 函数所在的类中右击，会出现 Run &#39;App.main()&#39; 的选项。如果找不到这个右下角的小方块，那么在项目名称上右击，选择Maven -&gt; Reimport 可以达到相同的效果。]]></content>
      <categories>
        <category>Idea</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Maven</tag>
        <tag>Idea</tag>
        <tag>Run Main</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 KickStart 无人值守的全命令行 KVM 虚拟机安装过程]]></title>
    <url>%2F2018%2F10%2F14%2F%E5%9F%BA%E4%BA%8E-KickStart-%E6%97%A0%E4%BA%BA%E5%80%BC%E5%AE%88%E7%9A%84%E5%85%A8%E5%91%BD%E4%BB%A4%E8%A1%8C-KVM-%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[摘要考虑到服务器没有配置界面，而需要在服务器上配置一些虚拟机，可以使用基于 KickStart 无人值守的方式，基于命令行来安装配置和使用虚拟机。安装完成的虚拟机直接可以使用SSH在局域网内连接。 配置主机环境libvirt-client.x86_64 0:3.9.0-14.el7_5.8 Libvirt 的客户端，主要功能是在宿主机关机时通知虚拟机正常关机，防止强制关机导致数据丢失。 qemu-kvm.x86_64 10:1.5.3-156.el7_5.5KVM在用户控件运行的程序 virt-manager.noarch 0:1.4.3-3.el7基于 libvirt 的可视化虚拟机管理工具 libvirt.x86_64 0:3.9.0-14.el7_5.8用于管理虚拟机的APi virt-viewer.x86_64 0:5.0-10.el7显示虚拟机控制台的console virt-top.x86_64 0:1.0.8-24.el7 查看虚拟机的资源使用情况，类似于top命令 qemu-img-1.5.3-156.el7_5.5.x86_64 virt-install-1.4.3-3.el7.noarch虚拟机安装工具 使用 HTTP 服务提供安装镜像1、安装 HTTP 服务通过运行下面命令安装 HTTP 服务：1yum install httpd 2、拷贝 ISO 镜像从网络下载或是从其他服务器将 CentOS 7 的二进制 DVD ISO 镜像拷贝到 HTTP 服务所在的主机上。这里拷贝到如下位置：1/home/packages/CentOS-7-x86_64-DVD-1804.iso 3、挂载镜像首先在 /mnt 目录下创建一个目录 /mnt/ctos7-install，这个目录名字随意。然后使用以下命令挂载镜像：1mount -o loop,ro -t iso9660 /home/packages/CentOS-7-x86_64-DVD-1804.iso /mnt/ctos7-install 4、拷贝安装包使用如下命令将安装镜像中的文件拷贝到 HTTP 服务的目录中：1cp -r /mnt/ctos7-install/ /var/www/html/ 拷贝完成之后，可以在 /var/www/html/ctos7-install 目录下看到以下内容：1234[root@weilu_125 ctos7-install]# lsCentOS_BuildTag GPL LiveOS RPM-GPG-KEY-CentOS-7EFI images Packages RPM-GPG-KEY-CentOS-Testing-7EULA isolinux repodata TRANS.TBL 这些内容就是镜像中的文件。 5、启动 HTTP 服务1systemctl start httpd.service 6、开放端口HTTP 服务默认使用的 80 端口一般防火墙是没有开放的，可以使用命令检查：1firewall-cmd --query-port=80/tcp 如果返回的是 no 则表示没有开启，使用如下命令开启：12345# 开启端口firewall-cmd --add-port=80/tcp --permanent# 重新载入配置firewall-cmd --reload 7、测试在浏览器中输入这台主机的 IP 地址加上路径：1http://192.168.0.125/ctos7-install/ 可以看到如下内容，说明HTTP服务配置成功。 编写 Kickstart 文件在 HTTP 服务目录下创建一个文件 /var/www/html/kickstart/ks.cfg，将如下内容填充到文件中：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586################################################################# Environment setup################################################################# url --url=&quot;http://192.168.0.125/kickstart/ks.cfg&quot;textcdromauth --enableshadow --passalgo=sha512keyboard --vckeymap=us --xlayouts=&apos;us&apos;lang en_US.UTF-8eula --agreedreboot################################################################# network configuration################################################################network --bootproto=static --ip=192.168.0.151 --gateway=192.168.0.1 --netmask=255.255.255.0 --noipv6 --device=eth0 --nameserver=192.168.0.1,8.8.8.8 --activatenetwork --hostname=weilu_151timezone Asia/Shanghai --isUtc################################################################# partitioning################################################################ignoredisk --only-use=vdabootloader --location=mbr --boot-drive=vdazerombrclearpart --none --initlabelautopart --type=lvm# part swap --asprimary --fstype=&quot;swap&quot; --size=1024# part /boot --fstype xfs --size=200# part pv.01 --size=1 --grow# volgroup rootvg01 pv.01# logvol / --fstype xfs --name=lv01 --vgname=rootvg01 --size=1 --grow############################################################################################ # User Accounts# Generate encrypted password: python -c &apos;import crypt; print(crypt.crypt(&quot;My Password&quot;))&apos;# Or openssl passwd -1 password############################################################################################rootpw king # user --groups=wheel --name=josepy --password=password --gecos=&quot;Mutai Josphat&quot;################################################################# SELinux and Firewalld#################################################################selinux --enforcing#selinux --permissiveselinux --disabled firewall --enabled --http --ssh --ftp --port=https:tcp --port=ipp:tcp# firewall --disabled ################################################################# Software Packages################################################################%packages --nobase --ignoremissing@core@basevim bash-completion%end 根据实际情况修改文件中的以下内容：键盘和语言设置12keyboard --vckeymap=us --xlayouts=&apos;us&apos;lang en_US.UTF-8 网络配置这里配置的是使用网桥连接，固定IP：1network --bootproto=static --ip=192.168.0.151 --gateway=192.168.0.1 --netmask=255.255.255.0 --noipv6 --device=eth0 --nameserver=192.168.0.1,8.8.8.8 --activate 主机名1network --hostname=weilu_151 时区1timezone Asia/Shanghai --isUtc 这里可以使用命令 timedatectl list-timezones 查看所有时区的列表。 root密码1rootpw king 在 rootpw 指令后面跟的就是root账号的密码。 创建虚拟机使用如下命令创建虚拟机，之后整个过程会自动进行，不需要交互操作：123456789101112virt-install \ --name centos7-3 \ --memory 2048 \ --vcpus 2 \ --disk path=/home/kvm3/centos7.0.qcow2,size=50 \ --location http://192.168.0.125/ctos7-install/ \ --os-variant centos7.0 \ --network bridge:br0 \ --graphics=none \ --console pty,target_type=serial \ -x &apos;console=ttyS0,115200n8 serial&apos; \ -x &quot;ks=http://192.168.0.125/kickstart/ks.cfg&quot; 执行该命令后，会自动安装配置虚拟机： 安装完成之后，命令行会自动连接登录到虚拟机中，输入用户名密码即可登录虚拟机。 管理虚拟机以命令行启动虚拟机以命令行启动虚拟机，并将该命令行连接到虚拟机中的命令行：1virsh start centos7-3 --console 如果期间出现以下错误：Active console session exists for this domain完整信息：12345[root@weilu_125 Pictures]# virsh start centos7-3 --consoleDomain centos7-3 startedConnected to domain centos7-3Escape character is ^]error: operation failed: Active console session exists for this domain 只需要重启虚拟机守护进程即可：1systemctl restart libvirtd.service 连接虚拟机对于已经启动的虚拟机，可以使用以下命令以命令行的方式连接虚拟机：1virsh console centos7-3 问题下面罗列一些在这个过程中可能碰到的问题。 WARNING KVM acceleration not available, using ‘qemu’这个问题的直观表现就是执行 virt-install 命令之后就卡住了，如下： 12345678910111213141516171819[root@weilu_123 centos7-1]# virt-install \&gt; --name centos7-1 \&gt; --memory 2048 \&gt; --vcpus 2 \&gt; --disk path=/home/kvms/centos7-1/centos7.0.qcow2,size=200 \&gt; --location http://192.168.0.123/ctos7-install/ \&gt; --os-variant centos7.0 \&gt; --network bridge:br0 \&gt; --graphics=none \&gt; --console pty,target_type=serial \&gt; -x &apos;console=ttyS0,115200n8 serial&apos; \&gt; -x &quot;ks=http://192.168.0.123/kickstart/ks.cfg&quot;开始安装......搜索文件 vmlinuz...... | 5.9 MB 00:00 搜索文件 initrd.img...... | 50 MB 00:00 正在分配 &apos;centos7.0.qcow2&apos; | 200 GB 00:00 连接到域 centos7-1换码符为 ^] 这个问题是BIOS的CPU虚拟化功能没有开启导致的。 只要进入 BIOS，将Intel Virtualization Technology开启即可，下面例子是华硕主板： 参考[1] RHEL and CentOS Kickstart on KVM Automated Installation With virt-install[2] CentOS 7 INSTALLING IN TEXT MODE[3] Use VNC mode to install CentOS 7[4] Kickstart Installation]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
        <tag>KickStart</tag>
        <tag>无人值守</tag>
        <tag>命令行安装虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DIY饭团的烘干机——全环绕立体暖风]]></title>
    <url>%2F2018%2F10%2F10%2FDIY%E9%A5%AD%E5%9B%A2%E7%9A%84%E7%83%98%E5%B9%B2%E6%9C%BA%E2%80%94%E2%80%94%E5%85%A8%E7%8E%AF%E7%BB%95%E7%AB%8B%E4%BD%93%E6%9A%96%E9%A3%8E%2F</url>
    <content type="text"><![CDATA[摘要日常给饭团洗澡后，吹干饭团的毛发实在是一个痛苦的过程，一个小吹风机两三个小时，实在伤不起，本打算买个烘干箱，但价格勉强可以接受的，箱子实在看不上，看得上的，价格实在是不美丽，因此打算自己DIY一个，先做个设计和预算再说。 饭团镇楼 设计草图整体草图 正面 左侧 右侧 背部 底部]]></content>
      <categories>
        <category>饭团的小日子</category>
      </categories>
      <tags>
        <tag>烘干机</tag>
        <tag>DIY</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7.4 配置SSH免密登录]]></title>
    <url>%2F2018%2F10%2F08%2FCentOS-7-4-%E9%85%8D%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[摘要目前局域网中参与配置的有三台机器，要配置这三台机器免密登录。 配置主机名配置主机名其实相当于在这三台机器上各自备份一个路由表，由IP到主机名之间的映射。下面以 177.11.12.115 为例，修改主机名：1vi /etc/hostname 将其中的内容修改为：1oolong116 然后增加映射：1vi /etc/hosts 在文件末尾添加以下内容：123177.11.12.113 oolong113177.11.12.115 oolong115177.11.12.115 oolong116 别忘了重启一下电脑，不然hostname不会生效1reboot 测试：12345[root@localhost local]# ping oolong115PING oolong115 (177.11.12.115) 56(84) bytes of data.64 bytes from oolong115 (177.11.12.115): icmp_seq=1 ttl=64 time=0.230 ms64 bytes from oolong115 (177.11.12.115): icmp_seq=2 ttl=64 time=0.121 ms64 bytes from oolong115 (177.11.12.115): icmp_seq=3 ttl=64 time=0.115 ms 这时，我们已经可以使用主机名 oolong115 来代替IP 177.11.12.115。同样的方法对另外两台机器设置，这样三台机器之间就可以通过主机名互相访问了。 生成密钥使用命令 ssh-keygen 生成密钥，在生成过程中会要求输入存放目录等内容，可以不输入，回车即可。12345678910111213141516171819202122[root@oolong113 ~]# ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Created directory &apos;/root/.ssh&apos;.Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:jpatt7hZb3V2sLjWm6Wvz3cLo2uFaOPGVpFCMBBsrvE root@oolong113The key&apos;s randomart image is:+---[RSA 2048]----+| .ooo. || o .. || o . . || . . . o . || + S o o. o || . E = + oo.+ .|| + B o..B ..|| . =.*..+ +=o|| =o+oo+. =*B|+----[SHA256]-----+ 进入目录 /root/.ssh/ 可以看到生成以下的两个文件：12-rw-------. 1 root root 1679 Sep 12 19:37 id_rsa-rw-r--r--. 1 root root 397 Sep 12 19:37 id_rsa.pub 如法炮制，给另外两台机器也生成密钥。 拷贝秘钥这里需要注意，要使用SSH的命令来拷贝秘钥，因为里面不能有其他字符，如果是通过文件编辑工具打开拷贝，可能会产生多余的换行符等内容，可能产生的问题是A能免密登录B，但B不能免密登录A。比如当前在 oolong116 这台机器上，进入 /root/.ssh/ 目录，执行下面命令，将秘钥拷贝到 113 上：1ssh-copy-id -i id_rsa.pub root@oolong113 拷贝后检查 113 的 /root/.ssh 目录下有没有 authorized_keys 文件。 依次的拷贝到其他机器上，同样的将其他机器上的秘钥要拷贝到176 上 测试123[root@oolong116 .ssh]# ssh oolong115Last login: Thu Sep 13 05:32:43 2018 from 177.11.12.115[root@oolong115 ~]# 此时，就登录上了 115 了。 批量拷贝秘钥到远程主机安装 sshpass1yum install sshpass 编写脚本进入秘钥所在目录 /root/.ssh，创建以下两个文件： remote-hosts 1234weilu131weilu132weilu135weilu151 copyscript.sh 1234for host in $(cat remote-hosts)do sshpass -p &apos;1234&apos; ssh-copy-id -o StrictHostKeyChecking=no root@$&#123;host&#125;done 其中参数 -p &#39;1234&#39; 表示密码，这里几台机器使用的相同密码，根据情况修改。 保存后需要修改拷贝脚本的权限： 1chmod 777 copyscript.sh 然后执行拷贝： 1./copyscript.sh]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>SSH</tag>
        <tag>免密登录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Config virtual network connection with bridge mode]]></title>
    <url>%2F2018%2F10%2F06%2FConfig-virtual-network-connection-with-bridge-mode%2F</url>
    <content type="text"><![CDATA[摘要在宿主机为 CentOS 7 的环境中通过KVM配置CentOS 7 虚拟机，使用 Bridge 模式配置网络连接，使虚拟机与宿主机处于同一网络环境中。 Create BrdigeInstall ModuleCentOS 7 在系统启动时默认加载了桥接模块。使用下面的命令可以判断这个模块是否加载。1234567891011121314[root@weilu_125 kvms]# modinfo bridgefilename: /lib/modules/3.10.0-862.el7.x86_64/kernel/net/bridge/bridge.ko.xzalias: rtnl-link-bridgeversion: 2.3license: GPLretpoline: Yrhelversion: 7.5srcversion: A0B6183F98024E85CD123C5depends: stp,llcintree: Yvermagic: 3.10.0-862.el7.x86_64 SMP mod_unload modversions signer: CentOS Linux kernel signing keysig_key: 3A:F3:CE:8A:74:69:6E:F1:BD:0F:37:E5:52:62:7B:71:09:E3:2B:96sig_hashalgo: sha256 如果这个模块没有加载，可以使用下面的命令进行加载。1modprobe --first-time bridge 安装桥接工具：1yum install bridge-utils -y create a network bridge要创建一个网桥，可以在 /etc/sysconfig/network-scripts/ 目录下创建一个名为 ifcfg-brN 的文件，将其中的 N 替换为数字，比如“0”。1vi /etc/sysconfig/network-scripts/ifcfg-br0 将下面的内容放置到这个文件中，对应你的主机环境修改相应配置：123456789TYPE=BridgeNAME=br0DEVICE=br0ONBOOT=yesBOOTPROTO=staticIPADDR=192.168.0.125GATEWAY=192.168.0.1NETMASK=255.255.255.0DNS1=192.168.0.1 创建完网桥之后，需要将网络配置接口挂载到这个网桥上。比如，我这边使用的本机已有的适配器 eno1： 1vi /etc/sysconfig/network-scripts/ifcfg-eno1 将以下内容放置到该配置文件中：123456TYPE=EthernetBRIDGE=br0NAME=eno1UUID=99244a4d-8cac-4023-9a09-8e50c547cd3aDEVICE=eno1ONBOOT=yes 使用如下命令重启网络服务：1systemctl restart network 查看网络配置：1234567891011121314151617181920212223242526[root@weilu_125 network-scripts]# ifconfigbr0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.0.125 netmask 255.255.255.0 broadcast 192.168.0.255 inet6 fe80::329c:23ff:fee1:f0d9 prefixlen 64 scopeid 0x20&lt;link&gt; ether 30:9c:23:e1:f0:d9 txqueuelen 1000 (Ethernet) RX packets 3503 bytes 599352 (585.3 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1932 bytes 1004140 (980.6 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0eno1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 ether 30:9c:23:e1:f0:d9 txqueuelen 1000 (Ethernet) RX packets 510891 bytes 330776497 (315.4 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 285983 bytes 46787578 (44.6 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 device interrupt 16 memory 0xa1100000-a1120000 lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 13058 bytes 1819263 (1.7 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 13058 bytes 1819263 (1.7 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Config Guest Connection在虚拟机的设置中，将 Network source 设置为通过刚刚设置的网桥进行连接。 然后修改虚拟机的网络配置：123456789TYPE=EthernetONBOOT=yesDEVICE=eth0BOOTPROTO=staticIPADDR=192.168.0.161NETMASK=255.255.255.0BROADCAST=192.168.0.255GATEWAY=192.168.0.1DNS1=192.168.0.1 使用如下命令重启网络服务：1systemctl restart network 查看网络配置：123456789101112131415161718[root@localhost ~]# ifconfigeth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.0.161 netmask 255.255.255.0 broadcast 192.168.0.255 inet6 fe80::5054:ff:fe24:c503 prefixlen 64 scopeid 0x20&lt;link&gt; ether 52:54:00:24:c5:03 txqueuelen 1000 (Ethernet) RX packets 212 bytes 183776 (179.4 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 186 bytes 18719 (18.2 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 68 bytes 5912 (5.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 68 bytes 5912 (5.7 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 testFrom Host123456789[root@weilu_125 network-scripts]# ping 192.168.0.161 -c 3PING 192.168.0.161 (192.168.0.161) 56(84) bytes of data.64 bytes from 192.168.0.161: icmp_seq=1 ttl=64 time=0.386 ms64 bytes from 192.168.0.161: icmp_seq=2 ttl=64 time=0.327 ms64 bytes from 192.168.0.161: icmp_seq=3 ttl=64 time=0.320 ms--- 192.168.0.161 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2000msrtt min/avg/max/mdev = 0.320/0.344/0.386/0.033 ms From Guest123456789[root@localhost ~]# ping 192.168.0.125 -c 3PING 192.168.0.125 (192.168.0.125) 56(84) bytes of data.64 bytes from 192.168.0.125: icmp_seq=1 ttl=64 time=0.101 ms64 bytes from 192.168.0.125: icmp_seq=2 ttl=64 time=0.260 ms64 bytes from 192.168.0.125: icmp_seq=3 ttl=64 time=0.252 ms--- 192.168.0.125 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 1999msrtt min/avg/max/mdev = 0.101/0.204/0.260/0.074 ms]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>Bridge</tag>
        <tag>KVM</tag>
        <tag>Network Connection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器基于OVS跨主机网络连接]]></title>
    <url>%2F2018%2F10%2F04%2FDocker%E5%AE%B9%E5%99%A8%E5%9F%BA%E4%BA%8EOVS%E8%B7%A8%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[摘要利用OVS技术将位于不同物理主机中的docker容器的网络连通，使之能够相互访问。 前置条件关闭 SELINUX在配置文件中：1vi /etc/selinux/config 将其中的内容 SELINUX=enforcing 修改：1SELINUX=disabled 然后重启 安装 open-vswitch参考【安装配置 Open-vSwitch-2.5.5】 启动 open-vSiwtch：1ovs-ctl start 配置 Docker 网桥IP修改守护进程的配置文件：1vi /etc/docker/daemon.json 在其中添加网桥的IP设置：1&quot;bip&quot;: &quot;172.17.1.1/24&quot; 重启docker：12systemctl stop dockersystemctl start docker 查看网桥IP：1234567891011[root@localhost ~]# ifconfigdocker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 172.17.1.1 netmask 255.255.255.0 broadcast 172.17.1.255 ether 02:42:c7:7b:7d:28 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0eno1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500... 确实变成了我们所设定的IP。 配置 Open-vSwitch创建网桥1ovs-vsctl add-br br0 查看网桥12[root@localhost docker-data]# ovs-vsctl list-brbr0 创建端口1ovs-vsctl add-port br0 gre0 -- set interface gre0 type=gre options:remote_ip=192.168.0.125 查看端口12[root@localhost ~]# ovs-vsctl list-ports br0gre0 查看详细配置信息1234567891011[root@localhost docker-data]# ovs-vsctl show 840c2123-021b-4137-b1d5-8b0963c9e6ac Bridge &quot;br0&quot; Port &quot;gre0&quot; Interface &quot;gre0&quot; type: gre options: &#123;remote_ip=&quot;192.168.0.125&quot;&#125; Port &quot;br0&quot; Interface &quot;br0&quot; type: internal ovs_version: &quot;2.5.5&quot; 连接 br0 和 docker0查看往前目前的连接情况：1234[root@localhost docker-data]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.024205ee36a6 novirbr0 8000.525400249fb4 yes virbr0-nic 可以注意到，此时的 docker0 是没有接口连接的，我们将其与 br0 连接起来：1brctl addif docker0 br0 此时再查看时，会发现其与接口br0连接了：1234[root@localhost ~]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.0242c77b7d28 no br0virbr0 8000.52540071bdcb yes virbr0-nic 挂载 docker0 和 br0查看这两个网络连接的状态：1234567891011[root@localhost ~]# ip link show...2: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 30:9c:23:e1:f0:d9 brd ff:ff:ff:ff:ff:ff...6: br0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop master docker0 state DOWN mode DEFAULT group default qlen 1000 link/ether a2:37:3f:c1:46:4e brd ff:ff:ff:ff:ff:ff...10: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default link/ether 02:42:c7:7b:7d:28 brd ff:ff:ff:ff:ff:ff... 其中还有很多个其他连接的情况，这里省略不看，主要关注 br0 和 docker0，留着 eno1 主要是为了对比。eno1 是本机物理网卡的连接，可以看到其状态 state UP，我们接下来要将 br0 和 docker0 也修改为 UP。1ip link set dev br0 up 在将 br0 改为 UP 时，docker0 状态改为 up，而br0 为 UNKNOW：123456: br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UNKNOWN mode DEFAULT group default qlen 1000 link/ether a2:37:3f:c1:46:4e brd ff:ff:ff:ff:ff:ff 10: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:c7:7b:7d:28 brd ff:ff:ff:ff:ff:ff 添加路由查看当前路由规则：12345[root@localhost ~]# ip route listdefault via 192.168.0.1 dev eno1 proto static metric 100 172.17.1.0/24 dev docker0 proto kernel scope link src 172.17.1.2 192.168.0.0/24 dev eno1 proto kernel scope link src 192.168.0.125 metric 100 192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1 添加路由：12ip route add 172.17.0.0/16 dev docker0route add -net 172.17.0.0/16 gw 192.168.0.125 检查确认：12345678[root@localhost ~]# ip route listdefault via 192.168.0.1 dev enp0s31f6 169.254.0.0/16 dev enp0s31f6 scope link metric 1002 172.17.0.0/16 via 192.168.0.125 dev enp0s31f6 172.17.0.0/16 dev docker0 scope link 172.17.1.0/24 dev docker0 proto kernel scope link src 172.17.1.1 192.168.0.0/24 dev enp0s31f6 proto kernel scope link src 192.168.0.123 192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1 网络结构]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>容器</tag>
        <tag>网络连接</tag>
        <tag>跨主机</tag>
        <tag>open-vswitch</tag>
        <tag>桥接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加载nbd模块失败 modprobe nbd Module nbd not found]]></title>
    <url>%2F2018%2F09%2F30%2F%E5%8A%A0%E8%BD%BDnbd%E6%A8%A1%E5%9D%97%E5%A4%B1%E8%B4%A5-modprobe-nbd-Module-nbd-not-found%2F</url>
    <content type="text"><![CDATA[摘要解决加载 nbd 模块时报错的问题，modprobe nbd Module nbd not found。 前置安装 elfutils-libelf-devel这个模块后面在编译内核时会使用。先给装上。1yum install elfutils-libelf-devel 下载查看操作系统版本和内核版本12345[root@weilu_125 vctos7-1]# cat /etc/redhat-releaseCentOS Linux release 7.5.1804 (Core) [root@weilu_125 vctos7-1]# uname -r3.10.0-862.el7.x86_64 查看 kernel-devel 和 kernel-headers 包，实际上已经安装了：1234567[root@weilu_125 vctos7-1]# yum list installed | grep kernelabrt-addon-kerneloops.x86_64 2.1.11-50.el7.centos @anacondakernel.x86_64 3.10.0-862.el7 @anacondakernel-devel.x86_64 3.10.0-862.el7 @anacondakernel-headers.x86_64 3.10.0-862.el7 @anacondakernel-tools.x86_64 3.10.0-862.el7 @anacondakernel-tools-libs.x86_64 3.10.0-862.el7 @anaconda 根据操作系统和内核版本找到对应的源码，下载：1wget http://vault.centos.org/7.5.1804/os/Source/SPackages/kernel-3.10.0-862.el7.src.rpm 使用命令安装这个包：1rpm -ihv kernel-3.10.0-862.el7.src.rpm 这个包安装后，默认会在 /root/rpmbuild 目录下，同时还会在 /usr/src/kernel 下面生成一个目录，后面会用到。 解压其中的一个包：1tar Jxvf /root/rpmbuild/SOURCES/linux-3.10.0-862.el7.tar.xz -C /usr/src/kernels/ 完成上述步骤之后，可以看到如下两个文件夹：12[root@weilu_125 kernels]# ls3.10.0-862.el7.x86_64 linux-3.10.0-862.el7 备份内核这里首先将内核移动到后缀增加了“-old”的目录下，然后将我们刚刚解压出来的内核目录拷贝过去，并进入内核目录1234[root@weilu_125 kernels]# mv $(uname -r) $(uname -r)-old[root@weilu_125 kernels]# mv linux-3.10.0-862.el7 $(uname -r)[root@weilu_125 kernels]# cd $(uname -r)[root@weilu_125 3.10.0-862.el7.x86_64]# 然后在这个目录下依次执行以下命令：123456make mrpropercp ../$(uname -r)-old/Module.symvers ./cp /boot/config-$(uname -r) ./.configmake oldconfigmake preparemake scripts 执行到这里暂停一下，修改文件:1/usr/src/kernels/3.10.0-862.el7.x86_64/drivers/block/nbd.c 修改如下配置：12// sreq.cmd_type = REQ_TYPE_SPECIAL;sreq.cmd_type = 7; 将这个变量之间设置为7即可，然后继续执行以下命令123make CONFIG_BLK_DEV_NBD=m M=drivers/blockcp drivers/block/nbd.ko /lib/modules/$(uname -r)/kernel/drivers/block/depmod -a 测试12345678910111213[root@weilu_125 block]# modinfo nbdfilename: /lib/modules/3.10.0-862.el7.x86_64/kernel/drivers/block/nbd.kolicense: GPLdescription: Network Block Deviceretpoline: Yrhelversion: 7.5srcversion: EDE909A294AC5FE08E81957depends: vermagic: 3.10.0 SMP mod_unload modversions parm: nbds_max:number of network block devices to initialize (default: 16) (int)parm: max_part:number of partitions per device (default: 0) (int)parm: debugflags:flags for controlling debug output (int)[root@weilu_125 block]#]]></content>
      <categories>
        <category>kernel</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nbd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Github结合Hexo搭建个人博客]]></title>
    <url>%2F2018%2F09%2F29%2F%E5%9F%BA%E4%BA%8EGithub%E7%BB%93%E5%90%88Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[摘要本文介绍如何使用Hexo构建自己的个人博客，同时利用 GitHub 的仓库将内容发布到公网上。 部署与配置GitHub配置申请GitHub账号，并且创建名为 weilu2.github.io 的仓库。 安装 NodeJs安装 Git从官网下载安装 git. 配置免密提交打开 Git Bash，进入用于存放博客的根目录，比如 d:\blog：1234weilu@weilu-PC MINGW64 ~$ cd /d/Blog/weilu@weilu-PC MINGW64 /d/Blog 配置 Git 用户名和邮箱12345weilu@weilu-PC MINGW64 /d/Bloggit config --global user.name &quot;weilu2&quot;weilu@weilu-PC MINGW64 /d/Bloggit config --global user.email &quot;weilu0324@163.com&quot; 生成密钥1$ ssh-keygen -t rsa -C &quot;weilu0324@163.com&quot; 生成的密钥存储路径一般在 C:\Users\weilu\.ssh 使用 ssh-agent 管理私钥启动 ssh-agent123weilu@weilu-PC MINGW64 ~/.ssh$ eval &quot;$(ssh-agent -s)&quot;Agent pid 11508 将生成的密钥添加到 ssh-agent123weilu@weilu-PC MINGW64 ~/.ssh$ ssh-add id_rsaIdentity added: id_rsa (id_rsa) 将公钥添加到GitHub中在 GitHub 个人的设置中，添加 SSH-KEY。 验证123$ ssh -T git@github.com...Hi weilu2! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 配置 Hexo安装使用命令12345678C:\Users\weilu&gt;npm install -g hexo-cliC:\Users\weilu\AppData\Roaming\npm\hexo -&gt; C:\Users\weilu\AppData\Roaming\npm\node_modules\hexo-cli\bin\hexonpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\hexo-cli\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)+ hexo-cli@1.1.0added 225 packages from 414 contributors in 31.47s 创建博客在D盘创建一个目录 D:\Blog，进入该目录，使用如下命令初始化博客：12345678910111213141516171819202122232425262728293031323334D:\Blog&gt;hexo initINFO Cloning hexo-starter to D:\BlogCloning into &apos;D:\Blog&apos;...remote: Enumerating objects: 1, done.remote: Counting objects: 100% (1/1), done.remote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 67Unpacking objects: 100% (68/68), done.Submodule &apos;themes/landscape&apos; (https://github.com/hexojs/hexo-theme-landscape.git) registered for path &apos;themes/landscape&apos;Cloning into &apos;D:/Blog/themes/landscape&apos;...remote: Enumerating objects: 6, done.remote: Counting objects: 100% (6/6), done.remote: Compressing objects: 100% (6/6), done.remote: Total 838 (delta 1), reused 3 (delta 0), pack-reused 832Receiving objects: 100% (838/838), 2.55 MiB | 25.00 KiB/s, done.Resolving deltas: 100% (441/441), done.Submodule path &apos;themes/landscape&apos;: checked out &apos;73a23c51f8487cfcd7c6deec96ccc7543960d350&apos;INFO Install dependenciesnpm WARN deprecated titlecase@1.1.2: no longer maintainednpm WARN deprecated postinstall-build@5.0.3: postinstall-build&apos;s behavior is now built into npm! You should migrate offof postinstall-build and use the new `prepare` lifecycle script with npm 5.0.0 or greater.&gt; nunjucks@3.1.3 postinstall D:\Blog\node_modules\nunjucks&gt; node postinstall-build.js srcnpm notice created a lockfile as package-lock.json. You should commit this file.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)added 420 packages from 484 contributors and audited 4704 packages in 15.704sfound 0 vulnerabilitiesINFO Start blogging with Hexo! 初始化的过程是从 hexo 仓库下载博客的目录结构和文件，根据网速，需要一定时间。 安装依赖模块：1234567D:\Blog&gt;npm installnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)audited 4704 packages in 2.15sfound 0 vulnerabilities 生成静态页面1234567891011121314151617181920212223242526272829303132D:\Blog&gt;hexo gINFO Start processingINFO Files loaded in 112 msINFO Generated: index.htmlINFO Generated: archives/index.htmlINFO Generated: archives/2018/09/index.htmlINFO Generated: fancybox/blank.gifINFO Generated: fancybox/jquery.fancybox.cssINFO Generated: fancybox/fancybox_loading@2x.gifINFO Generated: archives/2018/index.htmlINFO Generated: fancybox/fancybox_sprite.pngINFO Generated: fancybox/fancybox_sprite@2x.pngINFO Generated: fancybox/fancybox_loading.gifINFO Generated: fancybox/fancybox_overlay.pngINFO Generated: fancybox/helpers/fancybox_buttons.pngINFO Generated: js/script.jsINFO Generated: fancybox/jquery.fancybox.pack.jsINFO Generated: css/fonts/FontAwesome.otfINFO Generated: css/fonts/fontawesome-webfont.eotINFO Generated: fancybox/helpers/jquery.fancybox-buttons.jsINFO Generated: css/fonts/fontawesome-webfont.woffINFO Generated: css/style.cssINFO Generated: fancybox/helpers/jquery.fancybox-thumbs.cssINFO Generated: fancybox/helpers/jquery.fancybox-thumbs.jsINFO Generated: fancybox/helpers/jquery.fancybox-buttons.cssINFO Generated: fancybox/helpers/jquery.fancybox-media.jsINFO Generated: css/fonts/fontawesome-webfont.ttfINFO Generated: 2018/09/28/hello-world/index.htmlINFO Generated: css/fonts/fontawesome-webfont.svgINFO Generated: css/images/banner.jpgINFO Generated: fancybox/jquery.fancybox.jsINFO 28 files generated in 288 ms 启动服务器123D:\Blog&gt;hexo sINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 能够正常启动服务器，并在浏览器中访问，说明 Hexo 配置成功，接下来要做的事情就是讲生成的静态页面提交到 Github上即可。 提交 Hexo 到 GitHub修改 _config.yml 文件，在最后增加如下内容：1234deploy: type: git repository: git@github.com:weilu2/weilu2.github.io.git branch: master 安装 Hexo 插件：12345678910D:\Blog&gt;npm install hexo-deployer-git --savenpm WARN deprecated swig@1.4.2: This package is no longer maintainednpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)+ hexo-deployer-git@0.3.1added 31 packages from 36 contributors and audited 5874 packages in 7.482sfound 1 low severity vulnerability run `npm audit fix` to fix them, or `npm audit` for details 再次运行生成命令，就会自动生成静态文件，并部署到 git上了：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778D:\Blog&gt; hexo d -gINFO Start processingINFO Files loaded in 62 msINFO 0 files generated in 78 msINFO Deploying: gitINFO Setting up Git deployment...Initialized empty Git repository in D:/Blog/.deploy_git/.git/[master (root-commit) 67b0210] First commit 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 placeholderINFO Clearing .deploy_git folder...INFO Copying files from public folder...INFO Copying files from extend dirs...warning: LF will be replaced by CRLF in 2018/09/28/hello-world/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in archives/2018/09/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in archives/2018/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in archives/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in css/style.css.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/helpers/jquery.fancybox-buttons.css.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/helpers/jquery.fancybox-buttons.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/helpers/jquery.fancybox-media.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/helpers/jquery.fancybox-thumbs.css.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/helpers/jquery.fancybox-thumbs.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/jquery.fancybox.css.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/jquery.fancybox.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/jquery.fancybox.pack.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/script.js.The file will have its original line endings in your working directory[master 9321f7e] Site updated: 2018-09-29 00:08:21 29 files changed, 5777 insertions(+) create mode 100644 2018/09/28/hello-world/index.html create mode 100644 archives/2018/09/index.html create mode 100644 archives/2018/index.html create mode 100644 archives/index.html create mode 100644 css/fonts/FontAwesome.otf create mode 100644 css/fonts/fontawesome-webfont.eot create mode 100644 css/fonts/fontawesome-webfont.svg create mode 100644 css/fonts/fontawesome-webfont.ttf create mode 100644 css/fonts/fontawesome-webfont.woff create mode 100644 css/images/banner.jpg create mode 100644 css/style.css create mode 100644 fancybox/blank.gif create mode 100644 fancybox/fancybox_loading.gif create mode 100644 fancybox/fancybox_loading@2x.gif create mode 100644 fancybox/fancybox_overlay.png create mode 100644 fancybox/fancybox_sprite.png create mode 100644 fancybox/fancybox_sprite@2x.png create mode 100644 fancybox/helpers/fancybox_buttons.png create mode 100644 fancybox/helpers/jquery.fancybox-buttons.css create mode 100644 fancybox/helpers/jquery.fancybox-buttons.js create mode 100644 fancybox/helpers/jquery.fancybox-media.js create mode 100644 fancybox/helpers/jquery.fancybox-thumbs.css create mode 100644 fancybox/helpers/jquery.fancybox-thumbs.js create mode 100644 fancybox/jquery.fancybox.css create mode 100644 fancybox/jquery.fancybox.js create mode 100644 fancybox/jquery.fancybox.pack.js create mode 100644 index.html create mode 100644 js/script.js delete mode 100644 placeholderBranch &apos;master&apos; set up to track remote branch &apos;master&apos; from &apos;git@github.com:weilu2/weilu2.github.io.git&apos;.To github.com:weilu2/weilu2.github.io.git + c049402...9321f7e HEAD -&gt; master (forced update)INFO Deploy done: git 直接访问 写作创建分类页面12D:\Blog&gt;hexo new page categoriesINFO Created: D:\Blog\source\categories\index.md 编辑这个页面，增加 type ：12345---title: categoriesdate: 2018-09-29 09:10:18type: "categories"--- 修改主题下的 _config.yml 文件，在 menu 中增加分类导航：12345menu: Home: / categories: /categories Archives: /archivesrss: /atom.xml 创建标签页面12D:\Blog&gt;hexo new page tagsINFO Created: D:\Blog\source\tags\index.md 编辑页面，增加类型：12345---title: tagsdate: 2018-09-29 10:12:20type: &quot;tags&quot;--- 在主题的配置文件中，增加标签的链接：123456menu: Home: / categories: /categories tags: /tags Archives: /archivesrss: /atom.xml 创建内容页面12D:\Blog&gt;hexo new post &quot;基于Github结合Hexo搭建个人博客&quot;INFO Created: D:\Blog\source\_posts\2018-09-29-基于Github结合Hexo搭建个人博客.md]]></content>
      <categories>
        <category>乱七八糟</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>Hexo</tag>
        <tag>Blog</tag>
        <tag>Nodejs</tag>
      </tags>
  </entry>
</search>
