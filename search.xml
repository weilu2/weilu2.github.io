<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[npm install Error EACCES permission denied]]></title>
    <url>%2F2018%2F11%2F21%2Fnpm-install-Error-EACCES-permission-denied%2F</url>
    <content type="text"><![CDATA[摘要使用 npm install 安装项目时，遇到的一个错误 Error: EACCES: permission denied。记录一下解决方法。 npm install Error: EACCES: permission denied在使用 npm 安装模块时碰到的问题，异常信息如下。 这里简单记录一下解决方法，给命令增加如下的一些参数，大概就是一些允许权限之类的，没有具体研究：1npm install --unsafe-perm=true --allow-root 错误信息1234567891011121314151617181920212223242526272829[root@weilu125 adminMongo-master]# npm installnpm WARN deprecated to-iso-string@0.0.2: to-iso-string has been deprecated, use @segment/to-iso-string instead.npm WARN deprecated jade@0.26.3: Jade has been renamed to pug, please install the latest version of pug instead of jadenpm WARN deprecated electron-prebuilt@1.4.13: electron-prebuilt has been renamed to electron. For more details, see http://electron.atom.io/blog/2016/08/16/npm-install-electronnpm WARN deprecated sprintf@0.1.5: The sprintf package is deprecated in favor of sprintf-js.npm WARN deprecated mongodb@2.2.16: Please upgrade to 2.2.19 or highernpm WARN deprecated minimatch@0.3.0: Please update to minimatch 3.0.2 or higher to avoid a RegExp DoS issuenpm WARN deprecated circular-json@0.3.3: CircularJSON is in maintenance only, flatted is its successor.&gt; electron-prebuilt@1.4.13 postinstall /usr/local/adminMongo-master/node_modules/electron-prebuilt&gt; node install.js/usr/local/adminMongo-master/node_modules/electron-prebuilt/install.js:22 throw err ^Error: EACCES: permission denied, mkdir &apos;/usr/local/adminMongo-master/node_modules/electron-prebuilt/.electron&apos;npm WARN mocha-jsdom@1.2.0 requires a peer of jsdom@&gt;10.0.0 but none is installed. You must install peer dependencies yourself.npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! electron-prebuilt@1.4.13 postinstall: `node install.js`npm ERR! Exit status 1npm ERR! npm ERR! Failed at the electron-prebuilt@1.4.13 postinstall script.npm ERR! This is probably not a problem with npm. There is likely additional logging output above.npm ERR! A complete log of this run can be found in:npm ERR! /root/.npm/_logs/2018-11-21T01_22_13_662Z-debug.log]]></content>
      <categories>
        <category>Nodejs</category>
      </categories>
      <tags>
        <tag>npm</tag>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Scala 编写的 Spark 程序操作 HBase]]></title>
    <url>%2F2018%2F11%2F20%2F%E5%9F%BA%E4%BA%8E-Scala-%E7%BC%96%E5%86%99%E7%9A%84-Spark-%E7%A8%8B%E5%BA%8F%E6%93%8D%E4%BD%9C-HBase%2F</url>
    <content type="text"><![CDATA[摘要使用 Scala 编写 Spark 程序操作 HBase 中的数据。 环境准备集群环境Hadoop环境参考：Hadoop 集群部署方案 Zookeeper&amp;HBase环境参考：基于 Hadoop 集群部署 ZooKeeper 和 HBase 集群 Spark环境参考：基于 Hadoop 和 Yarn 集群部署 Spark 集群 运行环境整个过程是在本地 Idea 中使用 Scala 编写 Spark 程序，使用 SBT 打包后，通过 spark-submit 提交到 Yarn 中运行。 之前在部署 Spark 环境时，有设置过一个变量：1spark.yarn.jars hdfs://weilu131:9000/spark_jars/* 并且已经将 Spark 的相关 JAR 包上传到 HDFS 上的这个目录中。这是 Spark 程序的运行环境。由于需要连接 HBase，因此程序的运行环境还需要有 HBase 相关的 JAR 包。 将 HBase 根目录下的以下 JAR 包上传到 HDFS 的这个目录中：123456789/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/hbase*.jar /spark_jars/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/guava-12.0.1.jar /spark_jars/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/htrace-core-3.2.0-incubating.jar /spark_jars/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/protobuf-java-2.5.0.jar /spark_jars/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/metrics-core-2.2.0.jar /spark_jars 这里需要注意，由于使用的 HBase 和 Spark 各自有对一些 JAR 包的不同版本的依赖，因此需要都上传上去，如果没有可能会报找不到类的错误。在末尾会有相关示例。 数据准备进入 HBase shell 环境：1/usr/local/hbase-1.2.0-cdh5.15.0/bin/hbase shell 建表创建一张名为 student 的表，包含一个 info 列族。1create &apos;student&apos;,&apos;info&apos; 添加数据HBase 中添加数据是按照单元格添加，通过行键、列族、列名确定一个单元格。1234567put &apos;student&apos;,&apos;e018565c-ebaa-11e8-b4ec-3c970e0087f3&apos;,&apos;info:name&apos;,&apos;wcwang&apos;put &apos;student&apos;,&apos;e018565c-ebaa-11e8-b4ec-3c970e0087f3&apos;,&apos;info:gender&apos;,&apos;F&apos;put &apos;student&apos;,&apos;e018565c-ebaa-11e8-b4ec-3c970e0087f3&apos;,&apos;info:age&apos;,&apos;22&apos;put &apos;student&apos;,&apos;e0182f4a-ebaa-11e8-a353-3c970e0087f3&apos;,&apos;info:name&apos;,&apos;lx&apos;put &apos;student&apos;,&apos;e0182f4a-ebaa-11e8-a353-3c970e0087f3&apos;,&apos;info:gender&apos;,&apos;M&apos;put &apos;student&apos;,&apos;e0182f4a-ebaa-11e8-a353-3c970e0087f3&apos;,&apos;info:age&apos;,&apos;21&apos; 编写运行程序创建一个 Scala 项目，参考配置 Intellij Idea 和 Sbt 开发、打包、运行 Spark 程序。 代码然后在 src/main/scala/ 目录下创建一个 Scala 脚本，填充以下代码： 12345678910111213141516171819202122232425262728293031323334import org.apache.spark._ import org.apache.hadoop.hbase.HBaseConfiguration import org.apache.hadoop.hbase.mapreduce.TableInputFormat import org.apache.hadoop.hbase.util.Bytes object DataImport &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setMaster("yarn").set("spark.app.name", "MedicalQA") sparkConf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer") val sc = new SparkContext(sparkConf) val hbaseConf = HBaseConfiguration.create() hbaseConf.set("hbase.zookeeper.property.clientPort", "2181") hbaseConf.set("hbase.zookeeper.quorum", "192.168.0.131,192.168.0.132,192.168.0.133,192.168.0.151,192.168.0.152") hbaseConf.set("hbase.master", "192.168.0.131") hbaseConf.set(TableInputFormat.INPUT_TABLE, "student") val studRDD = sc.newAPIHadoopRDD(hbaseConf, classOf[TableInputFormat], classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable], classOf[org.apache.hadoop.hbase.client.Result]) val count = studRDD.count() println(count) studRDD.cache() studRDD.collect().foreach(&#123; row =&gt; &#123; val result = row._2 val key = Bytes.toString(result.getRow) val name = Bytes.toString(result.getValue("info".getBytes(), "name".getBytes())) val gender = Bytes.toString(result.getValue("info".getBytes(), "gender".getBytes())) val age = Bytes.toString(result.getValue("info".getBytes(), "age".getBytes())) println(key + " " + name + " " + gender + " " + age) &#125; &#125;) &#125; &#125; 在这段代码中首先统计了 student 表中数据的条数，然后将两条数据分别打印出来。 项目配置将 Hbase 下面的 hbase-site.xml 文件拷贝到项目中的 src/main/scala 目录中。 打包运行打包好的 JAR 包上传到服务器中，然后提交到 Spark 中运行：1/usr/local/spark-2.4.0-bin-hadoop2.6/bin/spark-submit --deploy-mode cluster /home/workspace/MedicalQAImport.jar 如果命令行和 Yarn 中没有抛异常，那就OK。然后检查 Yarn 中的日志。1232e0182f4a-ebaa-11e8-a353-3c970e0087f3 lx M 21e018565c-ebaa-11e8-b4ec-3c970e0087f3 wcwang F 22 注意，这里输出的日志不是在控制台输出，而是在所运行的那台服务器的日志中，可以通过 Yarn 的界面查看。 遇到问题java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge这里就是之前所说的 HBase 和 Spark 依赖于同一个类库的不同版本的问题。在上传 Spark 的 JAR 包时，已经将 metrics-core-3.1.5.jar 上传了。但实际上 HBase 依赖的是 metrics-core-2.2.0.jar 的版本，因此还需要将这个包也上传。 部分异常1234567891011org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:Tue Nov 20 08:55:43 CST 2018, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0Caused by: java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0 Caused by: java.io.IOException: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/GaugeCaused by: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/GaugeCaused by: java.lang.NoClassDefFoundError: com/yammer/metrics/core/GaugeCaused by: java.lang.ClassNotFoundException: com.yammer.metrics.core.Gauge 完整异常信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413518/11/20 08:55:44 ERROR ApplicationMaster: User class threw exception: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:Tue Nov 20 08:55:43 CST 2018, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:Tue Nov 20 08:55:43 CST 2018, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0 at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.throwEnrichedException(RpcRetryingCallerWithReadReplicas.java:320) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:247) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:62) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210) at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:327) at org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:302) at org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:167) at org.apache.hadoop.hbase.client.ClientScanner.&lt;init&gt;(ClientScanner.java:162) at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:867) at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:193) at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:89) at org.apache.hadoop.hbase.client.MetaScanner.allTableRegions(MetaScanner.java:324) at org.apache.hadoop.hbase.client.HRegionLocator.getAllRegionLocations(HRegionLocator.java:88) at org.apache.hadoop.hbase.util.RegionSizeCalculator.init(RegionSizeCalculator.java:94) at org.apache.hadoop.hbase.util.RegionSizeCalculator.&lt;init&gt;(RegionSizeCalculator.java:81) at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:256) at org.apache.hadoop.hbase.mapreduce.TableInputFormat.getSplits(TableInputFormat.java:240) at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126) at org.apache.spark.rdd.RDD.count(RDD.scala:1168) at DataImport$.main(DataImport.scala:18) at DataImport.main(DataImport.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)Caused by: java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0 at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:169) at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:80) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.IOException: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:334) at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:408) at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:204) at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:65) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:397) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:371) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:136) ... 4 moreCaused by: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:240) at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:336) at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:34094) at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:400) ... 10 moreCaused by: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:225) ... 13 moreCaused by: java.lang.ClassNotFoundException: com.yammer.metrics.core.Gauge at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 14 more18/11/20 08:55:44 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:Tue Nov 20 08:55:43 CST 2018, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0 at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.throwEnrichedException(RpcRetryingCallerWithReadReplicas.java:320) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:247) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:62) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210) at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:327) at org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:302) at org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:167) at org.apache.hadoop.hbase.client.ClientScanner.&lt;init&gt;(ClientScanner.java:162) at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:867) at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:193) at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:89) at org.apache.hadoop.hbase.client.MetaScanner.allTableRegions(MetaScanner.java:324) at org.apache.hadoop.hbase.client.HRegionLocator.getAllRegionLocations(HRegionLocator.java:88) at org.apache.hadoop.hbase.util.RegionSizeCalculator.init(RegionSizeCalculator.java:94) at org.apache.hadoop.hbase.util.RegionSizeCalculator.&lt;init&gt;(RegionSizeCalculator.java:81) at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:256) at org.apache.hadoop.hbase.mapreduce.TableInputFormat.getSplits(TableInputFormat.java:240) at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126) at org.apache.spark.rdd.RDD.count(RDD.scala:1168) at DataImport$.main(DataImport.scala:18) at DataImport.main(DataImport.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)Caused by: java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0 at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:169) at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:80) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.IOException: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:334) at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:408) at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:204) at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:65) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:397) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:371) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:136) ... 4 moreCaused by: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:240) at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:336) at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:34094) at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:400) ... 10 moreCaused by: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:225) ... 13 moreCaused by: java.lang.ClassNotFoundException: com.yammer.metrics.core.Gauge at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 14 more)18/11/20 08:55:44 INFO SparkContext: Invoking stop() from shutdown hook had a not serializable result解决在 Spark 的环境中配置：1sparkConf.set("spark.serializer","org.apache.spark.serializer.KryoSerializer") 异常信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110218/11/20 09:55:25 ERROR TaskSetManager: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritableSerialization stack: - object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33) - field (class: scala.Tuple2, name: _1, type: class java.lang.Object) - object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;)) - element of array (index: 0) - array (class [Lscala.Tuple2;, size 2); not retrying18/11/20 09:55:25 INFO YarnClusterScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 18/11/20 09:55:25 INFO YarnClusterScheduler: Cancelling stage 118/11/20 09:55:25 INFO YarnClusterScheduler: Killing all running tasks in stage 1: Stage cancelled18/11/20 09:55:25 INFO DAGScheduler: ResultStage 1 (collect at DataImport.scala:23) failed in 0.338 s due to Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritableSerialization stack: - object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33) - field (class: scala.Tuple2, name: _1, type: class java.lang.Object) - object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;)) - element of array (index: 0) - array (class [Lscala.Tuple2;, size 2)18/11/20 09:55:25 INFO DAGScheduler: Job 1 failed: collect at DataImport.scala:23, took 0.346124 s18/11/20 09:55:25 ERROR ApplicationMaster: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritableSerialization stack: - object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33) - field (class: scala.Tuple2, name: _1, type: class java.lang.Object) - object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;)) - element of array (index: 0) - array (class [Lscala.Tuple2;, size 2)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritableSerialization stack: - object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33) - field (class: scala.Tuple2, name: _1, type: class java.lang.Object) - object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;)) - element of array (index: 0) - array (class [Lscala.Tuple2;, size 2) at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926) at scala.Option.foreach(Option.scala:257) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:363) at org.apache.spark.rdd.RDD.collect(RDD.scala:944) at DataImport$.main(DataImport.scala:23) at DataImport.main(DataImport.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)18/11/20 09:55:25 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritableSerialization stack: - object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33) - field (class: scala.Tuple2, name: _1, type: class java.lang.Object) - object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;)) - element of array (index: 0) - array (class [Lscala.Tuple2;, size 2) at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926) at scala.Option.foreach(Option.scala:257) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126) at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:363) at org.apache.spark.rdd.RDD.collect(RDD.scala:944) at DataImport$.main(DataImport.scala:23) at DataImport.main(DataImport.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)) 参考：https://segmentfault.com/q/1010000007041500]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>HBase</tag>
        <tag>Scala</tag>
        <tag>Spark</tag>
        <tag>Scala操作HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spark 关键概念备忘]]></title>
    <url>%2F2018%2F11%2F20%2FSpark-%E5%85%B3%E9%94%AE%E6%A6%82%E5%BF%B5%E5%A4%87%E5%BF%98%2F</url>
    <content type="text"><![CDATA[摘要学习 Spark 过程中记录的一些比较重要的概念。填充了一部分内容，另有一部分留空的，后续理解逐步加深后进一步补全和拓展。 基本概念和架构基本概念1. RDDResillient Distributed Dataset 弹性分布式数据集，是分布式内存的一个抽象概念，提供了一种高度受限的共享内存模型 2. DAGDirected Acyclic Graph 有向无环图，反应 RDD 之间的依赖关系 3. Executor运行在工作节点（WorkerNode）上的一个进程，负责运行 Task 4. Application用户编写的 Spark 应用程序 5. Task运行在 Execotr 上的工作单元 6. Job一个 Job 包含多个 RDD 及作用于相应 RDD 上的各种操作 7. Stage是 Job 的基本调度单位，一个 Job 会分为多组 Task，每组 Task 成为 Stage，或者 TaskSet，代表一组关联的，相互之间没有 Shuffle 依赖关系的任务组成的任务集 运行架构 Spark有点：1、利用多线程执行具体的任务，减少任务启动开销2、Executor 中有一个 BlockManager 存储模块，结合内存和磁盘作为存储设备，减少 IO 开销 Spark 运行流程 STEP 1 为应用构建基本的运行环境，由 Driver 创建一个 SparkContext 进行资源的申请、任务的分配和监控。 STEP 2 资源管理器为 Executor 分配资源，并启动 Executor 进程。 STEP 3 SparkContext 根据 RDD 的依赖关系构建 DAG 图，DAG 图提交给 DAGScheduler 解析成 Stage，然后把一个个 TaskSet 提交给底层调度器 TaskSchedule 处理。 Executor 向 SprakContext 申请 Task。 TaskScheduler 将 Task 分发给 Executor ，并提供应用程序代码 STEP 4 Task 在 Excutor 上运行，把执行结果反馈给 TaskScheduler，然后反馈给 DAGScheduler，运行完成之后写入数据并释放资源。 Spark 运行特点1、每个 Application 都有自己专属的 Executor 进程，并且该进程在 Application 运行期间一直驻留。Executor进程以多线程的方式运行 Task。2、Spark 运行过程与资源管理器无关，只要能够获取 Executor 进程并保持通讯即可3、Task 采用了数据本地性和推测执行优化机制 RDDRDD 执行过程1、RDD 读取外部数据进行创建 2、经过一系列转换（Transformation）操作，每次都会产生新的 RDD，提供给下一次转换操作使用 3、最后一个 RDD 经过“动作”操作进行转换，并输出到外部数据源 一般讲一个 DAG 的一系列处理成为一个 Lineage（血缘关系） RDD 的依赖关系窄依赖1、一个父亲 RDD 的一个分区，转换得到一个儿子 RDD 的一个分区2、多个父亲 RDD 的若干个分区，转换得到一个儿子 RDD 的一个分区 宽依赖1、一个父亲 RDD 的一个分区，转换得到多个儿子 RDD 的若干个分区 Stage 划分DAG 中进行反向解析，遇到宽依赖就断开，遇到债依赖就把当前 RDD 加入到 Stage 中。将窄依赖尽量划分在同一个 Stage 中，实现流水线计算。 RDD 基本操作RDD 创建从本地文件系统加载数据 1val lines = sc.textFile("file:///home/spark/mydata/word.txt") 从分布式文件系统 HDFS 中加载数据1val lines = sc.textFile("hdfs://weilu131:9000/mydata/word.txt") 从集合中创建 RDD使用 sc.parallelize() 方法可以将数组转换为 RDD12val array = Array(1, 2, 3, 4, 5)val rdd = sc.parallelize(array) 对于列表 List 同上。 RDD 操作转换操作（Transformation）filter(func)筛选出能够满足函数 func 的元素，并返回一个新的数据集。12val lines = sc.textFile("hdfs://weilu131:9000/mydata/word.txt")val res = lines.filter(line =&gt; line.contains("Spark")).count() map(func)将每个元素传递到函数func中，并将结果返回为一个新的数据集12val lines = sc.textFile("hdfs://weilu131:9000/mydata/word.txt")val res = lines.map(line =&gt; line.split(" ").size).reduce((a,b) =&gt; if (a &gt; b) a else b) flatMap(func)与 map 类似，但每个输入元素都可以映射到0或多个输出结果 groupByKey()应用于(K,V)键值对数据集时，返回一个新的 (K, Iterable) 形式的数据集 reduceByKey(func)应用于 (K, V) 键值对的数据集时，返回一个新的 (K, V) 形式的数据集，其中的每个值是将每个 key 传递到函数 func 中进行聚合。 行动操作（Action）count()返回数据集中的元素个数 collect()以数组的形式返回数据集中的所有元素 first()返回数据集中第一个元素 take(n)以数组的形式返回数据集中的前 n 个元素 reduce(func)通过函数 func（输入两个参数并返回一个值）聚合函数集中的元素 foreach(func)将数据集中的每个元素传递到函数 func 中运行 惰性机制对于 RDD 而言，每一次转换都会形成新的 RDD，但是在转换操作过程中，只会记录轨迹，只有程序运行到行动操作时，才会真正的进行计算，这个被称为惰性求值。 持久化12345678val list = List("Hadoop", "Spark", "Hive")val rdd = sc.parallelize(list)// 行动操作，触发一次计算rdd.count()// 行动操作，再次触发一次计算rdd.collect().mkString(",") 在两次行动操作中每次触发的转换操作都是相同的，为了避免重复计算，可以对第一次转换的过程进行持久化。 persist(MEMORY_ONLY)将 RDD 作为反序列化对象存储于 JVM 中，如果内存不足，按照 LRU 原则替换缓存中内容。 persist(MEMORY_AND_DISK)将 RDD 作为反序列化的对象存储在 JVM 中，如果内存不足，超出部分会被存储在硬盘上 cache()persist(MEMORY_ONLY) 的快捷方式 unpersist()手动把持久化的 RDD 从缓存中删除 1234567891011val list = List("Hadoop", "Spark", "Hive")val rdd = sc.parallelize(list)// 标记为持久化rdd.cache()// 行动操作，触发一次计算，并缓存转换操作结果rdd.count()// 行动操作，直接使用缓存的转换操作结果rdd.collect().mkString(",") 分区RDD 分区的一个分区原则是使得分区的个数尽量等于整个集群中的CPU核心数目对于不同的 spark 部署模式而言，都可以使用 spark.default.parallelism 这个参数设置 在调用 textFile 和 parallelize 方法时候手动指定分区个数即可。 对于 parallelize 而言，如果没有在方法中指定分区数，则默认为 spake.deafault.parallelism。 对于textFile 而言，如果没有在方法中指定分区数，则默认为 min(defaultParallelism, 2)，其中，defaultParallelism 对应的就是 spark.default.parallelism 如果时从 HDFS 中读取文件，则分区数为文件分片数（比如，128MB/片） textFile 1sc.textFile(path, partitionNum) parallelize 1sc.parallelize(array, 2) // 设置两个分区 通过转换操作得到新的 RDD 时，直接调用 reparation 方法 自定义分区12345678910111213141516171819202122import org.apache.spark.&#123;Partitioner, SparkContext, SparkConf&#125; class UserPartitioner(numParts: Int) extends Partitioner &#123; override def numPartitions: Int = numParts override def getPartition(key: Any): Int = &#123; key.toString.toInt % 10 &#125; &#125; object ManualPartition &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() val sc = new SparkContext(conf) val data = sc.parallelize(1 to 5, 5) val data2 = data.map((_, 1)) val data3 = data2.partitionBy(new UserPartitioner(10)) val data4 = data3.map(_._1) data4.saveAsTextFile("hdfs://weilu131:9000/test/output") &#125; &#125; Pair RDD（键值对 RDD）创建 PairRDD12345678val list = List("Hadoop", "Hive", "HBase", "Spark", "Sqoop", "Spark")val rdd = sc.parallelize(list) // 创建 RDDval pairRDD = rdd.map(word =&gt; (word, 1))// 如果是在集群上运行 Spark 程序，那么这段代码不会打印任何内容pairRDD.foreach(println)// 需要先收集之后再打印pairRDD.collect().foreach(println) 打印内容：123456(Hadoop,1)(Hive,1)(HBase,1)(Spark,1)(Sqoop,1)(Spark,1) reduceByKey(func)key 相同，将值按照传入逻辑计算12345678910111213val list = List("Hadoop 2", "Spark 3", "HBase 5", "Spark 6", "Hadoop 1")val rdd = sc.parallelize(list)val split = (line : String) =&gt; &#123; val res = line.split(" ") (res(0), res(1).toInt)&#125;val pairRdd = rdd.map(split)pairRdd.collect().foreach(println) // 打印测试:1val res = pairRdd.reduceByKey((a,b) =&gt; a+b)res.collect().foreach(println) // 打印测试：2 打印结果：1234567891011// 第一次(Hadoop,2)(Spark,3)(HBase,5)(Spark,6)(Hadoop,1)// 第二次(Spark,9)(HBase,5)(Hadoop,3) groupByKey()key 相同，将值生成一个列表12345678910111213val list = List("Hadoop 2", "Spark 3", "HBase 5", "Spark 6", "Hadoop 1")val rdd = sc.parallelize(list)val split = (line : String) =&gt; &#123; val res = line.split(" ") (res(0), res(1).toInt)&#125;val pairRdd = rdd.map(split)pairRdd.collect().foreach(println) // 打印测试:1val res = pairRdd.groupByKey()res.collect().foreach(println) // 打印测试：2 打印结果：1234567891011// 第一次(Hadoop,2)(Spark,3)(HBase,5)(Spark,6)(Hadoop,1)// 第二次(Spark,CompactBuffer(6, 3))(HBase,CompactBuffer(5))(Hadoop,CompactBuffer(1, 2)) keys、values仅仅把 PairRDD 中的键或者值单独取出来形成一个 RDD sortByKey()123456789101112131415val list = List("Hadoop 2", "Spark 3", "HBase 5", "Spark 6", "Hadoop 1")val rdd = sc.parallelize(list)val split = (line : String) =&gt; &#123; val res = line.split(" ") (res(0), res(1).toInt)&#125;val pairRdd = rdd.map(split)val res = pairRdd.sortByKey(true)res.collect().foreach(println) // 打印测试：1val res = pairRdd.sortByKey(false)res.collect().foreach(println) // 打印测试：2 打印结果：12345678910111213# 1(HBase,5)(Hadoop,2)(Hadoop,1)(Spark,6)(Spark,3)# 2(Spark,3)(Spark,6)(Hadoop,1)(Hadoop,2)(HBase,5) mapValues(func)对 PairRDD 中的每个值进行处理，不影响 key. join将两个 PairRDD 根据 key 进行连接操作 combineByKey共享变量主要用于节省传输开销。当Spark在集群的多个节点上的多个任务上并行运行一个函数时，它会吧函数中涉及到的每个变量在每个任务中生成一个副本。但是，有时需要在多个任务之间共享变量，或者在任务和任务控制节点之间共享变量。 为满足这种需求，Spark提供了两种类型的变量：广播变量（broadcast variables）和累加器（accumulators）。广播变量用来把变量在所有节点的内存之间进行共享；累加器则支持在所有不同节点之间进行累加计算（比如计数、求和等） 广播变量允许程序开发人员在每个机器上缓存一个只读变量，而不是在每个机器上的每个任务都生成一个副本。Spark的“行动”操作会跨越多个阶段（Stage），对每个阶段内的所有任务所需要的公共数据，Spark会自动进行广播。 可以使用 broadcast() 方法封装广播变量123val broadcastVar = sc.broadcast(Array(1, 2, 3))println(broadcastVar.value) 累加器Spark 原生支持数值型累加器，可以通过自定义开发对新类型支持的累加器。 longAccumulator &amp; doubleAccumulatorSpark 自带长整型和双精度数值累加器，可以通过以上两个方法创建。创建完成之后可以使用 add 方法进行累加操作，但在每个节点上只能进行累加操作，不能读取。只有任务控制节点可以使用 value 方法读取累加器的值。 123val accum = sc.longAccumulator("OneAccumulator")sc.parallelize(Array(1, 2, 3)).foreach(x =&gt; accum.add(x))accum.value 数据读写文件系统数据读写读写本地文件1234val aFile = sc.textFile("file:///home/spark/somewords.txt")// 保存时会生成一个目录，内容被跌倒这个目录中aFile.saveAsTextFile("file:///home/spark/something.txt") 读写HDFS文件1234val aFile = sc.textFile("hdfs://weilu131:9000/home/spark/somewords.txt")// 保存时会生成一个目录，内容被跌倒这个目录中aFile.saveAsTextFile("hdfs://weilu131:9000/home/spark/something.txt")]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>RDD</tag>
        <tag>DAG</tag>
        <tag>Exector</tag>
        <tag>Job</tag>
        <tag>Stage</tag>
        <tag>PairRDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置 Intellij Idea 和 Sbt 开发、打包、运行 Spark 程序]]></title>
    <url>%2F2018%2F11%2F16%2F%E9%85%8D%E7%BD%AE-Intellij-Idea-%E5%92%8C-Sbt-%E5%BC%80%E5%8F%91%E3%80%81%E6%89%93%E5%8C%85%E3%80%81%E8%BF%90%E8%A1%8C-Spark-%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[摘要使用 Idea 集成 Sbt 配置 Scala 开发环境，开发 Spark 程序。 配置基本环境首先需要几个基本环境，一个是 JDK，还有 Scala，另外安装 Idea，以及在 Idea 中安装 Scala 插件，这里就不赘述了。 创建项目首先创建一个 Scala - sbt 项目： 填写项目名称，项目路径。另外需要注意的是选择合适的 sbt 和 scala 版本。 创建完成之后，会从远处服务器拉取项目结构信息，可能会耗费一些时间。 完成之后会生成如图所示的一个项目结构： 导入依赖编辑 build.sbt 文件，填写项目依赖，这里是 Spark 程序，需要添加 Spark-core依赖。另外这里还声明了 Scala 依赖。 1234567name := "MedicalQA"version := "0.1"scalaVersion := "2.11.12"libraryDependencies += "org.apache.spark" %% "spark-core" % "2.4.0" 在编辑了 build.sbt 文件后，会提示是否导入这些依赖，选择自动导入即可。 如果是第一次添加某个依赖，会从远程服务器下载，根据网络情况，耗费一定时间，需耐心等待。 导入之后，可以在 External Libraries 中看到这些依赖。 添加 SBT 依赖如果 External Libraries 中没有 sbt 相关的 JAR 包，可以手动添加。 打开 Project Structure，在 File 或者界面右上角的文件图标打开。 点击上方的小加号，选择 Scala SDK，图片中的实例是添加后的效果，一开始应该是空的。 在弹出的如下界面中选择 Browse，浏览本地目录，选中本地 Scala 的根目录即可。 编写测试程序创建一个 Scala 文件。 这里面写了一个小李子，是在hdfs上写入一个文本文件。完成之后就需要将项目编译成一个JAR包，上传到spark中执行。 1234567891011121314151617181920212223import java.io.BufferedOutputStreamimport org.apache.hadoop.fs.&#123;FileSystem, Path&#125;import org.apache.spark._object DataImport &#123; def main(args: Array[String]): Unit = &#123; val conf = new SparkConf() conf.setMaster("yarn") conf.set("spark.app.name", "MedicalQA") val sc = new SparkContext(conf) val sentence = "some words" val fs = FileSystem.get(sc.hadoopConfiguration) val output = fs.create(new Path("/test/somewords.txt")) val ops = new BufferedOutputStream(output) ops.write(sentence.getBytes("UTF-8")) ops.close() output.close() fs.close() &#125;&#125; 编译打包再次打开 Project Structure，选择 Artifacts，点击小加号 选择 Module，也就是项目，以及程序入口 Main Class，也就是主函数所在的类。 然后会看到如下的界面： 接下来我们要把jar包中引用的jar包去除掉，不打包在里面。选中这些JAR包，用上面的减号删除掉。 最后只剩下项目的代码 完成之后会在项目目录下看到 META-INF 目录。 然后进行编译。 编译打包完成之后，会在项目目录中看到打包的结果 运行测试接下来我们把这个JAR包提交到spark集群中跑跑看。 把 JAR 包放到 /home/workspace 目录下，执行命令： 1/usr/local/spark-2.4.0-bin-hadoop2.6/bin/spark-submit --class &quot;DataImport&quot; --deploy-mode cluster /home/workspace/MedicalQA.jar 跑完之后如果没有报什么错的话，用HDFS命令检查一下 123[root@weilu131 bin]# ./hdfs dfs -ls /testFound 1 items-rw-r--r-- 3 root supergroup 10 2018-11-16 15:52 /test/somewords.txt 发现文件确实写进去了，当然如果不放心还可以看看文件内容 12[root@weilu131 bin]# ./hdfs dfs -cat /test/somewords.txtsome words 还可以看看 Yarn 上面提交的任务执行情况]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Idea</tag>
        <tag>Scala</tag>
        <tag>Spark</tag>
        <tag>Sbt</tag>
        <tag>sbt package</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Hadoop 和 Yarn 集群部署 Spark 集群]]></title>
    <url>%2F2018%2F11%2F15%2F%E5%9F%BA%E4%BA%8E-Hadoop-%E5%92%8C-Yarn-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2-Spark-%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[摘要基于 Hadoop 和 Yarn 集群部署 Spark 集群。 部署 Scala下载 Scala，放置到 /usr/local/ 下，并解压。1tar -zxvf scala-2.11.12.tgz 配置环境变量：1vim /etc/profile 中增加以下内容：12export SCALA_HOME=/usr/local/scala-2.11.12export PATH=$PATH:$SCALA_HOME/bin 重新载入：1source /etc/profile 验证：12345[root@weilu131 local]# scalaWelcome to Scala 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_192).Type in expressions for evaluation. Or try :help.scala&gt; 部署 Spark下载&amp;解压从 Apache Spark download page 下载安装包。在选择安装包类型时，如果是针对某个版本的 Hadoop 的话，可以选择 Pre-build for Apache Hadoop 2.6，或 Pre-build for Apache Hadoop 2.7 and later。分别是针对 2.6 和 2.7 版本的。或者也可以选择 Pre-build with user-provided Apache Hadoop，表示适用于所有版本 Hadoop。 下载后解压到 /usr/local/ 目录。1tar -zxvf spark-2.4.0-bin-hadoop2.6.tgz 开放端口8080：Master节点上的端口，提供Web UI8081：Worker节点上的端口，提供Web UI 1234567firewall-cmd --add-port=8080/tcp --permanentfirewall-cmd --add-port=7077/tcp --permanentfirewall-cmd --add-port=8081/tcp --permanentfirewall-cmd --add-port=8030/tcp --permanentfirewall-cmd --add-port=30000-50000/tcp --permanentfirewall-cmd --reload 配置环境变量在 Spark 的 conf 目录下拷贝一份 spark-env.sh 文件：1cp spark-env.sh.template spark-env.sh 在文件最后添加以下内容：12345678910export JAVA_HOME=/usr/local/jdk1.8.0_181export SCALA_HOME=/usr/local/scala-2.11.12export HADOOP_HOME=/usr/local/hadoop-2.6.0-cdh5.15.0export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport SPARK_WORKING_MEMORY=1g #每一个worker节点上可用的最大内存export SPARK_MASTER_IP=weilu131 #驱动器节点IPexport SPARK_DIST_CLASSPATH=$(/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hadoop classpath) HADOOP_CONF_DIR要让 Spark 与 YARN 资源管理器通信的话，需要将 Hadoop 的配置信息告诉 Spark，通过配置 HADOOP_CONF_DIR 环境变量实现。 设置 Yarn 为资源管理器将 conf 目录下的配置文件模板拷贝一份：1cp spark-defaults.conf.template spark-defaults.conf 修改其中的 master 配置：1spark.master yarn 配置公共 JAR 包配置 Spark 的 jar 包。在配合 Hadoop 集群下提交任务时，会将 jar 包提交到 HDFS 上，为防止每次提交任务时都提交，所以在 HDFS 上上传一份公共的。 在 HDFS 上创建存放 jar 包的目录：1/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -mkdir /spark_jars 检查目录是否创建：1/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -ls / 将 spark 下的 jar 包上传到该目录下：1/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/spark-2.4.0-bin-hadoop2.6/jars/* /spark_jars 使用如下命令检查是否上传成功：1/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -ls /spark_jars 在 spark-defaults.conf 中增加以下内容：1spark.yarn.jars hdfs://weilu131:9000/spark_jars/* 配置 slaves配置从节点主机名（或者IP），在 Spark 的 conf 目录下拷贝一份 slaves 文件：1cp slaves.template slaves 在其中添加以下内容：123weilu132weilu135weilu151 启动 Spark 集群1/usr/local/spark-2.4.0-bin-hadoop2.6/sbin/start-all.sh 验证jps 命令在 Master 和 Worker 节点上分别使用 jps 命令，可以分别看到 Master 和 Worker 进程。 Web UI访问Master：http://192.168.0.131:8080/可以看到当前集群的状况。 启动 spark-shellSpark-shell 是 spark 提供的一个交互式编程环境，基于 Scala。123456789101112131415161718192021222324[root@weilu131 conf]# /usr/local/spark-2.4.0-bin-hadoop2.6/bin/spark-shell SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/usr/local/spark-2.4.0-bin-hadoop2.6/jars/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.6.0-cdh5.15.0/share/hadoop/common/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]2018-11-15 19:57:20 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicableSetting default log level to &quot;WARN&quot;.To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).Spark context Web UI available at http://weilu131:4040Spark context available as &apos;sc&apos; (master = yarn, app id = application_1542209821671_0001).Spark session available as &apos;spark&apos;.Welcome to ____ __ / __/__ ___ _____/ /__ _\ \/ _ \/ _ `/ __/ &apos;_/ /___/ .__/\_,_/_/ /_/\_\ version 2.4.0 /_/ Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181)Type in expressions to have them evaluated.Type :help for more information.scala&gt; 如果能够正常进入 scala 交互命令行，说明部署没有问题。 对于Yarn而言，实际上 spark-shell 也是一个应用程序，因此可以在Yarn上看到这个启动的 shell： 部署History Server（不完整）该部分内容可以暂时不用，没有完成部署验证，留待之后补全。开放Web UI 端口12firewall-cmd --add-port=18080/tcp --permanentfirewall-cmd --reload 创建日志目录：1/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -mkdir /spark_logs spark-defaults.conf这部分配置的是 Spark 写入日志的信息。123spark.eventLog.enabled true spark.eventLog.dir hdfs://weilu131:9000/spark_logsspark.eventLog.compress true spark-env.sh1export SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=10 -Dspark.history.fs.logDirectory=hdfs://weilu131:9000/spark_logs&quot; 启动日志服务器1/usr/local/spark-2.4.0-bin-hadoop2.6/sbin/start-history-server.sh 访问日志服务器：http://192.168.0.131:18080 参考[1] http://spark.apache.org/docs/latest/monitoring.html[2] https://www.fwqtg.net/%E3%80%90spark%E5%8D%81%E5%85%AB%E3%80%91spark-history-server.html[3] https://my.oschina.net/u/3754001/blog/1811243]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>Spark on Yarn</tag>
        <tag>Spark-shell</tag>
        <tag>History-Server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 常用配置 - 禁止系统休眠]]></title>
    <url>%2F2018%2F11%2F14%2FCentOS-7-%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE-%E7%A6%81%E6%AD%A2%E7%B3%BB%E7%BB%9F%E4%BC%91%E7%9C%A0%2F</url>
    <content type="text"><![CDATA[摘要CentOS 常用配置 修改文件：1vi /etc/default/grub 在文件末尾增加：1pcie_aspm=off]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>休眠</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala 语法之走马观花]]></title>
    <url>%2F2018%2F11%2F14%2FScala-%E8%AF%AD%E6%B3%95%E4%B9%8B%E8%B5%B0%E9%A9%AC%E8%A7%82%E8%8A%B1%2F</url>
    <content type="text"><![CDATA[摘要走马观花的学习了一下 Scala 的基本语法，以下示例基于 Scala 2.11.12 版本编写。 变量可变和不可变量Scala 中变量有两种不同类型：1）val：不可变，在声明时必须初始化，且之后不能再复制2）var：可变，声明时需要初始化，之后可以再次赋值 数据类型scala 可以根据赋值推断数据类型1val myName = "dcwang" 也可以指定类型1val myName2 : String = "dcwang" 还可以利用全限定名指定类型，Scala中使用的是Java类型1val myName3 : java.lang.String = "dcwang" 所有 scala 文件默认会导入 java.lang 下面所有的包，在 scala中表示为：1import java.lang._ 用下划线表示所有 12var price = 1.2var price2 : Double = 2.2 在 scala 中所有的基本类型都是 类 基本运算在 scala 中所有运算都是调用函数1val sum1 = 5 + 3 相当于1val sum2 = (5).+(3) 富包装类Range121 to 5res1: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5) 1 to 5 相当于 1.to(5)12scala&gt; 1 until 5res2: scala.collection.immutable.Range = Range(1, 2, 3, 4)\ 12scala&gt; 1 to 10 by 2res3: scala.collection.immutable.Range = Range(1, 3, 5, 7, 9) 控制结构判断123456789101112131415val x = 6if (x &gt; 0) &#123; // do something&#125; else &#123; // do something&#125;if (x &gt; 0) &#123; //&#125; else if (x == 0) &#123; //&#125; else &#123; //&#125; scala 中可以将 if 中判断的值赋值给变量 while 循环123456var i = 9while (i &gt; 0) &#123; i -= 1 printf("i is %d \n", i)&#125; for 循环123456789101112for (i &lt;- 1 to 5) println(i)for (i &lt;- 1 to 5 by 2) println(i)// 守卫（guard）表达式for (i &lt;- 1 to 5 if i % 2 == 0) println(i)for (i &lt;- 1 to 5; j &lt;- 1 to 3) println(i * j) 文件操作文本文件读写Scala 需要调用 java.io.PrintWriter 实现文件写入12345678import java.io.PrintWriterval out = new PrintWriter("output.txt")for (i &lt;- 1 to 5) out.println(i)out.close() 使用 Scala.io.Source 的 getLines 方法实现对文件中所有行的读取12345678import scala.io.Sourceval inputFile = Source.fromFile("output.txt")val lines = inputFile.getLinesfor (line &lt;- lines) println(line) 异常捕获Scala 不支持 Java 中的 checked exception，将所有异常当做运行时异常Scala 仍然使用 try-catch 捕获异常 1234567891011121314import java.io.FileReaderimport java.io.FileNotFoundExceptionimport java.io.IOExceptiontry &#123; val file = new FileReader("NotExistFile.txt")&#125; catch &#123; case ex: FileNotFoundException =&gt; // do something case ex: IOException =&gt; // do something&#125; finally &#123; file.close()&#125; 容器 CollectionsScala 提供了一套丰富的容器库，包括列表、数组、集合、映射等Scala 用三个包来组织容器，分别是123scala.collectionscala.collection.mntable scala.collection.immutable 列表 List共享相同类型的不可变的对象序列，定义在 scala.collection.immutable 中List 在声明时必须初始化123456789101112131415var strList = List("BigData", "Hadoop", "Spark")// 返回头部第一个元素strList.head// 返回除了第一个之外的其他元素（一个新的List）strList.tail// 连接操作（从右侧开始）var oneList = List(1, 2, 3)var otherList = List(4, 5)var newList = oneList::otherList// Nil 是一个空列表对象var intList = 1::2::3::Nil 集合 Set集合中的元素插入式无序的，以哈希方法对元素的值进行组织，便于快速查找 集和包括可变和不可变集和，分别位于 scala.collection.mntable 和 scala.collection.immutable 包中，默认情况下是不可变集和。 12var mySet = Set("Hadoop", "Spark")mySet += "Scala" 变量是可变的，集合不可变，加操作导致生成了一个新的集合。 可变集合，在原集合中增加一个元素。1234import scala.collection.mutable.Setval myMutableSet = Set("BigData", "Spark")myMutableSet += "Scala" 映射 Map映射时一系列键值对的容器。也有可变和不可变两个版本，默认情况下是不可变的。 1val university = Map("XMU" -&gt; "Xiamen University", "THU" -&gt; "Tsinghua University", "PKU" -&gt; "Peking University") 可变映射123456import scala.collection.mutable.Mapval university = Map("XMU" -&gt; "Xiamen University", "THU" -&gt; "Tsinghua University", "PKU" -&gt; "Peking University")university("XMU") = "Ximan University" // updateuniversity("FZU") = "Fuzhou University" // add 遍历映射1234567891011for ( (k,v) &lt;- university) &#123; // do something&#125;for ( k &lt;- university.keys ) &#123; // do something&#125;for ( v &lt;- university.values ) &#123; // do something&#125; 迭代器 Iterator迭代器不是一个集合，而是一种访问集合的方法。 迭代器有两个基本操作：next 和 hasNext。 123456789val iter = Iterator("Hadoop", "Spark", "Scala")while (iter.hasNext) &#123; println(iter.next())&#125;for (elem &lt;- iter) &#123; println(elem)&#125; grouped &amp; slidinggrouped 返回元素的增量分块1234567891011scala&gt; val xs = List(1,2,3,4,5)xs: List[Int] = List(1, 2, 3, 4, 5)scala&gt; val git = xs grouped 3git: Iterator[List[Int]] = non-empty iteratorscala&gt; git.next()res0: List[Int] = List(1, 2, 3)scala&gt; git.next()res1: List[Int] = List(4, 5) sliding 生成一个滑动元素的窗口1234567891011scala&gt; val sit = xs sliding 3sit: Iterator[List[Int]] = non-empty iteratorscala&gt; sit.next()res3: List[Int] = List(1, 2, 3)scala&gt; sit.next()res4: List[Int] = List(2, 3, 4)scala&gt; sit.next()res5: List[Int] = List(3, 4, 5) 数组 Array是一种可变的、可索引的、元素具有相同类型的数据集合，Scala提供了类似于Java中泛型的机制指定数组类型。也可以不指定类型。 123456val intValueArr = new Array[Int](3)intValueArr(0) = 23intValueArr(1) = 34intValueArr(2) = 45val strArr = Array("BigData", "Hadoop", "Spark") 多维数组定义多维数组使用 ofDim() 方法1val myMatrix = Array.ofDim[Int](3,4) // 三行四列 访问元素1myMatrix(0)(1) 不定长数组1234567891011121314import scala.collection.mutable.ArrayBufferval aMutableArr = ArrayBuffer(10, 20, 30)aMutableArr += 40println(aMutableArr) // ArrayBuffer(10, 20, 30, 40)aMutableArr.insert(2, 60, 40)println(aMutableArr) // ArrayBuffer(10, 20, 60, 40, 30, 40)aMutableArr -= 40println(aMutableArr) // ArrayBuffer(10, 20, 60, 30, 40)var temp = aMutableArr.remove(2)println(aMutableArr) // ArrayBuffer(10, 20, 30, 40) 元组 Tuple不同类型的值的集合。123val tuple = ("BigData", 2015, 45.0)println(tuple._1)println(tuple._2) 面向对象类基本类结构简单类123456789class Counter &#123; private var value = 0 def increment(): Unit = &#123; value += 1 &#125; def current(): Int = &#123; value &#125;&#125; 定义方法可以省略返回类型：123456789class Counter &#123; private var value = 0 def increment() &#123; value += 1 &#125; def current(): Int = &#123; value &#125;&#125; 方法传参123456789class Counter &#123; private var value = 0 def increment(step: Int) &#123; value += step &#125; def current(): Int = &#123; value &#125;&#125; 创建对象12345val myCounter = new CountermyCounter.increment()println(myCounter.current())val myCounter2 = new Counter() 编译&amp;执行如下定义的这样一个类在执行时，直接使用 scala 解释器执行即可，不需要编译。123456789class Counter &#123; private var value = 0 def increment() &#123; value += 1 &#125; def current(): Int = &#123; value &#125;&#125; 如果要编译，需要创建一个单例对象：1234567891011121314151617class Counter &#123; private var value = 0 def increment() &#123; value += 1 &#125; def current(): Int = &#123; value &#125;&#125;object MyCounter &#123; def main(args:Array[String]) &#123; val myCounter = new Counter myCounter.increment() println(myCounter.current) &#125;&#125; 编译（后面跟的是文件名）：1scalac counter.scala 编译之后会产生一些文件：123Counter.classMyCounter.classMyCounter$.class 执行执行时后面跟的是包含 main 方法的对象名称1scala -classpath . MyCounter getter &amp; setter1234567891011121314151617181920212223242526272829303132333435class Person &#123; private var privateName = "dcwang" private var privateAge = 25 // def name = privateName def setName(newName : String) &#123; privateName = newName &#125; def age = privateAge def setAge(newAge : Int) &#123; privateAge = newAge &#125; def grown(step : Int) &#123; privateAge += step &#125;&#125;object SomeOne &#123; def main(args:Array[String]) &#123; val someOne = new Person println(someOne.name) someOne.setName("yz") println(someOne.name) println(someOne.age) someOne.grown(2) println(someOne.age) &#125;&#125; 构造器Scala构造器包含一个主构造器和若干个辅构造器辅助构造器的名称为 this ，每个辅助构造器都必须调用一个已有的构造器 1234567891011121314151617181920212223242526272829303132333435363738class Person &#123; private var privateName = "dcwang" private var privateAge = 25 def this(name : String) &#123; this() // invoke main constructor this.privateName = name &#125; def this(name : String, age : Int) &#123; this(name) this.privateAge = age &#125; def name = privateName def setName(newName : String) &#123; privateName = newName &#125; def age = privateAge def setAge(newAge : Int) &#123; privateAge = newAge &#125; def grown(step : Int) &#123; privateAge += step &#125;&#125;object SomeOne &#123; def main(args:Array[String]) &#123; val someOne = new Person("yz", 18) println(someOne.name) println(someOne.age) &#125;&#125; 对象单例对象123456789101112object Person &#123; private var id = 0 def newId() = &#123; id += 1 id &#125;&#125;println(Person.newId())println(Person.newId())println(Person.newId()) 伴生对象在 Java 中经常用到同时包含实例方法和静态方法的类，在 Scala 中可以用伴生对象来实现类和它的伴生对象必须在同一个文件中，并且可以相互访问私有成员当单例对象与某个类具有相同的名称时，它被成为这个类的伴生对象 12345678910111213141516171819202122232425262728class Person &#123; private val id = Person.newPersonId() private var name = "" def this(name : String) &#123; this() this.name = name &#125; def info() &#123; println("The id of %s is %d. \n".format(name, id)) &#125;&#125;object Person &#123; private var lastId = 0 private def newPersonId() = &#123; lastId += 1 lastId &#125; def main(args : Array[String]) &#123; val person1 = new Person("yz") val person2 = new Person("yj") person1.info() person2.info() &#125;&#125; applay 方法update 方法继承在子类中重写超类抽象方法时不需要使用 override 关键字重写一个非抽象方法必须使用 override 修饰符只有主构造器可以调用超类的主构造器可以重写超类中的子段 抽象类12345678// 抽象类，不能直接实例化abstract class Car &#123; // 抽象子段，不需要初始化 val carBrand : String // 抽象方法，不需要使用 abstract 关键字 def info() def greeting() &#123; println("Welcome to my car!") &#125;&#125; 继承抽象类123456789101112class BMWCar extends Car &#123; override val carBrand = "BMW" // 重写抽象方法不用加 override def info() &#123; printf("This is a car") &#125; // 重写非抽象方法必须使用 override override def greeting() &#123; printf("something") &#125;&#125; 特质（trait）在 Scala 中没有接口的概念，而是提供了 trait，它实现了接口的功能，以及许多其他特性trait 是 Scala 中代码重用的基本单元，可以同时拥有抽象方法和具体方法在 Scala 中一个类只能继承一个超类，但是可以实现多个 trait，从而拥有 trait 中的方法和字段，实现多重继承。 123456789101112trait CarId &#123; var id : Int def currentId() : Int&#125;class BYDCarId extends CarId &#123; // 使用 extends 关键字继承 trait override var id = 10000 def currentId() : Int = &#123; id += 1; id &#125;&#125; 混入多个trait123456789101112131415161718trait CarId &#123; var id : Int def currentId() : Int&#125;trait CarGreeting &#123; def greeting(msg : String) &#123; println(msg) &#125;&#125;// 使用 extends 关键字继承 trait// 后面混入的多个 trait 可以反复使用 with 关键字class BYDCarId extends CarId with CarGreeting &#123; override var id = 10000 def currentId() : Int = &#123; id += 1; id &#125;&#125; 模式匹配简单匹配类似于 Java 中的 switch12345678910val colorNum = 1val colorStr = colorNum match &#123; case 1 =&gt; "red" case 2 =&gt; "green" case 3 =&gt; "yellow" case _ =&gt; "Not Allowed"&#125;println(colorStr) 获取匹配值可以声明一个变量 unexpected，用于获取进行匹配的值，然后在分支中进行操作。12345678910val colorNum = 4val colorStr = colorNum match &#123; case 1 =&gt; "red" case 2 =&gt; "green" case 3 =&gt; "yellow" case unexpected =&gt; unexpected + " is Not Allowed"&#125;println(colorStr) 类型模式可以匹配元素的类型，根据类型选择不同操作。1234567891011for (elem &lt;- List(9, 12.3, "Spark", "Hello")) &#123; val str = elem match &#123; case i : Int =&gt; i + " is an int value." case d : Double =&gt; d + " is a double value." case s : String =&gt; s + " is a string value." case "Hello" =&gt; "Hello is here." case _ =&gt; "unexpected value" &#125; println(str)&#125; 守卫（guard）语句将 case 的选项设置为全匹配，然后用 if 判断进行处理。那为什么不直接用判断？ case类的匹配一次匹配多个值，或者对比对象？12345678910111213case class Car(brand: String, price: Int)val myBYDCar = new Car("BYD", 89000)val myBMWCar = new Car("BMW", 1200000)val myBenzCar = new Car("Benz", 1500000)for (car &lt;- List(myBYDCar, myBMWCar, myBenzCar)) &#123; car match &#123; case Car("BYD", 89000) =&gt; println("BYD") case Car("BMW", 1200000) =&gt; println("BMW") case Car(brand, price) =&gt; println(brand + price) &#125;&#125; Option类型处理 None 返回值1234567val someMap = Map("spark" -&gt; 123, "hadoop" -&gt; 234)var someValue = someMap.get("hive")println(someValue.getOrElse("No such value")) // No such valuesomeValue = someMap.get("spark")println(someValue.getOrElse("No such value")) // 123 Option[T] 类中的 T 可以是各种数据类型，如果一个 Option 对象中包含值，那么这个对象就是 Some 类型，否则就是 None 类型。 如果返回值是一个集合，就可以对其使用 map 、foreach 或 filter 等方法1someMap.get("spark").foreach(println) 函数式编程函数定义函数字面量每个函数本身是一个值，可以被传递，类似于 JavaScript 中函数的概念。 函数的类型和值函数的类型是指传入参数和返回值的类型。1def counter(value: Int): Int = &#123; value += 1 &#125; 上面这个函数的类型就是： (Int) =&gt; Int如果有多个参数，使用逗号隔开；否则括号可以省略。 函数的值是指去掉了参数类型和返回值类型之后剩下的参数和函数体：1(value) =&gt; &#123; value += 1 &#125; 对比声明一个基本类型来声明一个函数：1val num : Int = 5 基本类型，声明一个变量 num，指定变量类型为 Int，然后给变量赋值为 5。 12var counter = (value : Int) =&gt; &#123; value + 1 &#125; : Intprintln(counter(2)) 匿名函数、Lamda表达式与闭包 Lamda 表达式123(参数) =&gt; 表达式(num: Int) =&gt; num * 2 闭包在一个函数内部可以访问外部变量的形式。123456var more = 1var addMore = (x: Int) =&gt; x + moreprintln(addMore(10))more = 5println(addMore(10)) 占位符语法 为了让函数字面量更简洁，可以使用下划线作为一个或多个参数的占位符，每个参数仅可以在函数字面量中出现一次。 1234567val numList = List(1, 2, 3, 4, 5)val res = numList.filter(x =&gt; x &gt; 3)print(res)val res2 = numList.filter(_ &gt; 3)print(res2) 针对集合的操作mapmap操作是针对集合的典型变换操作，将函数应用到集合中的每一个元素上，并产生一个新的结果集合。 12345val books = List("Hadoop", "Hive", "Spark")println(books) // List(Hadoop, Hive, Spark)val newbooks = books.map(s =&gt; s.toUpperCase)println(newbooks) // List(HADOOP, HIVE, SPARK) flatMap调用一个函数，将一个集合中的每个元素处理后形成的多个集合，合并成一个集合。123val books = List("Hadoop", "Hive", "Spark")val letters = books.flatMap(s =&gt; s.toList)println(letters) // List(H, a, d, o, o, p, H, i, v, e, S, p, a, r, k) filter遍历一个集合，过滤其中的元素形成一个新的集合。 1234val books = List("Hadoop", "Hive", "Spark")val newBooks = books.filter(&#123;value =&gt; value.contains("a")&#125;)println(newBooks) // List(Hadoop, Spark) reduce对给定集合中的两两元素，指定某给定函数的操作。1234567val numList = List(1, 3, 5)var res = numList.reduceLeft(&#123;_ - _&#125;)println(res) // -7res = numList.reduceRight(&#123;_ - _&#125;)println(res) // 3// 1 - (3 - 5) fold带初始值的规约12val numList = List(1, 3, 5)val res = numList.fold(10)(&#123;_ * _&#125;)]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>Scala</tag>
        <tag>Syntax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7 安装 GeForce RTX 2080 Ti 驱动]]></title>
    <url>%2F2018%2F11%2F13%2FCentOS7-%E5%AE%89%E8%A3%85-GeForce-RTX-2080-Ti-%E9%A9%B1%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[摘要CentOS7 采用手动方式安装 RTX 2080 ti 驱动。 下载NVIDIA官方驱动到NVIDIA的官方驱动网站下载对应显卡的驱动程序，下载后的文件格式为run。 https://www.geforce.cn/drivers bios禁用禁用secure boot，也就是设置为disable如果没有禁用secure boot,会导致NVIDIA驱动安装失败，或者不正常。 https://www.technorms.com/45538/disable-enable-secure-boot-asus-motherboard-uefi-bios-utility 禁用nouveau(未验证)12345678910111213[root@iflysse123 ~]# lshw -numeric -C display *-display UNCLAIMED description: VGA compatible controller product: NVIDIA Corporation [10DE:1E07] vendor: NVIDIA Corporation [10DE] physical id: 0 bus info: pci@0000:01:00.0 version: a1 width: 64 bits clock: 33MHz capabilities: pm msi pciexpress vga_controller bus_master cap_list configuration: latency=0 resources: memory:f6000000-f6ffffff memory:e0000000-efffffff memory:f0000000-f1ffffff ioport:e000(size=128) memory:f7000000-f707ffff 观察 configuration 中是否有一项 driver=nouveau，如果有，就需要禁用掉 nouveau 打开编辑配置文件： /etc/modprobe.d/blacklist.conf 在最后一行添加： blacklist nouveau 这一条的含义是禁用nouveau第三方驱动，之后也不需要改回来。 由于nouveau是构建在内核中的，所以要执行下面命令生效: update-initramfs -u mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bakdracut -v /boot/initramfs-$(uname -r).img $(uname -r) 停止可视化桌面为了安装新的Nvidia驱动程序，我们需要停止当前的显示服务器。最简单的方法是使用telinit命令更改为运行级别3。执行以下linux命令后，显示服务器将停止，因此请确保在继续之前保存所有当前工作（如果有）： telinit 3 验证## 12345678910111213[root@iflysse123 home]# lshw -numeric -C display *-display description: VGA compatible controller product: NVIDIA Corporation [10DE:1E07] vendor: NVIDIA Corporation [10DE] physical id: 0 bus info: pci@0000:01:00.0 version: a1 width: 64 bits clock: 33MHz capabilities: pm msi pciexpress vga_controller bus_master cap_list rom configuration: driver=nvidia latency=0 resources: irq:16 memory:f6000000-f6ffffff memory:e0000000-efffffff memory:f0000000-f1ffffff ioport:e000(size=128) memory:f7000000-f707ffff ## 123456789101112131415161718[root@iflysse123 home]# nvidia-smiFri Nov 9 17:14:38 2018 +-----------------------------------------------------------------------------+| NVIDIA-SMI 410.73 Driver Version: 410.73 CUDA Version: 10.0 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce RTX 208... Off | 00000000:01:00.0 Off | N/A || 32% 40C P0 56W / 280W | 0MiB / 10989MiB | 0% Default |+-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 参考https://linuxconfig.org/how-to-install-the-nvidia-drivers-on-centos-7-linuxhttps://blog.csdn.net/wf19930209/article/details/81877822]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>NVIDIA</tag>
        <tag>2080ti</tag>
        <tag>Driver</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Scrapyd 部署、管理 Scrapy 爬虫]]></title>
    <url>%2F2018%2F11%2F07%2F%E5%9F%BA%E4%BA%8E-Scrapyd-%E9%83%A8%E7%BD%B2%E3%80%81%E7%AE%A1%E7%90%86-Scrapy-%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[摘要Scrapy 是 Python 下一款非常好的爬虫框架，使用帮助快速实现爬虫。当爬虫数量较少时可以直接通过命令行的方式进行管理。但随着爬虫数量越来越多，版本不断更新，这时候就需要一些工具帮助我们进行爬虫管理了。而 Scrapyd 就是这样一个工具，其提供了一些基于 HTTP 的接口，帮助我们管理爬虫项目以及查看任务情况。 构建爬虫首先使用 scrapy 构建一个简单的爬虫，示例中随便使用了一个地址，只是请求页面，不做具体的内容解析和处理。 安装 Scrapy1pip install scrapy 如果是在 Windowns 上安装的话，最好先安装以下两块内容： Twisted-18.7.0-cp36-cp36m-win_amd64.whl 安装 twisted 编译好的版本，这个可以在如下网站下载： https://www.lfd.uci.edu/~gohlke/pythonlibs/ 安装，进入下载文件所在目录，使用如下命令：1pip install Twisted-18.7.0-cp36-cp36m-win_amd64.whl pywin32-223.win-amd64-py3.6.exe 这个直接双击安装即可。 然后使用如下命令安装 scrapy：1pip install scrapy 初始化 Scrapy 项目scrapy 提供了一个非常方便的命令直接创建好一个爬虫项目的框架，进入一个你想放置爬虫项目的目录，执行以下命令：1scrapy startproject IpSourceXici 命令执行完成后，会创建如下的一个目录结构： 创建爬虫在 spiders 目录中创建一个 py 文件，命名为 XiciSpider.py，其中添加以下内容： 123456789101112131415161718import scrapyclass XiciSpider(scrapy.Spider): name = &quot;XiciSpider&quot; def start_requests(self): for index in range(1, 7): url = &quot;http://www.89ip.cn/index_&quot; + str(index) + &quot;.html&quot; yield scrapy.Request(url=url, callback=self.parse) def parse(self, response): try: print(response.url) yield item except Exception as e: pass 这是一个非常简单的爬虫，仅仅访问了 URL，然后输出URL，没有进行任何处理。 配置爬虫根据需要在 settings.py 文件中设置一些配置。由于我们只是作示例，因此对爬取速度、数量等要求都不高，因此我们最好不要对目标服务器产生过高的负载。 USER_AGENT 伪装浏览器 1USER_AGENT = &apos;Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36&apos; CONCURRENT_REQUESTS 设置最大的并行数量，这里仅设置一个即可。默认不设置是16个。 1CONCURRENT_REQUESTS = 1 DOWNLOAD_DELAY 设置随机停顿，即在爬取同一个源头的网页时，设置停顿时间 DOWNLOAD_DELAY 乘以一个随机种子（通过 RANDOMIZE_DOWNLOAD_DELAY 开启）。单位是秒。 12RANDOMIZE_DOWNLOAD_DELAY = TrueDOWNLOAD_DELAY = 20 测试爬虫进入爬虫项目根目录，在这个示例中就是进入 IpSourceXici 目录，爬虫项目会构建两个 IpSourceXici 目录，进入最外层的一个即可，然后执行命令： 1scrapy crawl XiciSpider 之后会输出如下内容，说明爬虫没有问题： 配置 Scrapyd 服务安装 Scrapyd直接使用命令安装： 1pip install scrapyd 安装完成之后在命令行中输入命令启动 scrapyd 服务： 1scrapyd 服务默认监听 6800 端口。 测试 Scrapyd 服务直接在浏览器中访问地址： http://localhost:6800/ 可以查看到如下页面： 说明服务启动正常。当然，需要注意当前的 Available projects 后面应该是空的，因为我们还没有部署项目，这里是部署后的结果。 部署爬虫部署爬虫可以使用专门的部署工具 scrapyd-deploy。 安装 Scrapyd-clientscrapyd-deploy 是 scrapyd-client 中的一个命令。 如果是在 linux 平台下安装，可以直接使用命令： 1pip install scrapyd-client 不过这种方式在 windows 平台下安装会有问题，应该使用源码直接安装。 从 github 上下载： https://github.com/scrapy/scrapyd-client/releases 解压后进入源码的根目录，使用命令安装： 1python setup.py install 配置 scrapy.cfg修改爬虫项目根目录下文件 scrapy.cfg，参考以下内容： 123456[settings]default = IpSourceXici.settings[deploy:XiciProxy]url = http://localhost:6800/project = IpSourceXici 主要是取消 url 的注释，以及设置 deploy 的 Name。 部署进入爬虫所在目录使用命令：1scrapyd-deploy XiciProxy -p IpSourceXici -v r1 如果得到如下反馈，应该就是部署成功了1234Packing version r1Deploying to project &quot;IpSourceXici&quot; in http://localhost:6800/addversion.jsonServer response (200):&#123;&quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;project&quot;: &quot;IpSourceXici&quot;, &quot;version&quot;:&quot;r1&quot;, &quot;spiders&quot;: 1&#125; 验证可以直接访问：http://localhost:6800/ 查看以有效的项目，同时可以使用如下接口对项目进行简单的管理。 参考接口123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#列出所有工程http://localhost:6800/listprojects.json&#123; &quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;projects&quot;: [ &quot;IpSourceXici&quot; ]&#125;#查看爬虫http://localhost:6800/listspiders.json?project=IpSourceXici&#123; &quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;spiders&quot;: [ &quot;XiciSpider&quot; ]&#125;#列出版本http://localhost:6800/listversions.json?project=IpSourceXici&#123; &quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;versions&quot;: [ &quot;r1&quot;, &quot;r2&quot; ]&#125;#删除版本(POST)http://localhost:6800/delversion.jsonproject=IpSourceXiciversion=r2&#123; &quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;&#125;#执行爬虫(POST)http://localhost:6800/schedule.jsonproject=IpSourceXicispider=XiciSpider&#123; &quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;jobid&quot;: &quot;86a34534e23811e8b1413c970e0087f3&quot;&#125;#查看爬虫的执行状态http://localhost:6800/listjobs.json?project=IpSourceXici&#123; &quot;node_name&quot;: &quot;weilu-PC&quot;, &quot;status&quot;: &quot;ok&quot;, &quot;pending&quot;: [], &quot;running&quot;: [ &#123; &quot;id&quot;: &quot;86a34534e23811e8b1413c970e0087f3&quot;, &quot;spider&quot;: &quot;XiciSpider&quot;, &quot;pid&quot;: 5260, &quot;start_time&quot;: &quot;2018-11-07 10:55:06.534457&quot; &#125; ], &quot;finished&quot;: []&#125; 参考资料：[1] https://scrapyd.readthedocs.io/en/latest/index.html]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>Scrapy</tag>
        <tag>Scrapyd-client</tag>
        <tag>Scrapyd-deploy</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 Hadoop 集群部署 ZooKeeper 和 HBase 集群]]></title>
    <url>%2F2018%2F11%2F05%2F%E5%9F%BA%E4%BA%8E-Hadoop-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2-ZooKeeper-%E5%92%8C-HBase-%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[摘要之前的环境中配置了 Hadoop 集群以及 Yarm。现在基于 HDFS 部署 ZooKeeper 集群和 HBase 集群。 三台机器中，将 131 作为 Master、其余两台作为 Region。另外设置 132 为 backup-master。三台机器都部署 zookeeper。 部署 Zookeeper在 weilu131/weilu132/weilu135三台上部署 zookeeper。 开放端口12345678910111213141516firewall-cmd --add-port=2888/tcp --permanentfirewall-cmd --add-port=3888/tcp --permanentfirewall-cmd --add-port=2181/tcp --permanentfirewall-cmd --add-port=16000/tcp --permanentfirewall-cmd --add-port=16010/tcp --permanentfirewall-cmd --add-port=16020/tcp --permanentfirewall-cmd --add-port=60000/tcp --permanentfirewall-cmd --add-port=50010/tcp --permanentfirewall-cmd --add-port=60020/tcp --permanentfirewall-cmd --add-port=60010/tcp --permanentfirewall-cmd --add-port=60030/tcp --permanentfirewall-cmd --reload 将安装包拷贝到 /usr/local/ 目录下，解压：1tar -zxvf zookeeper-3.4.5-cdh5.15.0.tar.gz zoo.cfg在 zookeeper 包的根目录创建一个文件夹，随意命名：1/usr/local/zookeeper-3.4.5-cdh5.15.0/zookeeperDataDir 进入 /usr/local/zookeeper-3.4.5-cdh5.15.0/conf 目录，拷贝一份 zoo_sample.cfg 文件，命名为 zoo.cfg：1cp zoo_sample.cfg zoo.cfg 在 zoo.cfg 中配置以下内容：123456789101112131415161718# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial synchronization phase can takeinitLimit=10# The number of ticks that can pass between sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.dataDir=/usr/local/zookeeper-3.4.5-cdh5.15.0/zookeeperDataDir# the port at which the clients will connectclientPort=2181server.1=weilu131:2888:3888server.2=weilu132:2888:3888server.3=weilu135:2888:3888 myid在每台 ZooKeeper 节点的数据目录（dataDir目录）下存放一个 myid 文件，文件中存放一个集群中唯一的ID，表明这台机器的ID，ID范围是 1~255 分发配置将 zoo.cfg 和 myid 分发到其他及台机器上，对应修改 myid。 1scp zoo.cfg root@weilu135:/usr/local/zookeeper-3.4.5-cdh5.15.0/conf 分发目录时增加参数 -r1scp -r zookeeperDataDir/ root@weilu151:/usr/local/zookeeper-3.4.5-cdh5.15.0/zookeeperDataDir 启动 zookeeper启动时需要分别在每个节点使用 bin/zkServer.sh 脚本启动：1/usr/local/zookeeper-3.4.5-cdh5.15.0/bin/zkServer.sh start 在所有节点启动完成之前，如果使用 ./zkServer.sh status 查看状态时，会提示123JMX enabled by defaultUsing config: /usr/local/zookeeper-3.4.5-cdh5.15.0/bin/../conf/zoo.cfgError contacting service. It is probably not running. 如果都启动之后，再次查看会得到：123JMX enabled by defaultUsing config: /usr/local/zookeeper-3.4.5-cdh5.15.0/bin/../conf/zoo.cfgMode: follower 部署 HBase将安装包拷贝到 /usr/local/ 目录下，解压 Hbase：1tar -zxvf hbase-1.2.0-cdh5.15.0.tar.gz hbase-env.sh配置 JDK 环境：1export JAVA_HOME=/usr/local/jdk1.8.0_181 配置不使用 HBase 默认自带的 ZooKeeper：1export HBASE_MANAGES_ZK=false hbase-site.xml创建临时文件目录：1mkdir -p /home/hbase/tmp 配置123456789101112131415161718192021222324252627282930&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hbase/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://weilu131:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;weilu131,weilu132,weilu135&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;60010&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.regionserver.info.port&lt;/name&gt; &lt;value&gt;60030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master.port&lt;/name&gt; &lt;value&gt;16000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; regionservers配置 Region 服务器列表，在 regionservers 文件中配置以下内容，类似于 Hadoop 集群的 slaves 列表：12weilu132weilu135 backup-masters在 conf/backup-masters 中用来配置备份 master 服务器。1weilu132 分发配置1234567891011# hbase-env.sh 文件scp hbase-env.sh root@weilu132:/usr/local/hbase-1.2.0-cdh5.15.0/conf/# hbase-site.xmlscp hbase-site.xml root@weilu132:/usr/local/hbase-1.2.0-cdh5.15.0/conf/# regionserversscp regionservers root@weilu132:/usr/local/hbase-1.2.0-cdh5.15.0/conf/#backup-mastersscp backup-masters root@weilu132:/usr/local/hbase-1.2.0-cdh5.15.0/conf/ 启动服务1/usr/local/hbase-1.2.0-cdh5.15.0/bin/start-hbase.sh 验证对于 Master 可以访问以下地址，查看集群情况。 访问Master：http://192.168.0.131:60010 对于 Region 可以访问以下地址，查看情况。 访问Master：http://192.168.0.132:60030 问题zookeeper.MetaTableLocator: Failed verification of hbase:meta1234567892018-11-05 22:27:04,735 INFO [weilu131:60000.activeMasterManager] zookeeper.MetaTableLocator: Failed verification of hbase:meta,,1 at address=weilu135,60020,1541427295657, exception=org.apache.hadoop.hbase.NotServingRegionException: Region hbase:meta,,1 is not online on weilu135,60020,1541428018822 at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2997) at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegion(RSRpcServices.java:1069) at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegionInfo(RSRpcServices.java:1349) at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$2.callBlockingMethod(AdminProtos.java:22233) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2191) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:112) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:183) at org.apache.hadoop.hbase.ipc.RpcExecutor$Handler.run(RpcExecutor.java:163) 这个问题是 zookeeper 的数据出错导致的，将 zookeeper 集群都停掉，然后将 Datadir目录中除了配置的 myid 以外的文件都删掉，然后启动 zookeeper，应该就可以了。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>ZooKeeper</tag>
        <tag>HBase</tag>
        <tag>集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop集群部署方案]]></title>
    <url>%2F2018%2F10%2F30%2FHadoop%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[摘要记录一下 Hadoop 集群部署过程，简单的一拖三，包括 Hadoop 和 Yarn。 环境说明配置环境准备四台机器，四台机器环境是 CentOS 7.5，IP和主机名配置如下：1234192.168.0.131 weilu131192.168.0.132 weilu132192.168.0.135 weilu135192.168.0.151 weilu151 注意：主机名不可以有下划线 前置配置免密登录生成密钥：1ssh-keygen 这个会生成在 /root/.ssh/ 目录下，然后进入该目录，将公钥拷贝到其它两台机器上：1ssh-copy-id -i id_rsa.pub root@weilu132 具体可以参考：https://weilu2.github.io/2018/10/08/CentOS-7-4-%E9%85%8D%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/ JDK环境从本地将JDK包拷贝到机器上：123[root@weilu_125 packages]# scp jdk-8u181-linux-x64.tar.gz root@192.168.0.131:/usr/local/root@192.168.0.131&apos;s password: jdk-8u181-linux-x64.tar.gz 100% 177MB 11.1MB/s 00:15 解压到当前目录：1tar -xvzf jdk-8u181-linux-x64.tar.gz 配置环境变量：1vim /etc/profile 在其中末尾添加以下内容：123export JAVA_HOME=/usr/local/jdk1.8.0_181 export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar export PATH=$PATH:$JAVA_HOME/bin 然后重新加载配置文件：1source /etc/profile 检查配置结果：1234[root@weilu_132 local]# java -versionjava version &quot;1.8.0_181&quot;Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) 用相同的方法将其余两台机器也配置好。 防火墙配置如果开启了防火墙，那么就需要开启以下端口 9000这个端口是 Hadoop 集群中 NameNode 与 DataNode 通信的端口。 8031这个端口是 Yarn 的 ResourceManager 与 NodeManager 通信的端口。 配置 Hadoop下载：http://archive.cloudera.com/cdh5/cdh/5/ 文件放置在 /usr/local/ 下，解压缩：1tar -zxvf hadoop-2.6.0-cdh5.15.0.tar.gz hadoop-env.sh修改 hadoop 环境配置中的 JDK 配置：1vim /usr/local/hadoop-2.6.0-cdh5.15.0/etc/hadoop/hadoop-env.sh 修改其中的 JAVA_HOME1export JAVA_HOME=/usr/local/jdk1.8.0_181 core-site.xml配置 NameNode 的URI：123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://weilu131:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hdfs-site.xml配置 NameNode配置 NameNode 在本地文件系统中存放内容的位置。首先创建目录：1mkdir -p /home/hadoop/tmp/dfs/name 配置 DataNode配置 DataNode 在本地文件系统中存放内容的位置。首先创建目录：1mkdir -p /home/hadoop/tmp/dfs/data 这个目录是自己定义的。12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置 Yarnyarn-site.xml12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;weilu131&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置 MapReducemapred-site.xml在每个节点上配置以下内容。 从模板中拷贝一份 mapred-site.xml 文件：1cp mapred-site.xml.template mapred-site.xml 在新文件中添加以下内容：123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置从节点slaves在 slaves 文件中列出从节点的主机名或IP： 123weilu132weilu135weilu151 配置内存分配默认的内存分配策略是基于至少 8G 内存的分配，如果所使用的机器内存少于 8G，就需要进行手动配置。 分配策略一个 YARN 的 job 执行需要两类资源： 一个 Application Master(AM)，负责监控应用以及在集群上协调分布的 executor； 若干 executor，由 AM 创建，实际 job 的执行者。 他们都运行在从节点的容器中。每个从节点运行一个 NodeManager 守护进程，负责在节点上创建容器。整个集群在一个 ResourceManager 的管理下，RM 负责在从节点上调度容器等。 整个集群正常工作，需要配置四类资源：1、每个节点允许所有 YARN 容器占用的总的内存。 这个内存应该比其他所有的都大，否则应用程序无法正常执行。 这个值通过 yarn-site.xml 文件中的 yarn.nodemanager.resource.memory-mb 配置。 2、单个容器允许占用的内存 每个节点上可以有多个容器，单个容器需要配置允许的最大内存和最小内存。 通过 yarn-site.xml 文件中的 yarn.scheduler.maximum-allocation-mb 和 yarn.scheduler.minimum-allocation-mb 配置。 3、ApplicationMaster 允许的内存 这个值是在容器最大内存限制内的一个常数值。 通过 mapred-site.xml 文件中的 yarn.app.mapreduce.am.resource.mb 配置。 4、每个 map 和 reduce 操作允许的最大内存 通过 mapred-site.xml 文件中的 mapreduce.map.memory.mb 和 mapreduce.reduce.memory.mb 配置。 这些配置之间的关系可以用下图表示： 内存分配 属性 内存大小 yarn.nodemanager.resource.memory-mb 3000 yarn.scheduler.maximum-allocation-mb 3000 yarn.scheduler.minimum-allocation-mb 256 yarn.app.mapreduce.am.resource.mb 512 mapreduce.map.memory.mb 512 mapreduce.reduce.memory.mb 512 yarn-site.xml在 yarn-site.xml 文件中添加以下内容： 12345678910111213141516&lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt; &lt;value&gt;3000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;3000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;256&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; mapred-site.xml在 mapred-site.xml 文件中添加以下内容： 123456789101112&lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt; &lt;value&gt;512&lt;/value&gt;&lt;/property&gt; 分发配置文件注意，以上所有配置都是在 weilu131 上配置的，完成之后，将配置文件分发到其他几台机器上，每台机器的配置文件都是一模一样的，下面使用 scp 命令分发，以从节点文件为例： 123scp slaves root@weilu_132:/usr/local/hadoop-2.6.0-cdh5.15.0/etc/hadoopscp slaves root@weilu_135:/usr/local/hadoop-2.6.0-cdh5.15.0/etc/hadoopscp slaves root@weilu_151:/usr/local/hadoop-2.6.0-cdh5.15.0/etc/hadoop 格式化文件系统123[root@weilu_131 bin]# /usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs namenode -format...18/10/30 21:12:08 INFO common.Storage: Storage directory /home/hadoop/tmp/dfs/name has been successfully formatted. 看到有这行输出表明格式化成功。 启动12345678910111213141516[root@weilu_131 sbin]# /usr/local/hadoop-2.6.0-cdh5.15.0/sbin/start-all.sh This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh18/10/30 22:17:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableStarting namenodes on [weilu131]weilu131: starting namenode, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/hadoop-root-namenode-weilu131.outweilu132: starting datanode, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/hadoop-root-datanode-weilu_132.outweilu135: starting datanode, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/hadoop-root-datanode-weilu_135.outweilu151: starting datanode, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/hadoop-root-datanode-weilu_151.outStarting secondary namenodes [0.0.0.0]0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/hadoop-root-secondarynamenode-weilu131.out18/10/30 22:17:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicablestarting yarn daemonsstarting resourcemanager, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/yarn-root-resourcemanager-weilu_131.outweilu132: starting nodemanager, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/yarn-root-nodemanager-weilu_132.outweilu135: starting nodemanager, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/yarn-root-nodemanager-weilu_135.outweilu151: starting nodemanager, logging to /usr/local/hadoop-2.6.0-cdh5.15.0/logs/yarn-root-nodemanager-weilu_151.out 验证验证进程在 weilu131上：12345[root@weilu131 sbin]# jps3927 NameNode4520 Jps4254 ResourceManager4111 SecondaryNameNode 因为在 weilu131 上部署了 NameNode 和 ResourceManager，因此使用 jps 命令应该能够看到这几个进程。 在其余几个节点上：1234[root@weilu132 hadoop]# jps1290 DataNode1531 Jps1391 NodeManager 因为其余几个节点上面只部署了 DataNode 和 NodeManager，因此使用 jps 命令应该能够看到这几个进程。 Hadoop Web 端验证访问：http://192.168.0.131:50070 可以看到以下页面： 打开其中的 Live Node，可以看到目前存活的节点： Yarn Web 端验证 打开其中的 Active Node，可以看到目前存活的节点： 问题集群正常启动 50070 页面显示没有 Live NodeNameNode 和 DataNode 都正常启动，但是访问 50070 页面发现检测不到任何节点。查看 DataNode 的日志，发现如下内容：12342018-10-30 23:02:04,907 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: weilu131/192.168.0.131:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)2018-10-30 23:02:05,909 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: weilu131/192.168.0.131:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)2018-10-30 23:02:06,911 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: weilu131/192.168.0.131:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)2018-10-30 23:02:07,912 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: weilu131/192.168.0.131:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) 一种可能性是由于 NameNode 的防火墙开着，并且不允许访问 9000 端口，这样 DataNode 就没法向 NameNode 报告状态，将9000端口开启后，该问题马上解决了，验证了该猜想。 集群正常启动 Yarn 页面上看不到活动节点查看 NodeManager 节点上的 Yarn 启动日志，可以看到以下错误信息：123org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.net.NoRouteToHostException: No Route to Host from weilu132/192.168.0.132 to weilu131:8031 failed on socket timeout exception: java.net.NoRouteToHostException: 没有到主机的路由; For more details see: http://wiki.apache.org/hadoop/NoRouteToHost at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.serviceStart(NodeStatusUpdaterImpl.java:215)... 问题还是一样的问题，无法和 ResourceManager 节点进行通讯，yarn 使用的是 8031 端口，设置一下，然后重启集群。 参考[1] https://www.linode.com/docs/databases/hadoop/how-to-install-and-set-up-hadoop-cluster/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>集群</tag>
        <tag>hadoop</tag>
        <tag>hdfs</tag>
        <tag>yarn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 设置防火墙开放指定端口]]></title>
    <url>%2F2018%2F10%2F30%2FCentOS-7-%E8%AE%BE%E7%BD%AE%E9%98%B2%E7%81%AB%E5%A2%99%E5%BC%80%E6%94%BE%E6%8C%87%E5%AE%9A%E7%AB%AF%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[摘要记录一下 Linux 常用的关于防火墙端口操作的命令。 查看已打开的端口1netstat -anp 查看想开的端口是否已开1firewall-cmd --query-port=666/tcp 若此提示 FirewallD is not running表示为不可知的防火墙 需要查看状态并开启防火墙 查看防火墙状态1systemctl status firewalld running 状态即防火墙已经开启dead 状态即防火墙未开启 开启防火墙1systemctl start firewalld 没有任何提示即开启成功 或者： 1service firewalld start 关闭防火墙1systemctl stop firewalld centos7.3 上述方式可能无法开启，可以先1systemctl unmask firewalld.service 然后 1systemctl start firewalld.service 查看想开的端口是否已开1firewall-cmd --query-port=666/tcp 提示no表示未开 开永久端口号1firewall-cmd --add-port=666/tcp --permanent 提示 success 表示成功 重新载入配置1firewall-cmd --reload 比如添加规则之后，需要执行此命令 #移除端口 1firewall-cmd --permanent --remove-port=666/tcp]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>firewall</tag>
        <tag>防火墙</tag>
        <tag>port</tag>
        <tag>端口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 网络配置之初始配置、静态IP以及网桥]]></title>
    <url>%2F2018%2F10%2F27%2FCentOS-7-%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%E4%B9%8B%E5%88%9D%E5%A7%8B%E9%85%8D%E7%BD%AE%E3%80%81%E9%9D%99%E6%80%81IP%E4%BB%A5%E5%8F%8A%E7%BD%91%E6%A1%A5%2F</url>
    <content type="text"><![CDATA[摘要记录一下 CentOS 7 的网络配置，一开始是安装完的初始化配置，动态分配IP；然后改成了静态IP；之后又修改为配置网桥。 初始配置123456789101112131415TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=dhcpDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s31f6UUID=98eb9b94-878c-4430-8ce5-13471bb46997DEVICE=enp0s31f6ONBOOT=no 静态IP12345678910111213141516171819202122TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticDEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s31f6UUID=98eb9b94-878c-4430-8ce5-13471bb46997DEVICE=enp0s31f6ONBOOT=yesIPADDR=192.168.0.123GATEWAY=192.168.0.1NETMASK=255.255.255.0NM_CONTROLLED=noDNS1=8.8.8.8DNS2=8.8.4.4 网桥ifcfg-br0文件： 123456789TYPE=BridgeNAME=br0DEVICE=br0ONBOOT=yesBOOTPROTO=staticIPADDR=192.168.0.123GATEWAY=192.168.0.1NETMASK=255.255.255.0DNS1=192.168.0.1 ifcfg-enp0s31f6文件： 123456TYPE=EthernetBRIDGE=br0NAME=enp0s31f6UUID=99244a4d-8cac-4023-9a09-8e50c547cd3aDEVICE=enp0s31f6ONBOOT=yes]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Bridge</tag>
        <tag>Network</tag>
        <tag>Static IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用MAT对Java程序进行内存分析之小试牛刀]]></title>
    <url>%2F2018%2F10%2F26%2F%E4%BD%BF%E7%94%A8MAT%E5%AF%B9Java%E7%A8%8B%E5%BA%8F%E8%BF%9B%E8%A1%8C%E5%86%85%E5%AD%98%E5%88%86%E6%9E%90%E4%B9%8B%E5%B0%8F%E8%AF%95%E7%89%9B%E5%88%80%2F</url>
    <content type="text"><![CDATA[摘要今天在写爬虫的时候发现数据抓到五十万左右时候，进程占用内存1.5G左右，程序完全卡死了。爬虫是用 WebMagic 框架写的，逻辑也不复杂，看不出什么问题；阅读了 WebMagic 的源码，看着也没有什么问题。就尝试通过内存分析的方式看看能否找出问题的原因。 启动程序，打开 %JAVA_HOME%/bin/jconsole.exe 工具，根据进程ID连接到运行中到Java程序 可以看到程序在执行过程中不断的占用内容，然后被GC回收内存。 程序初始化的时候读取了大概五十万条左右数据，所以一开始就占用了500M左右的内存，后续GC回收时，一般在500M左右徘徊 随着程序执行时间不断累积，可以看到内存占用越来越大了，GC回收后相对于初始化时，仍然有一百多M没有回收掉 运行了大概半个小时之后程序挂，在运行期间使用 jmap 工具保存了若干个时间节点下的内存快照。 12345678910111213C:\Java\jdk1.8.0_151\bin&gt;jmap -dump:format=b,file=D:/jvmdump/heap.bin 4344Dumping heap to D:\jvmdump\heap.bin ...Heap dump file createdC:\Java\jdk1.8.0_151\bin&gt;jmap -dump:format=b,file=D:/jvmdump/heap.bin 4344Dumping heap to D:\jvmdump\heap.bin ...File exists...C:\Java\jdk1.8.0_151\bin&gt;jmap -dump:format=b,file=D:/jvmdump/heap10.bin 4344Dumping heap to D:\jvmdump\heap10.bin ...Heap dump file created jmp 工具生成的是内存快照，因此每个文件的大小就是当前程序所使用内存的大小。 接下来使用 MAT（Memory Analyzer Tool）工具对内存进行分析。 MAT工具下载地址：http://www.eclipse.org/mat/downloads.php 对比查看不同时间段占用内存最大的对象情况。 通过这个对比分析，可以很明显看到内存占用最大的对象是数据库连接对象，结合我的程序，能够判断出来应该是使用的数据连接池有问题。更换一个连接池测试一下发现，确实是这个问题。这样，就未完成了一次简单的内存分析。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>MAT</tag>
        <tag>内存分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无法启动此程序，因为计算机中丢失 api-ms-win-crt-runtime-l1-1-0.dll]]></title>
    <url>%2F2018%2F10%2F26%2F%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E6%AD%A4%E7%A8%8B%E5%BA%8F%EF%BC%8C%E5%9B%A0%E4%B8%BA%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%AD%E4%B8%A2%E5%A4%B1-api-ms-win-crt-runtime-l1-1-0-dll%2F</url>
    <content type="text"><![CDATA[摘要我这边碰到的错误是在安装 MySQL Workbench 时出现的，这个文件是 Visual C++ Redistributable 下的一个文件。我按照其官网上标注的安装前提，安装了 Visual C++ 2015 Redistributable for Visual Studio 2015，重新安装 Workbench 仍然报该错误。 问题描述 解析其主要原因是在安装 Visual C++ 2015 Redistributable for Visual Studio 2015 时，系统中已经存在了某个版本了，并且文件 api-ms-win-crt-runtime-l1-1-0.dll 可能正在被某个程序所引用，在重新安装时，并没有将之前的 api-ms-win-crt-runtime-l1-1-0.dll 文件更新覆盖，从而导致没有真正解决该问题。 解决方法解决方法就是先找到该文件，将其删除，删除的过程中涉及到某个程序引用这个文件的，先将那个程序退出再删除。 在目录 C:\Windows\System32和C:\Windows\SysWOW64 下查找 api-ms-win-crt-runtime-l1-1-0.dll 文件，如果有，就将其删除。 然后重新安装 Visual C++ 2015 Redistributable for Visual Studio 2015，安装之后可能要重启，之后再安装 Workbench 即可正常运行。 对应到在安装其他程序过程中碰到该问题的情况，只需要按照相同的方法，安装对应版本的 Visual C++ 即可。]]></content>
      <categories>
        <category>Windows</category>
      </categories>
      <tags>
        <tag>Visual C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL启动错误：在本地计算机上的MySQL57服务启动后停止。某些服务在未由其他服务或程序使用时将自动停止]]></title>
    <url>%2F2018%2F10%2F26%2FMySQL%E5%90%AF%E5%8A%A8%E9%94%99%E8%AF%AF%EF%BC%9A%E5%9C%A8%E6%9C%AC%E5%9C%B0%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%B8%8A%E7%9A%84MySQL57%E6%9C%8D%E5%8A%A1%E5%90%AF%E5%8A%A8%E5%90%8E%E5%81%9C%E6%AD%A2%E3%80%82%E6%9F%90%E4%BA%9B%E6%9C%8D%E5%8A%A1%E5%9C%A8%E6%9C%AA%E7%94%B1%E5%85%B6%E4%BB%96%E6%9C%8D%E5%8A%A1%E6%88%96%E7%A8%8B%E5%BA%8F%E4%BD%BF%E7%94%A8%E6%97%B6%E5%B0%86%E8%87%AA%E5%8A%A8%E5%81%9C%E6%AD%A2%2F</url>
    <content type="text"><![CDATA[摘要在 MySQL 服务启动时，出现错误“在本地计算机上的MySQL57服务启动后停止。某些服务在未由其他服务或程序使用时将自动停止”，服务无法正常启动。 问题描述这个问题是在MySQL服务停掉之后，重新启动时出现的，情形如下： 问题原因和解决方法一开始是有点蒙的，后来想到刚刚停掉服务时，是为了修改配置文件 my.ini 的： 我在配置文件中配置了服务器的字符集，实际上应该写作utf8，多了一个横杠之后就会报错，去掉后重启就正常了。 这个配置文件错误，是导致这个问题的一个原因。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>my.ini</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Configure Maven Java Project in Idea on CentOS 7]]></title>
    <url>%2F2018%2F10%2F20%2FConfigure-Maven-Java-Project-in-Idea-on-CentOS-7%2F</url>
    <content type="text"><![CDATA[摘要介绍如何在 CentOS 7 环境下安装配置 JDK、Maven、Idea。以及如何创建一个可以运行的 Maven Java 项目。 卸载 OpenJDK使用命令查看当前系统的 jdk 版本：1234[root@localhost ~]# java -versionopenjdk version &quot;1.8.0_161&quot;OpenJDK Runtime Environment (build 1.8.0_161-b14)OpenJDK 64-Bit Server VM (build 25.161-b14, mixed mode) 确实是 openjdk，然后使用 yum 命令查看有多少个包：12345678[root@localhost ~]# yum list installed | grep javajava-1.7.0-openjdk.x86_64 1:1.7.0.171-2.6.13.2.el7 @anacondajava-1.7.0-openjdk-headless.x86_64 1:1.7.0.171-2.6.13.2.el7 @anacondajava-1.8.0-openjdk.x86_64 1:1.8.0.161-2.b14.el7 @anacondajava-1.8.0-openjdk-headless.x86_64 1:1.8.0.161-2.b14.el7 @anacondajavapackages-tools.noarch 3.4.1-11.el7 @anacondapython-javapackages.noarch 3.4.1-11.el7 @anacondatzdata-java.noarch 2018c-1.el7 @anaconda 可以看到，系统中有两个版本的 openjdk，我们先删除 1.7 的：1234567[root@localhost ~]# yum -y remove java-1.7.0-openjdk*...删除: java-1.7.0-openjdk.x86_64 1:1.7.0.171-2.6.13.2.el7 java-1.7.0-openjdk-headless.x86_64 1:1.7.0.171-2.6.13.2.el7 完毕！ 用同样的方法将 1.8 的也删除即可。12yum -y remove java-1.8.0-openjdk*... 安装 JDK下载 JDKhttp://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 这里下载源码版本的。 将包移动到 /usr/local/ 目录下，并解压：1tar -xvzf jdk-8u181-linux-x64.tar.gz 配置环境变量打开配置文件：1vim /etc/profile 在文件最后添加如下内容：123export JAVA_HOME=/usr/local/jdk1.8.0_181export CLASSPATH=.:$JAVA_HOME/jre/lib/rt.jar:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport PATH=$PATH:$JAVA_HOME/bin 效果如下： 保存后退出，然后重新加载配置文件：1source /etc/profile 测试1234[root@localhost local]# java -versionjava version &quot;1.8.0_181&quot;Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode) 安装 Idea下载：https://www.jetbrains.com/idea/download/#section=linux得到文件：ideaIC-2018.2.4.tar.gz 将该文件拷贝到 /usr/local 目录下，然后解压：1tar -zxvf ideaIC-2018.2.4.tar.gz 进入该目录，其中有一个 Install-Linux-tar.txt 文件，其中说明了如何安装。 进入该文件夹的 bin 目录中，执行：1./idea.sh 第一次执行时，会生成一些配置文件，以及要求你进行一些设置。 安装 Maven下载：1wget http://mirror.bit.edu.cn/apache/maven/maven-3/3.5.4/binaries/apache-maven-3.5.4-bin.tar.gz 将下载文件拷贝到 /usr/local/ 中然后解压：1tar -zxvf /usr/local/apache-maven-3.5.4-bin.tar.gz 创建 Java 项目在 IDEA 的顶部的菜单中 File -&gt; New -&gt; Project 打开如下界面，选择 Maven，设置好项目所使用的 SDK，勾选 Create from archetype，然后从下面选择一个 Maven 的QuickStart 项目模板。 填写项目的 Maven 相关信息： 这里注意，使用的 Maven Home 目录选择你所下载的 Maven 版本，同时将Maven的设置文件和仓库目录修改为你自定义的。 这里基本不用修改，如果需要可以更改项目名称和存放路径。 之后，项目的创建可能需要一些时间来下载和引入相关的模板文件。 完成之后，你会在信息框中看到 BUILD SUCCESS的字样，不过这个时候这个Java项目是没有办法直接运行的。需要点击右下角的方框中的 Enable Auto Import才行。 之后，项目就变成一个可执行的项目了，在 Main 函数所在的类中右击，会出现 Run &#39;App.main()&#39; 的选项。如果找不到这个右下角的小方块，那么在项目名称上右击，选择Maven -&gt; Reimport 可以达到相同的效果。]]></content>
      <categories>
        <category>Idea</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Maven</tag>
        <tag>Idea</tag>
        <tag>Run Main</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于 KickStart 无人值守的全命令行 KVM 虚拟机安装过程]]></title>
    <url>%2F2018%2F10%2F14%2F%E5%9F%BA%E4%BA%8E-KickStart-%E6%97%A0%E4%BA%BA%E5%80%BC%E5%AE%88%E7%9A%84%E5%85%A8%E5%91%BD%E4%BB%A4%E8%A1%8C-KVM-%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AE%89%E8%A3%85%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[摘要考虑到服务器没有配置界面，而需要在服务器上配置一些虚拟机，可以使用基于 KickStart 无人值守的方式，基于命令行来安装配置和使用虚拟机。安装完成的虚拟机直接可以使用SSH在局域网内连接。 配置主机环境libvirt-client.x86_64 0:3.9.0-14.el7_5.8 Libvirt 的客户端，主要功能是在宿主机关机时通知虚拟机正常关机，防止强制关机导致数据丢失。 qemu-kvm.x86_64 10:1.5.3-156.el7_5.5KVM在用户控件运行的程序 virt-manager.noarch 0:1.4.3-3.el7基于 libvirt 的可视化虚拟机管理工具 libvirt.x86_64 0:3.9.0-14.el7_5.8用于管理虚拟机的APi virt-viewer.x86_64 0:5.0-10.el7显示虚拟机控制台的console virt-top.x86_64 0:1.0.8-24.el7 查看虚拟机的资源使用情况，类似于top命令 qemu-img-1.5.3-156.el7_5.5.x86_64 virt-install-1.4.3-3.el7.noarch虚拟机安装工具 使用 HTTP 服务提供安装镜像1、安装 HTTP 服务通过运行下面命令安装 HTTP 服务：1yum install httpd 2、拷贝 ISO 镜像从网络下载或是从其他服务器将 CentOS 7 的二进制 DVD ISO 镜像拷贝到 HTTP 服务所在的主机上。这里拷贝到如下位置：1/home/packages/CentOS-7-x86_64-DVD-1804.iso 3、挂载镜像首先在 /mnt 目录下创建一个目录 /mnt/ctos7-install，这个目录名字随意。然后使用以下命令挂载镜像：1mount -o loop,ro -t iso9660 /home/packages/CentOS-7-x86_64-DVD-1804.iso /mnt/ctos7-install 4、拷贝安装包使用如下命令将安装镜像中的文件拷贝到 HTTP 服务的目录中：1cp -r /mnt/ctos7-install/ /var/www/html/ 拷贝完成之后，可以在 /var/www/html/ctos7-install 目录下看到以下内容：1234[root@weilu_125 ctos7-install]# lsCentOS_BuildTag GPL LiveOS RPM-GPG-KEY-CentOS-7EFI images Packages RPM-GPG-KEY-CentOS-Testing-7EULA isolinux repodata TRANS.TBL 这些内容就是镜像中的文件。 5、启动 HTTP 服务1systemctl start httpd.service 6、开放端口HTTP 服务默认使用的 80 端口一般防火墙是没有开放的，可以使用命令检查：1firewall-cmd --query-port=80/tcp 如果返回的是 no 则表示没有开启，使用如下命令开启：12345# 开启端口firewall-cmd --add-port=80/tcp --permanent# 重新载入配置firewall-cmd --reload 7、测试在浏览器中输入这台主机的 IP 地址加上路径：1http://192.168.0.125/ctos7-install/ 可以看到如下内容，说明HTTP服务配置成功。 编写 Kickstart 文件在 HTTP 服务目录下创建一个文件 /var/www/html/kickstart/ks.cfg，将如下内容填充到文件中：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586################################################################# Environment setup################################################################# url --url=&quot;http://192.168.0.125/kickstart/ks.cfg&quot;textcdromauth --enableshadow --passalgo=sha512keyboard --vckeymap=us --xlayouts=&apos;us&apos;lang en_US.UTF-8eula --agreedreboot################################################################# network configuration################################################################network --bootproto=static --ip=192.168.0.151 --gateway=192.168.0.1 --netmask=255.255.255.0 --noipv6 --device=eth0 --nameserver=192.168.0.1,8.8.8.8 --activatenetwork --hostname=weilu_151timezone Asia/Shanghai --isUtc################################################################# partitioning################################################################ignoredisk --only-use=vdabootloader --location=mbr --boot-drive=vdazerombrclearpart --none --initlabelautopart --type=lvm# part swap --asprimary --fstype=&quot;swap&quot; --size=1024# part /boot --fstype xfs --size=200# part pv.01 --size=1 --grow# volgroup rootvg01 pv.01# logvol / --fstype xfs --name=lv01 --vgname=rootvg01 --size=1 --grow############################################################################################ # User Accounts# Generate encrypted password: python -c &apos;import crypt; print(crypt.crypt(&quot;My Password&quot;))&apos;# Or openssl passwd -1 password############################################################################################rootpw king # user --groups=wheel --name=josepy --password=password --gecos=&quot;Mutai Josphat&quot;################################################################# SELinux and Firewalld#################################################################selinux --enforcing#selinux --permissiveselinux --disabled firewall --enabled --http --ssh --ftp --port=https:tcp --port=ipp:tcp# firewall --disabled ################################################################# Software Packages################################################################%packages --nobase --ignoremissing@core@basevim bash-completion%end 根据实际情况修改文件中的以下内容：键盘和语言设置12keyboard --vckeymap=us --xlayouts=&apos;us&apos;lang en_US.UTF-8 网络配置这里配置的是使用网桥连接，固定IP：1network --bootproto=static --ip=192.168.0.151 --gateway=192.168.0.1 --netmask=255.255.255.0 --noipv6 --device=eth0 --nameserver=192.168.0.1,8.8.8.8 --activate 主机名1network --hostname=weilu_151 时区1timezone Asia/Shanghai --isUtc 这里可以使用命令 timedatectl list-timezones 查看所有时区的列表。 root密码1rootpw king 在 rootpw 指令后面跟的就是root账号的密码。 创建虚拟机使用如下命令创建虚拟机，之后整个过程会自动进行，不需要交互操作：123456789101112virt-install \ --name centos7-3 \ --memory 2048 \ --vcpus 2 \ --disk path=/home/kvm3/centos7.0.qcow2,size=50 \ --location http://192.168.0.125/ctos7-install/ \ --os-variant centos7.0 \ --network bridge:br0 \ --graphics=none \ --console pty,target_type=serial \ -x &apos;console=ttyS0,115200n8 serial&apos; \ -x &quot;ks=http://192.168.0.125/kickstart/ks.cfg&quot; 执行该命令后，会自动安装配置虚拟机： 安装完成之后，命令行会自动连接登录到虚拟机中，输入用户名密码即可登录虚拟机。 管理虚拟机以命令行启动虚拟机以命令行启动虚拟机，并将该命令行连接到虚拟机中的命令行：1virsh start centos7-3 --console 如果期间出现以下错误：Active console session exists for this domain完整信息：12345[root@weilu_125 Pictures]# virsh start centos7-3 --consoleDomain centos7-3 startedConnected to domain centos7-3Escape character is ^]error: operation failed: Active console session exists for this domain 只需要重启虚拟机守护进程即可：1systemctl restart libvirtd.service 连接虚拟机对于已经启动的虚拟机，可以使用以下命令以命令行的方式连接虚拟机：1virsh console centos7-3 问题下面罗列一些在这个过程中可能碰到的问题。 WARNING KVM acceleration not available, using ‘qemu’这个问题的直观表现就是执行 virt-install 命令之后就卡住了，如下： 12345678910111213141516171819[root@weilu_123 centos7-1]# virt-install \&gt; --name centos7-1 \&gt; --memory 2048 \&gt; --vcpus 2 \&gt; --disk path=/home/kvms/centos7-1/centos7.0.qcow2,size=200 \&gt; --location http://192.168.0.123/ctos7-install/ \&gt; --os-variant centos7.0 \&gt; --network bridge:br0 \&gt; --graphics=none \&gt; --console pty,target_type=serial \&gt; -x &apos;console=ttyS0,115200n8 serial&apos; \&gt; -x &quot;ks=http://192.168.0.123/kickstart/ks.cfg&quot;开始安装......搜索文件 vmlinuz...... | 5.9 MB 00:00 搜索文件 initrd.img...... | 50 MB 00:00 正在分配 &apos;centos7.0.qcow2&apos; | 200 GB 00:00 连接到域 centos7-1换码符为 ^] 这个问题是BIOS的CPU虚拟化功能没有开启导致的。 只要进入 BIOS，将Intel Virtualization Technology开启即可，下面例子是华硕主板： 参考[1] RHEL and CentOS Kickstart on KVM Automated Installation With virt-install[2] CentOS 7 INSTALLING IN TEXT MODE[3] Use VNC mode to install CentOS 7[4] Kickstart Installation]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>KVM</tag>
        <tag>KickStart</tag>
        <tag>无人值守</tag>
        <tag>命令行安装虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DIY饭团的烘干机——全环绕立体暖风]]></title>
    <url>%2F2018%2F10%2F10%2FDIY%E9%A5%AD%E5%9B%A2%E7%9A%84%E7%83%98%E5%B9%B2%E6%9C%BA%E2%80%94%E2%80%94%E5%85%A8%E7%8E%AF%E7%BB%95%E7%AB%8B%E4%BD%93%E6%9A%96%E9%A3%8E%2F</url>
    <content type="text"><![CDATA[摘要日常给饭团洗澡后，吹干饭团的毛发实在是一个痛苦的过程，一个小吹风机两三个小时，实在伤不起，本打算买个烘干箱，但价格勉强可以接受的，箱子实在看不上，看得上的，价格实在是不美丽，因此打算自己DIY一个，先做个设计和预算再说。 饭团镇楼 设计草图整体草图 正面 左侧 右侧 背部 底部]]></content>
      <categories>
        <category>饭团的小日子</category>
      </categories>
      <tags>
        <tag>烘干机</tag>
        <tag>DIY</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7.4 配置SSH免密登录]]></title>
    <url>%2F2018%2F10%2F08%2FCentOS-7-4-%E9%85%8D%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[摘要目前局域网中参与配置的有三台机器，要配置这三台机器免密登录。 配置主机名配置主机名其实相当于在这三台机器上各自备份一个路由表，由IP到主机名之间的映射。下面以 177.11.12.115 为例，修改主机名：1vi /etc/hostname 将其中的内容修改为：1oolong116 然后增加映射：1vi /etc/hosts 在文件末尾添加以下内容：123177.11.12.113 oolong113177.11.12.115 oolong115177.11.12.115 oolong116 别忘了重启一下电脑，不然hostname不会生效1reboot 测试：12345[root@localhost local]# ping oolong115PING oolong115 (177.11.12.115) 56(84) bytes of data.64 bytes from oolong115 (177.11.12.115): icmp_seq=1 ttl=64 time=0.230 ms64 bytes from oolong115 (177.11.12.115): icmp_seq=2 ttl=64 time=0.121 ms64 bytes from oolong115 (177.11.12.115): icmp_seq=3 ttl=64 time=0.115 ms 这时，我们已经可以使用主机名 oolong115 来代替IP 177.11.12.115。同样的方法对另外两台机器设置，这样三台机器之间就可以通过主机名互相访问了。 生成密钥使用命令 ssh-keygen 生成密钥，在生成过程中会要求输入存放目录等内容，可以不输入，回车即可。12345678910111213141516171819202122[root@oolong113 ~]# ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Created directory &apos;/root/.ssh&apos;.Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:jpatt7hZb3V2sLjWm6Wvz3cLo2uFaOPGVpFCMBBsrvE root@oolong113The key&apos;s randomart image is:+---[RSA 2048]----+| .ooo. || o .. || o . . || . . . o . || + S o o. o || . E = + oo.+ .|| + B o..B ..|| . =.*..+ +=o|| =o+oo+. =*B|+----[SHA256]-----+ 进入目录 /root/.ssh/ 可以看到生成以下的两个文件：12-rw-------. 1 root root 1679 Sep 12 19:37 id_rsa-rw-r--r--. 1 root root 397 Sep 12 19:37 id_rsa.pub 如法炮制，给另外两台机器也生成密钥。 拷贝秘钥这里需要注意，要使用SSH的命令来拷贝秘钥，因为里面不能有其他字符，如果是通过文件编辑工具打开拷贝，可能会产生多余的换行符等内容，可能产生的问题是A能免密登录B，但B不能免密登录A。比如当前在 oolong116 这台机器上，进入 /root/.ssh/ 目录，执行下面命令，将秘钥拷贝到 113 上：1ssh-copy-id -i id_rsa.pub root@oolong113 拷贝后检查 113 的 /root/.ssh 目录下有没有 authorized_keys 文件。 依次的拷贝到其他机器上，同样的将其他机器上的秘钥要拷贝到176 上 测试123[root@oolong116 .ssh]# ssh oolong115Last login: Thu Sep 13 05:32:43 2018 from 177.11.12.115[root@oolong115 ~]# 此时，就登录上了 115 了。 批量拷贝秘钥到远程主机安装 sshpass1yum install sshpass 编写脚本进入秘钥所在目录 /root/.ssh，创建以下两个文件： remote-hosts 1234weilu131weilu132weilu135weilu151 copyscript.sh 1234for host in $(cat remote-hosts)do sshpass -p &apos;1234&apos; ssh-copy-id -o StrictHostKeyChecking=no root@$&#123;host&#125;done 其中参数 -p &#39;1234&#39; 表示密码，这里几台机器使用的相同密码，根据情况修改。 保存后需要修改拷贝脚本的权限： 1chmod 777 copyscript.sh 然后执行拷贝： 1./copyscript.sh]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>SSH</tag>
        <tag>免密登录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Config virtual network connection with bridge mode]]></title>
    <url>%2F2018%2F10%2F06%2FConfig-virtual-network-connection-with-bridge-mode%2F</url>
    <content type="text"><![CDATA[摘要在宿主机为 CentOS 7 的环境中通过KVM配置CentOS 7 虚拟机，使用 Bridge 模式配置网络连接，使虚拟机与宿主机处于同一网络环境中。 Create BrdigeInstall ModuleCentOS 7 在系统启动时默认加载了桥接模块。使用下面的命令可以判断这个模块是否加载。1234567891011121314[root@weilu_125 kvms]# modinfo bridgefilename: /lib/modules/3.10.0-862.el7.x86_64/kernel/net/bridge/bridge.ko.xzalias: rtnl-link-bridgeversion: 2.3license: GPLretpoline: Yrhelversion: 7.5srcversion: A0B6183F98024E85CD123C5depends: stp,llcintree: Yvermagic: 3.10.0-862.el7.x86_64 SMP mod_unload modversions signer: CentOS Linux kernel signing keysig_key: 3A:F3:CE:8A:74:69:6E:F1:BD:0F:37:E5:52:62:7B:71:09:E3:2B:96sig_hashalgo: sha256 如果这个模块没有加载，可以使用下面的命令进行加载。1modprobe --first-time bridge 安装桥接工具：1yum install bridge-utils -y create a network bridge要创建一个网桥，可以在 /etc/sysconfig/network-scripts/ 目录下创建一个名为 ifcfg-brN 的文件，将其中的 N 替换为数字，比如“0”。1vi /etc/sysconfig/network-scripts/ifcfg-br0 将下面的内容放置到这个文件中，对应你的主机环境修改相应配置：123456789TYPE=BridgeNAME=br0DEVICE=br0ONBOOT=yesBOOTPROTO=staticIPADDR=192.168.0.125GATEWAY=192.168.0.1NETMASK=255.255.255.0DNS1=192.168.0.1 创建完网桥之后，需要将网络配置接口挂载到这个网桥上。比如，我这边使用的本机已有的适配器 eno1： 1vi /etc/sysconfig/network-scripts/ifcfg-eno1 将以下内容放置到该配置文件中：123456TYPE=EthernetBRIDGE=br0NAME=eno1UUID=99244a4d-8cac-4023-9a09-8e50c547cd3aDEVICE=eno1ONBOOT=yes 使用如下命令重启网络服务：1systemctl restart network 查看网络配置：1234567891011121314151617181920212223242526[root@weilu_125 network-scripts]# ifconfigbr0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.0.125 netmask 255.255.255.0 broadcast 192.168.0.255 inet6 fe80::329c:23ff:fee1:f0d9 prefixlen 64 scopeid 0x20&lt;link&gt; ether 30:9c:23:e1:f0:d9 txqueuelen 1000 (Ethernet) RX packets 3503 bytes 599352 (585.3 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 1932 bytes 1004140 (980.6 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0eno1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 ether 30:9c:23:e1:f0:d9 txqueuelen 1000 (Ethernet) RX packets 510891 bytes 330776497 (315.4 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 285983 bytes 46787578 (44.6 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 device interrupt 16 memory 0xa1100000-a1120000 lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 13058 bytes 1819263 (1.7 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 13058 bytes 1819263 (1.7 MiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 Config Guest Connection在虚拟机的设置中，将 Network source 设置为通过刚刚设置的网桥进行连接。 然后修改虚拟机的网络配置：123456789TYPE=EthernetONBOOT=yesDEVICE=eth0BOOTPROTO=staticIPADDR=192.168.0.161NETMASK=255.255.255.0BROADCAST=192.168.0.255GATEWAY=192.168.0.1DNS1=192.168.0.1 使用如下命令重启网络服务：1systemctl restart network 查看网络配置：123456789101112131415161718[root@localhost ~]# ifconfigeth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.0.161 netmask 255.255.255.0 broadcast 192.168.0.255 inet6 fe80::5054:ff:fe24:c503 prefixlen 64 scopeid 0x20&lt;link&gt; ether 52:54:00:24:c5:03 txqueuelen 1000 (Ethernet) RX packets 212 bytes 183776 (179.4 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 186 bytes 18719 (18.2 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10&lt;host&gt; loop txqueuelen 1000 (Local Loopback) RX packets 68 bytes 5912 (5.7 KiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 68 bytes 5912 (5.7 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 testFrom Host123456789[root@weilu_125 network-scripts]# ping 192.168.0.161 -c 3PING 192.168.0.161 (192.168.0.161) 56(84) bytes of data.64 bytes from 192.168.0.161: icmp_seq=1 ttl=64 time=0.386 ms64 bytes from 192.168.0.161: icmp_seq=2 ttl=64 time=0.327 ms64 bytes from 192.168.0.161: icmp_seq=3 ttl=64 time=0.320 ms--- 192.168.0.161 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 2000msrtt min/avg/max/mdev = 0.320/0.344/0.386/0.033 ms From Guest123456789[root@localhost ~]# ping 192.168.0.125 -c 3PING 192.168.0.125 (192.168.0.125) 56(84) bytes of data.64 bytes from 192.168.0.125: icmp_seq=1 ttl=64 time=0.101 ms64 bytes from 192.168.0.125: icmp_seq=2 ttl=64 time=0.260 ms64 bytes from 192.168.0.125: icmp_seq=3 ttl=64 time=0.252 ms--- 192.168.0.125 ping statistics ---3 packets transmitted, 3 received, 0% packet loss, time 1999msrtt min/avg/max/mdev = 0.101/0.204/0.260/0.074 ms]]></content>
      <categories>
        <category>KVM</category>
      </categories>
      <tags>
        <tag>Bridge</tag>
        <tag>KVM</tag>
        <tag>Network Connection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器基于OVS跨主机网络连接]]></title>
    <url>%2F2018%2F10%2F04%2FDocker%E5%AE%B9%E5%99%A8%E5%9F%BA%E4%BA%8EOVS%E8%B7%A8%E4%B8%BB%E6%9C%BA%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[摘要利用OVS技术将位于不同物理主机中的docker容器的网络连通，使之能够相互访问。 前置条件关闭 SELINUX在配置文件中：1vi /etc/selinux/config 将其中的内容 SELINUX=enforcing 修改：1SELINUX=disabled 然后重启 安装 open-vswitch参考【安装配置 Open-vSwitch-2.5.5】 启动 open-vSiwtch：1ovs-ctl start 配置 Docker 网桥IP修改守护进程的配置文件：1vi /etc/docker/daemon.json 在其中添加网桥的IP设置：1&quot;bip&quot;: &quot;172.17.1.1/24&quot; 重启docker：12systemctl stop dockersystemctl start docker 查看网桥IP：1234567891011[root@localhost ~]# ifconfigdocker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 172.17.1.1 netmask 255.255.255.0 broadcast 172.17.1.255 ether 02:42:c7:7b:7d:28 txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0eno1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500... 确实变成了我们所设定的IP。 配置 Open-vSwitch创建网桥1ovs-vsctl add-br br0 查看网桥12[root@localhost docker-data]# ovs-vsctl list-brbr0 创建端口1ovs-vsctl add-port br0 gre0 -- set interface gre0 type=gre options:remote_ip=192.168.0.125 查看端口12[root@localhost ~]# ovs-vsctl list-ports br0gre0 查看详细配置信息1234567891011[root@localhost docker-data]# ovs-vsctl show 840c2123-021b-4137-b1d5-8b0963c9e6ac Bridge &quot;br0&quot; Port &quot;gre0&quot; Interface &quot;gre0&quot; type: gre options: &#123;remote_ip=&quot;192.168.0.125&quot;&#125; Port &quot;br0&quot; Interface &quot;br0&quot; type: internal ovs_version: &quot;2.5.5&quot; 连接 br0 和 docker0查看往前目前的连接情况：1234[root@localhost docker-data]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.024205ee36a6 novirbr0 8000.525400249fb4 yes virbr0-nic 可以注意到，此时的 docker0 是没有接口连接的，我们将其与 br0 连接起来：1brctl addif docker0 br0 此时再查看时，会发现其与接口br0连接了：1234[root@localhost ~]# brctl showbridge name bridge id STP enabled interfacesdocker0 8000.0242c77b7d28 no br0virbr0 8000.52540071bdcb yes virbr0-nic 挂载 docker0 和 br0查看这两个网络连接的状态：1234567891011[root@localhost ~]# ip link show...2: eno1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000 link/ether 30:9c:23:e1:f0:d9 brd ff:ff:ff:ff:ff:ff...6: br0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop master docker0 state DOWN mode DEFAULT group default qlen 1000 link/ether a2:37:3f:c1:46:4e brd ff:ff:ff:ff:ff:ff...10: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default link/ether 02:42:c7:7b:7d:28 brd ff:ff:ff:ff:ff:ff... 其中还有很多个其他连接的情况，这里省略不看，主要关注 br0 和 docker0，留着 eno1 主要是为了对比。eno1 是本机物理网卡的连接，可以看到其状态 state UP，我们接下来要将 br0 和 docker0 也修改为 UP。1ip link set dev br0 up 在将 br0 改为 UP 时，docker0 状态改为 up，而br0 为 UNKNOW：123456: br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master docker0 state UNKNOWN mode DEFAULT group default qlen 1000 link/ether a2:37:3f:c1:46:4e brd ff:ff:ff:ff:ff:ff 10: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether 02:42:c7:7b:7d:28 brd ff:ff:ff:ff:ff:ff 添加路由查看当前路由规则：12345[root@localhost ~]# ip route listdefault via 192.168.0.1 dev eno1 proto static metric 100 172.17.1.0/24 dev docker0 proto kernel scope link src 172.17.1.2 192.168.0.0/24 dev eno1 proto kernel scope link src 192.168.0.125 metric 100 192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1 添加路由：12ip route add 172.17.0.0/16 dev docker0route add -net 172.17.0.0/16 gw 192.168.0.125 检查确认：12345678[root@localhost ~]# ip route listdefault via 192.168.0.1 dev enp0s31f6 169.254.0.0/16 dev enp0s31f6 scope link metric 1002 172.17.0.0/16 via 192.168.0.125 dev enp0s31f6 172.17.0.0/16 dev docker0 scope link 172.17.1.0/24 dev docker0 proto kernel scope link src 172.17.1.1 192.168.0.0/24 dev enp0s31f6 proto kernel scope link src 192.168.0.123 192.168.122.0/24 dev virbr0 proto kernel scope link src 192.168.122.1 网络结构]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>容器</tag>
        <tag>网络连接</tag>
        <tag>跨主机</tag>
        <tag>open-vswitch</tag>
        <tag>桥接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加载nbd模块失败 modprobe nbd Module nbd not found]]></title>
    <url>%2F2018%2F09%2F30%2F%E5%8A%A0%E8%BD%BDnbd%E6%A8%A1%E5%9D%97%E5%A4%B1%E8%B4%A5-modprobe-nbd-Module-nbd-not-found%2F</url>
    <content type="text"><![CDATA[摘要解决加载 nbd 模块时报错的问题，modprobe nbd Module nbd not found。 前置安装 elfutils-libelf-devel这个模块后面在编译内核时会使用。先给装上。1yum install elfutils-libelf-devel 下载查看操作系统版本和内核版本12345[root@weilu_125 vctos7-1]# cat /etc/redhat-releaseCentOS Linux release 7.5.1804 (Core) [root@weilu_125 vctos7-1]# uname -r3.10.0-862.el7.x86_64 查看 kernel-devel 和 kernel-headers 包，实际上已经安装了：1234567[root@weilu_125 vctos7-1]# yum list installed | grep kernelabrt-addon-kerneloops.x86_64 2.1.11-50.el7.centos @anacondakernel.x86_64 3.10.0-862.el7 @anacondakernel-devel.x86_64 3.10.0-862.el7 @anacondakernel-headers.x86_64 3.10.0-862.el7 @anacondakernel-tools.x86_64 3.10.0-862.el7 @anacondakernel-tools-libs.x86_64 3.10.0-862.el7 @anaconda 根据操作系统和内核版本找到对应的源码，下载：1wget http://vault.centos.org/7.5.1804/os/Source/SPackages/kernel-3.10.0-862.el7.src.rpm 使用命令安装这个包：1rpm -ihv kernel-3.10.0-862.el7.src.rpm 这个包安装后，默认会在 /root/rpmbuild 目录下，同时还会在 /usr/src/kernel 下面生成一个目录，后面会用到。 解压其中的一个包：1tar Jxvf /root/rpmbuild/SOURCES/linux-3.10.0-862.el7.tar.xz -C /usr/src/kernels/ 完成上述步骤之后，可以看到如下两个文件夹：12[root@weilu_125 kernels]# ls3.10.0-862.el7.x86_64 linux-3.10.0-862.el7 备份内核这里首先将内核移动到后缀增加了“-old”的目录下，然后将我们刚刚解压出来的内核目录拷贝过去，并进入内核目录1234[root@weilu_125 kernels]# mv $(uname -r) $(uname -r)-old[root@weilu_125 kernels]# mv linux-3.10.0-862.el7 $(uname -r)[root@weilu_125 kernels]# cd $(uname -r)[root@weilu_125 3.10.0-862.el7.x86_64]# 然后在这个目录下依次执行以下命令：123456make mrpropercp ../$(uname -r)-old/Module.symvers ./cp /boot/config-$(uname -r) ./.configmake oldconfigmake preparemake scripts 执行到这里暂停一下，修改文件:1/usr/src/kernels/3.10.0-862.el7.x86_64/drivers/block/nbd.c 修改如下配置：12// sreq.cmd_type = REQ_TYPE_SPECIAL;sreq.cmd_type = 7; 将这个变量之间设置为7即可，然后继续执行以下命令123make CONFIG_BLK_DEV_NBD=m M=drivers/blockcp drivers/block/nbd.ko /lib/modules/$(uname -r)/kernel/drivers/block/depmod -a 测试12345678910111213[root@weilu_125 block]# modinfo nbdfilename: /lib/modules/3.10.0-862.el7.x86_64/kernel/drivers/block/nbd.kolicense: GPLdescription: Network Block Deviceretpoline: Yrhelversion: 7.5srcversion: EDE909A294AC5FE08E81957depends: vermagic: 3.10.0 SMP mod_unload modversions parm: nbds_max:number of network block devices to initialize (default: 16) (int)parm: max_part:number of partitions per device (default: 0) (int)parm: debugflags:flags for controlling debug output (int)[root@weilu_125 block]#]]></content>
      <categories>
        <category>kernel</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>nbd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Github结合Hexo搭建个人博客]]></title>
    <url>%2F2018%2F09%2F29%2F%E5%9F%BA%E4%BA%8EGithub%E7%BB%93%E5%90%88Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[摘要本文介绍如何使用Hexo构建自己的个人博客，同时利用 GitHub 的仓库将内容发布到公网上。 部署与配置GitHub配置申请GitHub账号，并且创建名为 weilu2.github.io 的仓库。 安装 NodeJs安装 Git从官网下载安装 git. 配置免密提交打开 Git Bash，进入用于存放博客的根目录，比如 d:\blog：1234weilu@weilu-PC MINGW64 ~$ cd /d/Blog/weilu@weilu-PC MINGW64 /d/Blog 配置 Git 用户名和邮箱12345weilu@weilu-PC MINGW64 /d/Bloggit config --global user.name &quot;weilu2&quot;weilu@weilu-PC MINGW64 /d/Bloggit config --global user.email &quot;weilu0324@163.com&quot; 生成密钥1$ ssh-keygen -t rsa -C &quot;weilu0324@163.com&quot; 生成的密钥存储路径一般在 C:\Users\weilu\.ssh 使用 ssh-agent 管理私钥启动 ssh-agent123weilu@weilu-PC MINGW64 ~/.ssh$ eval &quot;$(ssh-agent -s)&quot;Agent pid 11508 将生成的密钥添加到 ssh-agent123weilu@weilu-PC MINGW64 ~/.ssh$ ssh-add id_rsaIdentity added: id_rsa (id_rsa) 将公钥添加到GitHub中在 GitHub 个人的设置中，添加 SSH-KEY。 验证123$ ssh -T git@github.com...Hi weilu2! You&apos;ve successfully authenticated, but GitHub does not provide shell access. 配置 Hexo安装使用命令12345678C:\Users\weilu&gt;npm install -g hexo-cliC:\Users\weilu\AppData\Roaming\npm\hexo -&gt; C:\Users\weilu\AppData\Roaming\npm\node_modules\hexo-cli\bin\hexonpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\hexo-cli\node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)+ hexo-cli@1.1.0added 225 packages from 414 contributors in 31.47s 创建博客在D盘创建一个目录 D:\Blog，进入该目录，使用如下命令初始化博客：12345678910111213141516171819202122232425262728293031323334D:\Blog&gt;hexo initINFO Cloning hexo-starter to D:\BlogCloning into &apos;D:\Blog&apos;...remote: Enumerating objects: 1, done.remote: Counting objects: 100% (1/1), done.remote: Total 68 (delta 0), reused 0 (delta 0), pack-reused 67Unpacking objects: 100% (68/68), done.Submodule &apos;themes/landscape&apos; (https://github.com/hexojs/hexo-theme-landscape.git) registered for path &apos;themes/landscape&apos;Cloning into &apos;D:/Blog/themes/landscape&apos;...remote: Enumerating objects: 6, done.remote: Counting objects: 100% (6/6), done.remote: Compressing objects: 100% (6/6), done.remote: Total 838 (delta 1), reused 3 (delta 0), pack-reused 832Receiving objects: 100% (838/838), 2.55 MiB | 25.00 KiB/s, done.Resolving deltas: 100% (441/441), done.Submodule path &apos;themes/landscape&apos;: checked out &apos;73a23c51f8487cfcd7c6deec96ccc7543960d350&apos;INFO Install dependenciesnpm WARN deprecated titlecase@1.1.2: no longer maintainednpm WARN deprecated postinstall-build@5.0.3: postinstall-build&apos;s behavior is now built into npm! You should migrate offof postinstall-build and use the new `prepare` lifecycle script with npm 5.0.0 or greater.&gt; nunjucks@3.1.3 postinstall D:\Blog\node_modules\nunjucks&gt; node postinstall-build.js srcnpm notice created a lockfile as package-lock.json. You should commit this file.npm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)added 420 packages from 484 contributors and audited 4704 packages in 15.704sfound 0 vulnerabilitiesINFO Start blogging with Hexo! 初始化的过程是从 hexo 仓库下载博客的目录结构和文件，根据网速，需要一定时间。 安装依赖模块：1234567D:\Blog&gt;npm installnpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;any&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)audited 4704 packages in 2.15sfound 0 vulnerabilities 生成静态页面1234567891011121314151617181920212223242526272829303132D:\Blog&gt;hexo gINFO Start processingINFO Files loaded in 112 msINFO Generated: index.htmlINFO Generated: archives/index.htmlINFO Generated: archives/2018/09/index.htmlINFO Generated: fancybox/blank.gifINFO Generated: fancybox/jquery.fancybox.cssINFO Generated: fancybox/fancybox_loading@2x.gifINFO Generated: archives/2018/index.htmlINFO Generated: fancybox/fancybox_sprite.pngINFO Generated: fancybox/fancybox_sprite@2x.pngINFO Generated: fancybox/fancybox_loading.gifINFO Generated: fancybox/fancybox_overlay.pngINFO Generated: fancybox/helpers/fancybox_buttons.pngINFO Generated: js/script.jsINFO Generated: fancybox/jquery.fancybox.pack.jsINFO Generated: css/fonts/FontAwesome.otfINFO Generated: css/fonts/fontawesome-webfont.eotINFO Generated: fancybox/helpers/jquery.fancybox-buttons.jsINFO Generated: css/fonts/fontawesome-webfont.woffINFO Generated: css/style.cssINFO Generated: fancybox/helpers/jquery.fancybox-thumbs.cssINFO Generated: fancybox/helpers/jquery.fancybox-thumbs.jsINFO Generated: fancybox/helpers/jquery.fancybox-buttons.cssINFO Generated: fancybox/helpers/jquery.fancybox-media.jsINFO Generated: css/fonts/fontawesome-webfont.ttfINFO Generated: 2018/09/28/hello-world/index.htmlINFO Generated: css/fonts/fontawesome-webfont.svgINFO Generated: css/images/banner.jpgINFO Generated: fancybox/jquery.fancybox.jsINFO 28 files generated in 288 ms 启动服务器123D:\Blog&gt;hexo sINFO Start processingINFO Hexo is running at http://localhost:4000 . Press Ctrl+C to stop. 能够正常启动服务器，并在浏览器中访问，说明 Hexo 配置成功，接下来要做的事情就是讲生成的静态页面提交到 Github上即可。 提交 Hexo 到 GitHub修改 _config.yml 文件，在最后增加如下内容：1234deploy: type: git repository: git@github.com:weilu2/weilu2.github.io.git branch: master 安装 Hexo 插件：12345678910D:\Blog&gt;npm install hexo-deployer-git --savenpm WARN deprecated swig@1.4.2: This package is no longer maintainednpm WARN optional SKIPPING OPTIONAL DEPENDENCY: fsevents@1.2.4 (node_modules\fsevents):npm WARN notsup SKIPPING OPTIONAL DEPENDENCY: Unsupported platform for fsevents@1.2.4: wanted &#123;&quot;os&quot;:&quot;darwin&quot;,&quot;arch&quot;:&quot;&quot;&#125; (current: &#123;&quot;os&quot;:&quot;win32&quot;,&quot;arch&quot;:&quot;x64&quot;&#125;)+ hexo-deployer-git@0.3.1added 31 packages from 36 contributors and audited 5874 packages in 7.482sfound 1 low severity vulnerability run `npm audit fix` to fix them, or `npm audit` for details 再次运行生成命令，就会自动生成静态文件，并部署到 git上了：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778D:\Blog&gt; hexo d -gINFO Start processingINFO Files loaded in 62 msINFO 0 files generated in 78 msINFO Deploying: gitINFO Setting up Git deployment...Initialized empty Git repository in D:/Blog/.deploy_git/.git/[master (root-commit) 67b0210] First commit 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 placeholderINFO Clearing .deploy_git folder...INFO Copying files from public folder...INFO Copying files from extend dirs...warning: LF will be replaced by CRLF in 2018/09/28/hello-world/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in archives/2018/09/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in archives/2018/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in archives/index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in css/style.css.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/helpers/jquery.fancybox-buttons.css.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/helpers/jquery.fancybox-buttons.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/helpers/jquery.fancybox-media.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/helpers/jquery.fancybox-thumbs.css.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/helpers/jquery.fancybox-thumbs.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/jquery.fancybox.css.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/jquery.fancybox.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in fancybox/jquery.fancybox.pack.js.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in index.html.The file will have its original line endings in your working directorywarning: LF will be replaced by CRLF in js/script.js.The file will have its original line endings in your working directory[master 9321f7e] Site updated: 2018-09-29 00:08:21 29 files changed, 5777 insertions(+) create mode 100644 2018/09/28/hello-world/index.html create mode 100644 archives/2018/09/index.html create mode 100644 archives/2018/index.html create mode 100644 archives/index.html create mode 100644 css/fonts/FontAwesome.otf create mode 100644 css/fonts/fontawesome-webfont.eot create mode 100644 css/fonts/fontawesome-webfont.svg create mode 100644 css/fonts/fontawesome-webfont.ttf create mode 100644 css/fonts/fontawesome-webfont.woff create mode 100644 css/images/banner.jpg create mode 100644 css/style.css create mode 100644 fancybox/blank.gif create mode 100644 fancybox/fancybox_loading.gif create mode 100644 fancybox/fancybox_loading@2x.gif create mode 100644 fancybox/fancybox_overlay.png create mode 100644 fancybox/fancybox_sprite.png create mode 100644 fancybox/fancybox_sprite@2x.png create mode 100644 fancybox/helpers/fancybox_buttons.png create mode 100644 fancybox/helpers/jquery.fancybox-buttons.css create mode 100644 fancybox/helpers/jquery.fancybox-buttons.js create mode 100644 fancybox/helpers/jquery.fancybox-media.js create mode 100644 fancybox/helpers/jquery.fancybox-thumbs.css create mode 100644 fancybox/helpers/jquery.fancybox-thumbs.js create mode 100644 fancybox/jquery.fancybox.css create mode 100644 fancybox/jquery.fancybox.js create mode 100644 fancybox/jquery.fancybox.pack.js create mode 100644 index.html create mode 100644 js/script.js delete mode 100644 placeholderBranch &apos;master&apos; set up to track remote branch &apos;master&apos; from &apos;git@github.com:weilu2/weilu2.github.io.git&apos;.To github.com:weilu2/weilu2.github.io.git + c049402...9321f7e HEAD -&gt; master (forced update)INFO Deploy done: git 直接访问 写作创建分类页面12D:\Blog&gt;hexo new page categoriesINFO Created: D:\Blog\source\categories\index.md 编辑这个页面，增加 type ：12345---title: categoriesdate: 2018-09-29 09:10:18type: "categories"--- 修改主题下的 _config.yml 文件，在 menu 中增加分类导航：12345menu: Home: / categories: /categories Archives: /archivesrss: /atom.xml 创建标签页面12D:\Blog&gt;hexo new page tagsINFO Created: D:\Blog\source\tags\index.md 编辑页面，增加类型：12345---title: tagsdate: 2018-09-29 10:12:20type: &quot;tags&quot;--- 在主题的配置文件中，增加标签的链接：123456menu: Home: / categories: /categories tags: /tags Archives: /archivesrss: /atom.xml 创建内容页面12D:\Blog&gt;hexo new post &quot;基于Github结合Hexo搭建个人博客&quot;INFO Created: D:\Blog\source\_posts\2018-09-29-基于Github结合Hexo搭建个人博客.md]]></content>
      <categories>
        <category>乱七八糟</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>Hexo</tag>
        <tag>Blog</tag>
        <tag>Nodejs</tag>
      </tags>
  </entry>
</search>
