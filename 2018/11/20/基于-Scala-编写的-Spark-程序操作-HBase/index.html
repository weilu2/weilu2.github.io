<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>基于 Scala 编写的 Spark 程序操作 HBase | 全宇宙尖端技术研究基地</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="摘要使用 Scala 编写 Spark 程序操作 HBase 中的数据。">
<meta name="keywords" content="HBase,Scala,Spark,Scala操作HBase">
<meta property="og:type" content="article">
<meta property="og:title" content="基于 Scala 编写的 Spark 程序操作 HBase">
<meta property="og:url" content="http://yoursite.com/2018/11/20/基于-Scala-编写的-Spark-程序操作-HBase/index.html">
<meta property="og:site_name" content="全宇宙尖端技术研究基地">
<meta property="og:description" content="摘要使用 Scala 编写 Spark 程序操作 HBase 中的数据。">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-11-20T03:06:03.214Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="基于 Scala 编写的 Spark 程序操作 HBase">
<meta name="twitter:description" content="摘要使用 Scala 编写 Spark 程序操作 HBase 中的数据。">
  
    <link rel="alternate" href="/atom.xml" title="全宇宙尖端技术研究基地" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">全宇宙尖端技术研究基地</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">专门研究各种全宇宙高精尖技术，比如“各种版本的Hello World怎么写”、“各种版本操作系统如何开机关机”、“如何喂养一只卖不出去的布偶猫”等...</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-基于-Scala-编写的-Spark-程序操作-HBase" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/20/基于-Scala-编写的-Spark-程序操作-HBase/" class="article-date">
  <time datetime="2018-11-20T03:03:37.000Z" itemprop="datePublished">2018-11-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Scala/">Scala</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      基于 Scala 编写的 Spark 程序操作 HBase
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>使用 Scala 编写 Spark 程序操作 HBase 中的数据。<br><a id="more"></a></p>
<h1 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h1><h2 id="集群环境"><a href="#集群环境" class="headerlink" title="集群环境"></a>集群环境</h2><p><strong>Hadoop环境</strong><br>参考：<a href="https://weilu2.github.io/2018/10/30/Hadoop%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E6%96%B9%E6%A1%88/" target="_blank" rel="noopener">Hadoop 集群部署方案</a></p>
<p><strong>Zookeeper&amp;HBase环境</strong><br>参考：<a href="https://weilu2.github.io/2018/11/05/%E5%9F%BA%E4%BA%8E-Hadoop-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2-ZooKeeper-%E5%92%8C-HBase-%E9%9B%86%E7%BE%A4/" target="_blank" rel="noopener">基于 Hadoop 集群部署 ZooKeeper 和 HBase 集群</a></p>
<p><strong>Spark环境</strong><br>参考：<a href="https://weilu2.github.io/2018/11/15/%E5%9F%BA%E4%BA%8E-Hadoop-%E5%92%8C-Yarn-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2-Spark-%E9%9B%86%E7%BE%A4/" target="_blank" rel="noopener">基于 Hadoop 和 Yarn 集群部署 Spark 集群</a></p>
<h2 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a>运行环境</h2><p>整个过程是在本地 Idea 中使用 Scala 编写 Spark 程序，使用 SBT 打包后，通过 <code>spark-submit</code> 提交到 Yarn 中运行。</p>
<p>之前在部署 Spark 环境时，有设置过一个变量：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.yarn.jars		hdfs://weilu131:9000/spark_jars/*</span><br></pre></td></tr></table></figure></p>
<p>并且已经将 Spark 的相关 JAR 包上传到 HDFS 上的这个目录中。这是 Spark 程序的运行环境。由于需要连接 HBase，因此程序的运行环境还需要有 HBase 相关的 JAR 包。</p>
<p>将 HBase 根目录下的以下 JAR 包上传到 HDFS 的这个目录中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/hbase*.jar /spark_jars</span><br><span class="line"></span><br><span class="line">/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/guava-12.0.1.jar /spark_jars</span><br><span class="line"></span><br><span class="line">/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/htrace-core-3.2.0-incubating.jar /spark_jars</span><br><span class="line"></span><br><span class="line">/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/protobuf-java-2.5.0.jar /spark_jars</span><br><span class="line"></span><br><span class="line">/usr/local/hadoop-2.6.0-cdh5.15.0/bin/hdfs dfs -put /usr/local/hbase-1.2.0-cdh5.15.0/lib/metrics-core-2.2.0.jar /spark_jars</span><br></pre></td></tr></table></figure></p>
<p>这里需要注意，由于使用的 HBase 和 Spark 各自有对一些 JAR 包的不同版本的依赖，因此需要都上传上去，如果没有可能会报找不到类的错误。在末尾会有相关示例。</p>
<h1 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h1><p>进入 HBase shell 环境：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/hbase-1.2.0-cdh5.15.0/bin/hbase shell</span><br></pre></td></tr></table></figure></p>
<h2 id="建表"><a href="#建表" class="headerlink" title="建表"></a>建表</h2><p>创建一张名为 <code>student</code> 的表，包含一个 <code>info</code> 列族。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create &apos;student&apos;,&apos;info&apos;</span><br></pre></td></tr></table></figure></p>
<h2 id="添加数据"><a href="#添加数据" class="headerlink" title="添加数据"></a>添加数据</h2><p>HBase 中添加数据是按照单元格添加，通过行键、列族、列名确定一个单元格。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">put &apos;student&apos;,&apos;e018565c-ebaa-11e8-b4ec-3c970e0087f3&apos;,&apos;info:name&apos;,&apos;wcwang&apos;</span><br><span class="line">put &apos;student&apos;,&apos;e018565c-ebaa-11e8-b4ec-3c970e0087f3&apos;,&apos;info:gender&apos;,&apos;F&apos;</span><br><span class="line">put &apos;student&apos;,&apos;e018565c-ebaa-11e8-b4ec-3c970e0087f3&apos;,&apos;info:age&apos;,&apos;22&apos;</span><br><span class="line"></span><br><span class="line">put &apos;student&apos;,&apos;e0182f4a-ebaa-11e8-a353-3c970e0087f3&apos;,&apos;info:name&apos;,&apos;lx&apos;</span><br><span class="line">put &apos;student&apos;,&apos;e0182f4a-ebaa-11e8-a353-3c970e0087f3&apos;,&apos;info:gender&apos;,&apos;M&apos;</span><br><span class="line">put &apos;student&apos;,&apos;e0182f4a-ebaa-11e8-a353-3c970e0087f3&apos;,&apos;info:age&apos;,&apos;21&apos;</span><br></pre></td></tr></table></figure></p>
<h1 id="编写运行程序"><a href="#编写运行程序" class="headerlink" title="编写运行程序"></a>编写运行程序</h1><p>创建一个 Scala 项目，参考<a href="https://weilu2.github.io/2018/11/16/%E9%85%8D%E7%BD%AE-Intellij-Idea-%E5%92%8C-Sbt-%E5%BC%80%E5%8F%91%E3%80%81%E6%89%93%E5%8C%85%E3%80%81%E8%BF%90%E8%A1%8C-Spark-%E7%A8%8B%E5%BA%8F/" target="_blank" rel="noopener">配置 Intellij Idea 和 Sbt 开发、打包、运行 Spark 程序</a>。</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>然后在 <code>src/main/scala/</code> 目录下创建一个 Scala 脚本，填充以下代码：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark._  </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">HBaseConfiguration</span>  </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.<span class="type">TableInputFormat</span>  </span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span>  </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataImport</span> </span>&#123;  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;  </span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"yarn"</span>).set(<span class="string">"spark.app.name"</span>, <span class="string">"MedicalQA"</span>)  </span><br><span class="line">    sparkConf.set(<span class="string">"spark.serializer"</span>,<span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)  </span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">val</span> hbaseConf = <span class="type">HBaseConfiguration</span>.create()  </span><br><span class="line">    hbaseConf.set(<span class="string">"hbase.zookeeper.property.clientPort"</span>, <span class="string">"2181"</span>)  </span><br><span class="line">    hbaseConf.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"192.168.0.131,192.168.0.132,192.168.0.133,192.168.0.151,192.168.0.152"</span>)  </span><br><span class="line">    hbaseConf.set(<span class="string">"hbase.master"</span>, <span class="string">"192.168.0.131"</span>)  </span><br><span class="line">    hbaseConf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>, <span class="string">"student"</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">val</span> studRDD = sc.newAPIHadoopRDD(hbaseConf, classOf[<span class="type">TableInputFormat</span>], classOf[org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span>], classOf[org.apache.hadoop.hbase.client.<span class="type">Result</span>])  </span><br><span class="line">    <span class="keyword">val</span> count = studRDD.count()  </span><br><span class="line">    println(count)  </span><br><span class="line">  </span><br><span class="line">    studRDD.cache()  </span><br><span class="line">    studRDD.collect().foreach(&#123;  </span><br><span class="line">      row =&gt; &#123;  </span><br><span class="line">        <span class="keyword">val</span> result = row._2  </span><br><span class="line">        <span class="keyword">val</span> key = <span class="type">Bytes</span>.toString(result.getRow)  </span><br><span class="line">        <span class="keyword">val</span> name = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"info"</span>.getBytes(), <span class="string">"name"</span>.getBytes()))  </span><br><span class="line">        <span class="keyword">val</span> gender = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"info"</span>.getBytes(), <span class="string">"gender"</span>.getBytes()))  </span><br><span class="line">        <span class="keyword">val</span> age = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"info"</span>.getBytes(), <span class="string">"age"</span>.getBytes()))  </span><br><span class="line">        println(key + <span class="string">"  "</span> + name + <span class="string">"  "</span> + gender + <span class="string">"  "</span> + age)  </span><br><span class="line">      &#125;  </span><br><span class="line">    &#125;)  </span><br><span class="line">  &#125;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在这段代码中首先统计了 student 表中数据的条数，然后将两条数据分别打印出来。</p>
<h2 id="项目配置"><a href="#项目配置" class="headerlink" title="项目配置"></a>项目配置</h2><p>将 Hbase 下面的 <code>hbase-site.xml</code> 文件拷贝到项目中的 <code>src/main/scala</code> 目录中。</p>
<h2 id="打包运行"><a href="#打包运行" class="headerlink" title="打包运行"></a>打包运行</h2><p>打包好的 JAR 包上传到服务器中，然后提交到 Spark 中运行：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/spark-2.4.0-bin-hadoop2.6/bin/spark-submit --deploy-mode cluster /home/workspace/MedicalQAImport.jar</span><br></pre></td></tr></table></figure></p>
<p>如果命令行和 Yarn 中没有抛异常，那就OK。然后检查 Yarn 中的日志。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">e0182f4a-ebaa-11e8-a353-3c970e0087f3  lx  M  21</span><br><span class="line">e018565c-ebaa-11e8-b4ec-3c970e0087f3  wcwang  F  22</span><br></pre></td></tr></table></figure></p>
<p>注意，这里输出的日志不是在控制台输出，而是在所运行的那台服务器的日志中，可以通过 Yarn 的界面查看。</p>
<h1 id="遇到问题"><a href="#遇到问题" class="headerlink" title="遇到问题"></a>遇到问题</h1><h2 id="java-lang-NoClassDefFoundError-com-yammer-metrics-core-Gauge"><a href="#java-lang-NoClassDefFoundError-com-yammer-metrics-core-Gauge" class="headerlink" title="java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge"></a>java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge</h2><p>这里就是之前所说的 HBase 和 Spark 依赖于同一个类库的不同版本的问题。在上传 Spark 的 JAR 包时，已经将 <code>metrics-core-3.1.5.jar</code> 上传了。但实际上 HBase 依赖的是 <code>metrics-core-2.2.0.jar</code> 的版本，因此还需要将这个包也上传。</p>
<p><strong>部分异常</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:</span><br><span class="line">Tue Nov 20 08:55:43 CST 2018, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0</span><br><span class="line">Caused by: java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0</span><br><span class="line">	</span><br><span class="line">Caused by: java.io.IOException: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge</span><br><span class="line"></span><br><span class="line">Caused by: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge</span><br><span class="line"></span><br><span class="line">Caused by: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge</span><br><span class="line"></span><br><span class="line">Caused by: java.lang.ClassNotFoundException: com.yammer.metrics.core.Gauge</span><br></pre></td></tr></table></figure></p>
<p><strong>完整异常信息</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line">18/11/20 08:55:44 ERROR ApplicationMaster: User class threw exception: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:</span><br><span class="line">Tue Nov 20 08:55:43 CST 2018, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0</span><br><span class="line"></span><br><span class="line">org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:</span><br><span class="line">Tue Nov 20 08:55:43 CST 2018, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0</span><br><span class="line"></span><br><span class="line">	at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.throwEnrichedException(RpcRetryingCallerWithReadReplicas.java:320)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:247)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:62)</span><br><span class="line">	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:327)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:302)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:167)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ClientScanner.&lt;init&gt;(ClientScanner.java:162)</span><br><span class="line">	at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:867)</span><br><span class="line">	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:193)</span><br><span class="line">	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:89)</span><br><span class="line">	at org.apache.hadoop.hbase.client.MetaScanner.allTableRegions(MetaScanner.java:324)</span><br><span class="line">	at org.apache.hadoop.hbase.client.HRegionLocator.getAllRegionLocations(HRegionLocator.java:88)</span><br><span class="line">	at org.apache.hadoop.hbase.util.RegionSizeCalculator.init(RegionSizeCalculator.java:94)</span><br><span class="line">	at org.apache.hadoop.hbase.util.RegionSizeCalculator.&lt;init&gt;(RegionSizeCalculator.java:81)</span><br><span class="line">	at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:256)</span><br><span class="line">	at org.apache.hadoop.hbase.mapreduce.TableInputFormat.getSplits(TableInputFormat.java:240)</span><br><span class="line">	at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)</span><br><span class="line">	at scala.Option.getOrElse(Option.scala:121)</span><br><span class="line">	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)</span><br><span class="line">	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)</span><br><span class="line">	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)</span><br><span class="line">	at DataImport$.main(DataImport.scala:18)</span><br><span class="line">	at DataImport.main(DataImport.scala)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)</span><br><span class="line">Caused by: java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0</span><br><span class="line">	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:169)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:80)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.io.IOException: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge</span><br><span class="line">	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:334)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:408)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:204)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:65)</span><br><span class="line">	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:397)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:371)</span><br><span class="line">	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:136)</span><br><span class="line">	... 4 more</span><br><span class="line">Caused by: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:240)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:336)</span><br><span class="line">	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:34094)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:400)</span><br><span class="line">	... 10 more</span><br><span class="line">Caused by: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:225)</span><br><span class="line">	... 13 more</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: com.yammer.metrics.core.Gauge</span><br><span class="line">	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">	... 14 more</span><br><span class="line">18/11/20 08:55:44 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:</span><br><span class="line">Tue Nov 20 08:55:43 CST 2018, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0</span><br><span class="line"></span><br><span class="line">	at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.throwEnrichedException(RpcRetryingCallerWithReadReplicas.java:320)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:247)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:62)</span><br><span class="line">	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:327)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:302)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:167)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ClientScanner.&lt;init&gt;(ClientScanner.java:162)</span><br><span class="line">	at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:867)</span><br><span class="line">	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:193)</span><br><span class="line">	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:89)</span><br><span class="line">	at org.apache.hadoop.hbase.client.MetaScanner.allTableRegions(MetaScanner.java:324)</span><br><span class="line">	at org.apache.hadoop.hbase.client.HRegionLocator.getAllRegionLocations(HRegionLocator.java:88)</span><br><span class="line">	at org.apache.hadoop.hbase.util.RegionSizeCalculator.init(RegionSizeCalculator.java:94)</span><br><span class="line">	at org.apache.hadoop.hbase.util.RegionSizeCalculator.&lt;init&gt;(RegionSizeCalculator.java:81)</span><br><span class="line">	at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits(TableInputFormatBase.java:256)</span><br><span class="line">	at org.apache.hadoop.hbase.mapreduce.TableInputFormat.getSplits(TableInputFormat.java:240)</span><br><span class="line">	at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)</span><br><span class="line">	at scala.Option.getOrElse(Option.scala:121)</span><br><span class="line">	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)</span><br><span class="line">	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)</span><br><span class="line">	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)</span><br><span class="line">	at DataImport$.main(DataImport.scala:18)</span><br><span class="line">	at DataImport.main(DataImport.scala)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)</span><br><span class="line">Caused by: java.net.SocketTimeoutException: callTimeout=60000, callDuration=68474: row &apos;student,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=weilu151,60020,1542616359146, seqNum=0</span><br><span class="line">	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:169)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run(ResultBoundedCompletionService.java:80)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">Caused by: java.io.IOException: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge</span><br><span class="line">	at org.apache.hadoop.hbase.protobuf.ProtobufUtil.getRemoteException(ProtobufUtil.java:334)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:408)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:204)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:65)</span><br><span class="line">	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:210)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:397)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas$RetryingRPC.call(ScannerCallableWithReplicas.java:371)</span><br><span class="line">	at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:136)</span><br><span class="line">	... 4 more</span><br><span class="line">Caused by: com.google.protobuf.ServiceException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:240)</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.AbstractRpcClient$BlockingRpcChannelImplementation.callBlockingMethod(AbstractRpcClient.java:336)</span><br><span class="line">	at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.scan(ClientProtos.java:34094)</span><br><span class="line">	at org.apache.hadoop.hbase.client.ScannerCallable.openScanner(ScannerCallable.java:400)</span><br><span class="line">	... 10 more</span><br><span class="line">Caused by: java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge</span><br><span class="line">	at org.apache.hadoop.hbase.ipc.AbstractRpcClient.callBlockingMethod(AbstractRpcClient.java:225)</span><br><span class="line">	... 13 more</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: com.yammer.metrics.core.Gauge</span><br><span class="line">	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)</span><br><span class="line">	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">	... 14 more</span><br><span class="line">)</span><br><span class="line">18/11/20 08:55:44 INFO SparkContext: Invoking stop() from shutdown hook</span><br></pre></td></tr></table></figure></p>
<h2 id="had-a-not-serializable-result"><a href="#had-a-not-serializable-result" class="headerlink" title="had a not serializable result"></a>had a not serializable result</h2><p><strong>解决</strong><br>在 Spark 的环境中配置：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sparkConf.set(<span class="string">"spark.serializer"</span>,<span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br></pre></td></tr></table></figure></p>
<p><strong>异常信息</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line">18/11/20 09:55:25 ERROR TaskSetManager: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line">Serialization stack:</span><br><span class="line">	- object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33)</span><br><span class="line">	- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)</span><br><span class="line">	- object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;))</span><br><span class="line">	- element of array (index: 0)</span><br><span class="line">	- array (class [Lscala.Tuple2;, size 2); not retrying</span><br><span class="line">18/11/20 09:55:25 INFO YarnClusterScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool </span><br><span class="line">18/11/20 09:55:25 INFO YarnClusterScheduler: Cancelling stage 1</span><br><span class="line">18/11/20 09:55:25 INFO YarnClusterScheduler: Killing all running tasks in stage 1: Stage cancelled</span><br><span class="line">18/11/20 09:55:25 INFO DAGScheduler: ResultStage 1 (collect at DataImport.scala:23) failed in 0.338 s due to Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line">Serialization stack:</span><br><span class="line">	- object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33)</span><br><span class="line">	- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)</span><br><span class="line">	- object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;))</span><br><span class="line">	- element of array (index: 0)</span><br><span class="line">	- array (class [Lscala.Tuple2;, size 2)</span><br><span class="line">18/11/20 09:55:25 INFO DAGScheduler: Job 1 failed: collect at DataImport.scala:23, took 0.346124 s</span><br><span class="line">18/11/20 09:55:25 ERROR ApplicationMaster: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line">Serialization stack:</span><br><span class="line">	- object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33)</span><br><span class="line">	- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)</span><br><span class="line">	- object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;))</span><br><span class="line">	- element of array (index: 0)</span><br><span class="line">	- array (class [Lscala.Tuple2;, size 2)</span><br><span class="line">org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line">Serialization stack:</span><br><span class="line">	- object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33)</span><br><span class="line">	- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)</span><br><span class="line">	- object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;))</span><br><span class="line">	- element of array (index: 0)</span><br><span class="line">	- array (class [Lscala.Tuple2;, size 2)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)</span><br><span class="line">	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)</span><br><span class="line">	at scala.Option.foreach(Option.scala:257)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)</span><br><span class="line">	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)</span><br><span class="line">	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)</span><br><span class="line">	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)</span><br><span class="line">	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)</span><br><span class="line">	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)</span><br><span class="line">	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)</span><br><span class="line">	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)</span><br><span class="line">	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)</span><br><span class="line">	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)</span><br><span class="line">	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)</span><br><span class="line">	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)</span><br><span class="line">	at DataImport$.main(DataImport.scala:23)</span><br><span class="line">	at DataImport.main(DataImport.scala)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)</span><br><span class="line">18/11/20 09:55:25 INFO ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritable</span><br><span class="line">Serialization stack:</span><br><span class="line">	- object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: 65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33)</span><br><span class="line">	- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)</span><br><span class="line">	- object (class scala.Tuple2, (65 30 31 38 35 36 35 63 2d 65 62 61 61 2d 31 31 65 38 2d 62 34 65 63 2d 33 63 39 37 30 65 30 30 38 37 66 33,keyvalues=&#123;e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:age/1542606358169/Put/vlen=2/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:gender/1542606352584/Put/vlen=1/seqid=0, e0182f4a-ebaa-11e8-a353-3c970e0087f3/info:name/1542606346782/Put/vlen=2/seqid=0&#125;))</span><br><span class="line">	- element of array (index: 0)</span><br><span class="line">	- array (class [Lscala.Tuple2;, size 2)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)</span><br><span class="line">	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)</span><br><span class="line">	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)</span><br><span class="line">	at scala.Option.foreach(Option.scala:257)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)</span><br><span class="line">	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)</span><br><span class="line">	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)</span><br><span class="line">	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)</span><br><span class="line">	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)</span><br><span class="line">	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)</span><br><span class="line">	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)</span><br><span class="line">	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)</span><br><span class="line">	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)</span><br><span class="line">	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)</span><br><span class="line">	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)</span><br><span class="line">	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)</span><br><span class="line">	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)</span><br><span class="line">	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)</span><br><span class="line">	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)</span><br><span class="line">	at DataImport$.main(DataImport.scala:23)</span><br><span class="line">	at DataImport.main(DataImport.scala)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">	at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:678)</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>参考：<a href="https://segmentfault.com/q/1010000007041500" target="_blank" rel="noopener">https://segmentfault.com/q/1010000007041500</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/11/20/基于-Scala-编写的-Spark-程序操作-HBase/" data-id="cjp5jgwrk009yf8lu4q0rkgzo" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HBase/">HBase</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Scala/">Scala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Scala操作HBase/">Scala操作HBase</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/">Spark</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/11/21/npm-install-Error-EACCES-permission-denied/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          npm install Error EACCES permission denied
        
      </div>
    </a>
  
  
    <a href="/2018/11/20/Spark-关键概念备忘/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Spark 关键概念备忘</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='//unpkg.com/valine/dist/Valine.min.js'></script>
<div id="vcomment"></div>
<script>
	window.onload=function(){
		var valine = new Valine();
	    valine.init({
	        el:'#vcomment',
	        visitor: true,
	        appId: 'vnzbDotbVyiabKcguwfQoQdU-gzGzoHsz',
	        appKey: 'c3TWlvkfKgsN0dcLC7ftudvf'
	    })
	}
</script>
</section>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Hadoop/">Hadoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Idea/">Idea</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Java/">Java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/KVM/">KVM</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MachineLearning/">MachineLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/MySQL/">MySQL</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Nodejs/">Nodejs</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scala/">Scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Scrapy/">Scrapy</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Spark/">Spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Windows/">Windows</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kernel/">kernel</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/乱七八糟/">乱七八糟</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/饭团的小日子/">饭团的小日子</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/2080ti/">2080ti</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D/">3D</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-Bar-Charts/">3D Bar Charts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-Scatter-Plots/">3D Scatter Plots</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-Surfaces/">3D Surfaces</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-散点图/">3D 散点图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-曲面图/">3D 曲面图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/3D-柱状图/">3D 柱状图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/AdaBoostClassifier/">AdaBoostClassifier</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Array/">Array</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BaggingClassifier/">BaggingClassifier</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bar-Charts/">Bar Charts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/BernoulliNB/">BernoulliNB</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Blog/">Blog</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Bridge/">Bridge</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cross-Validation/">Cross Validation</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DAG/">DAG</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DIY/">DIY</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Analysis/">Data Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Science/">Data Science</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Visualization/">Data Visualization</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-wrangling/">Data wrangling</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DataFrame/">DataFrame</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DecisionTreeClassifier/">DecisionTreeClassifier</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Driver/">Driver</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Exector/">Exector</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Explorator-Analysis/">Explorator Analysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ExtraTreeClassifier/">ExtraTreeClassifier</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Feature-Selection/">Feature Selection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GaussianNB/">GaussianNB</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GitHub/">GitHub</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/GradientBoostingClassifier/">GradientBoostingClassifier</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HBase/">HBase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Historgrams/">Historgrams</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/History-Server/">History-Server</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Idea/">Idea</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/JVM/">JVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/">Java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Job/">Job</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KMeans/">KMeans</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KNeighborsClassifier/">KNeighborsClassifier</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KVM/">KVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kaggle/">Kaggle</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/KickStart/">KickStart</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Line-Charts/">Line Charts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LinearDiscriminantAnalysis/">LinearDiscriminantAnalysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LinearSVC/">LinearSVC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Log4J/">Log4J</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LogBack/">LogBack</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LogisticRegression/">LogisticRegression</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MAT/">MAT</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MLPClassifier/">MLPClassifier</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Math-Function/">Math Function</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matplotlib/">Matplotlib</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Maven/">Maven</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/MongoDB/">MongoDB</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NVIDIA/">NVIDIA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Network/">Network</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Network-Connection/">Network Connection</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Nodejs/">Nodejs</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NuSVC/">NuSVC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Numpy/">Numpy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PairRDD/">PairRDD</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pandas/">Pandas</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PassiveAggressiveClassifier/">PassiveAggressiveClassifier</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Pie-Charts/">Pie Charts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Plotly/">Plotly</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/">Python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/QuadraticDiscriminantAnalysis/">QuadraticDiscriminantAnalysis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RDD/">RDD</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RadiusNeighborsClassifier/">RadiusNeighborsClassifier</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/RandomForestClassifier/">RandomForestClassifier</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Run-Main/">Run Main</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SKLearn/">SKLearn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SLF4J/">SLF4J</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SSH/">SSH</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVC/">SVC</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Sbt/">Sbt</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scala/">Scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scala操作HBase/">Scala操作HBase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scrapy/">Scrapy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scrapyd-client/">Scrapyd-client</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Scrapyd-deploy/">Scrapyd-deploy</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Seaborn/">Seaborn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Series/">Series</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Slicing/">Slicing</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark/">Spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark-on-Yarn/">Spark on Yarn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Spark-shell/">Spark-shell</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Stage/">Stage</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Static-IP/">Static IP</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Syntax/">Syntax</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Titanic/">Titanic</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Visual-C/">Visual C++</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ZooKeeper/">ZooKeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/docker/">docker</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/firewall/">firewall</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hdfs/">hdfs</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/my-ini/">my.ini</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nbd/">nbd</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nodejs/">nodejs</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/npm/">npm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/open-vswitch/">open-vswitch</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/port/">port</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sbt-package/">sbt package</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/yarn/">yarn</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/交互式/">交互式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/休眠/">休眠</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/免密登录/">免密登录</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/内存分析/">内存分析</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/命令行安装虚拟机/">命令行安装虚拟机</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/多线程/">多线程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/容器/">容器</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/折线图/">折线图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/无人值守/">无人值守</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/柱状图/">柱状图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/桥接/">桥接</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/烘干机/">烘干机</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/爬虫/">爬虫</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/生产者消费者模式/">生产者消费者模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/直方图/">直方图</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/端口/">端口</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/网络连接/">网络连接</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/跨主机/">跨主机</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/防火墙/">防火墙</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/集群/">集群</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/饼状图/">饼状图</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/2080ti/" style="font-size: 10px;">2080ti</a> <a href="/tags/3D/" style="font-size: 12.5px;">3D</a> <a href="/tags/3D-Bar-Charts/" style="font-size: 10px;">3D Bar Charts</a> <a href="/tags/3D-Scatter-Plots/" style="font-size: 12.5px;">3D Scatter Plots</a> <a href="/tags/3D-Surfaces/" style="font-size: 10px;">3D Surfaces</a> <a href="/tags/3D-散点图/" style="font-size: 12.5px;">3D 散点图</a> <a href="/tags/3D-曲面图/" style="font-size: 10px;">3D 曲面图</a> <a href="/tags/3D-柱状图/" style="font-size: 10px;">3D 柱状图</a> <a href="/tags/AdaBoostClassifier/" style="font-size: 10px;">AdaBoostClassifier</a> <a href="/tags/Array/" style="font-size: 10px;">Array</a> <a href="/tags/BaggingClassifier/" style="font-size: 10px;">BaggingClassifier</a> <a href="/tags/Bar-Charts/" style="font-size: 12.5px;">Bar Charts</a> <a href="/tags/BernoulliNB/" style="font-size: 10px;">BernoulliNB</a> <a href="/tags/Blog/" style="font-size: 10px;">Blog</a> <a href="/tags/Bridge/" style="font-size: 12.5px;">Bridge</a> <a href="/tags/Cross-Validation/" style="font-size: 10px;">Cross Validation</a> <a href="/tags/DAG/" style="font-size: 10px;">DAG</a> <a href="/tags/DIY/" style="font-size: 10px;">DIY</a> <a href="/tags/Data-Analysis/" style="font-size: 10px;">Data Analysis</a> <a href="/tags/Data-Science/" style="font-size: 10px;">Data Science</a> <a href="/tags/Data-Visualization/" style="font-size: 12.5px;">Data Visualization</a> <a href="/tags/Data-wrangling/" style="font-size: 10px;">Data wrangling</a> <a href="/tags/DataFrame/" style="font-size: 10px;">DataFrame</a> <a href="/tags/DecisionTreeClassifier/" style="font-size: 10px;">DecisionTreeClassifier</a> <a href="/tags/Driver/" style="font-size: 10px;">Driver</a> <a href="/tags/Exector/" style="font-size: 10px;">Exector</a> <a href="/tags/Explorator-Analysis/" style="font-size: 10px;">Explorator Analysis</a> <a href="/tags/ExtraTreeClassifier/" style="font-size: 10px;">ExtraTreeClassifier</a> <a href="/tags/Feature-Selection/" style="font-size: 10px;">Feature Selection</a> <a href="/tags/GaussianNB/" style="font-size: 10px;">GaussianNB</a> <a href="/tags/GitHub/" style="font-size: 10px;">GitHub</a> <a href="/tags/GradientBoostingClassifier/" style="font-size: 10px;">GradientBoostingClassifier</a> <a href="/tags/HBase/" style="font-size: 15px;">HBase</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Historgrams/" style="font-size: 12.5px;">Historgrams</a> <a href="/tags/History-Server/" style="font-size: 10px;">History-Server</a> <a href="/tags/Idea/" style="font-size: 12.5px;">Idea</a> <a href="/tags/JVM/" style="font-size: 10px;">JVM</a> <a href="/tags/Java/" style="font-size: 10px;">Java</a> <a href="/tags/Job/" style="font-size: 10px;">Job</a> <a href="/tags/KMeans/" style="font-size: 10px;">KMeans</a> <a href="/tags/KNeighborsClassifier/" style="font-size: 10px;">KNeighborsClassifier</a> <a href="/tags/KVM/" style="font-size: 12.5px;">KVM</a> <a href="/tags/Kaggle/" style="font-size: 10px;">Kaggle</a> <a href="/tags/KickStart/" style="font-size: 10px;">KickStart</a> <a href="/tags/Line-Charts/" style="font-size: 12.5px;">Line Charts</a> <a href="/tags/LinearDiscriminantAnalysis/" style="font-size: 10px;">LinearDiscriminantAnalysis</a> <a href="/tags/LinearSVC/" style="font-size: 10px;">LinearSVC</a> <a href="/tags/Linux/" style="font-size: 10px;">Linux</a> <a href="/tags/Log4J/" style="font-size: 10px;">Log4J</a> <a href="/tags/LogBack/" style="font-size: 10px;">LogBack</a> <a href="/tags/LogisticRegression/" style="font-size: 10px;">LogisticRegression</a> <a href="/tags/MAT/" style="font-size: 10px;">MAT</a> <a href="/tags/MLPClassifier/" style="font-size: 10px;">MLPClassifier</a> <a href="/tags/Machine-Learning/" style="font-size: 10px;">Machine Learning</a> <a href="/tags/Math-Function/" style="font-size: 10px;">Math Function</a> <a href="/tags/Matplotlib/" style="font-size: 12.5px;">Matplotlib</a> <a href="/tags/Maven/" style="font-size: 12.5px;">Maven</a> <a href="/tags/MongoDB/" style="font-size: 10px;">MongoDB</a> <a href="/tags/NVIDIA/" style="font-size: 10px;">NVIDIA</a> <a href="/tags/Network/" style="font-size: 10px;">Network</a> <a href="/tags/Network-Connection/" style="font-size: 10px;">Network Connection</a> <a href="/tags/Nodejs/" style="font-size: 10px;">Nodejs</a> <a href="/tags/NuSVC/" style="font-size: 10px;">NuSVC</a> <a href="/tags/Numpy/" style="font-size: 15px;">Numpy</a> <a href="/tags/PairRDD/" style="font-size: 10px;">PairRDD</a> <a href="/tags/Pandas/" style="font-size: 12.5px;">Pandas</a> <a href="/tags/PassiveAggressiveClassifier/" style="font-size: 10px;">PassiveAggressiveClassifier</a> <a href="/tags/Pie-Charts/" style="font-size: 12.5px;">Pie Charts</a> <a href="/tags/Plotly/" style="font-size: 10px;">Plotly</a> <a href="/tags/Python/" style="font-size: 20px;">Python</a> <a href="/tags/QuadraticDiscriminantAnalysis/" style="font-size: 10px;">QuadraticDiscriminantAnalysis</a> <a href="/tags/RDD/" style="font-size: 10px;">RDD</a> <a href="/tags/RadiusNeighborsClassifier/" style="font-size: 10px;">RadiusNeighborsClassifier</a> <a href="/tags/RandomForestClassifier/" style="font-size: 10px;">RandomForestClassifier</a> <a href="/tags/Run-Main/" style="font-size: 10px;">Run Main</a> <a href="/tags/SKLearn/" style="font-size: 10px;">SKLearn</a> <a href="/tags/SLF4J/" style="font-size: 10px;">SLF4J</a> <a href="/tags/SSH/" style="font-size: 10px;">SSH</a> <a href="/tags/SVC/" style="font-size: 10px;">SVC</a> <a href="/tags/Sbt/" style="font-size: 10px;">Sbt</a> <a href="/tags/Scala/" style="font-size: 17.5px;">Scala</a> <a href="/tags/Scala操作HBase/" style="font-size: 10px;">Scala操作HBase</a> <a href="/tags/Scrapy/" style="font-size: 10px;">Scrapy</a> <a href="/tags/Scrapyd-client/" style="font-size: 10px;">Scrapyd-client</a> <a href="/tags/Scrapyd-deploy/" style="font-size: 10px;">Scrapyd-deploy</a> <a href="/tags/Seaborn/" style="font-size: 10px;">Seaborn</a> <a href="/tags/Series/" style="font-size: 10px;">Series</a> <a href="/tags/Slicing/" style="font-size: 10px;">Slicing</a> <a href="/tags/Spark/" style="font-size: 15px;">Spark</a> <a href="/tags/Spark-on-Yarn/" style="font-size: 10px;">Spark on Yarn</a> <a href="/tags/Spark-shell/" style="font-size: 10px;">Spark-shell</a> <a href="/tags/Stage/" style="font-size: 10px;">Stage</a> <a href="/tags/Static-IP/" style="font-size: 10px;">Static IP</a> <a href="/tags/Syntax/" style="font-size: 10px;">Syntax</a> <a href="/tags/Titanic/" style="font-size: 10px;">Titanic</a> <a href="/tags/Visual-C/" style="font-size: 10px;">Visual C++</a> <a href="/tags/ZooKeeper/" style="font-size: 10px;">ZooKeeper</a> <a href="/tags/docker/" style="font-size: 10px;">docker</a> <a href="/tags/firewall/" style="font-size: 10px;">firewall</a> <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a> <a href="/tags/hdfs/" style="font-size: 10px;">hdfs</a> <a href="/tags/linux/" style="font-size: 10px;">linux</a> <a href="/tags/my-ini/" style="font-size: 10px;">my.ini</a> <a href="/tags/nbd/" style="font-size: 10px;">nbd</a> <a href="/tags/nodejs/" style="font-size: 10px;">nodejs</a> <a href="/tags/npm/" style="font-size: 10px;">npm</a> <a href="/tags/open-vswitch/" style="font-size: 10px;">open-vswitch</a> <a href="/tags/port/" style="font-size: 10px;">port</a> <a href="/tags/sbt-package/" style="font-size: 10px;">sbt package</a> <a href="/tags/yarn/" style="font-size: 10px;">yarn</a> <a href="/tags/交互式/" style="font-size: 10px;">交互式</a> <a href="/tags/休眠/" style="font-size: 10px;">休眠</a> <a href="/tags/免密登录/" style="font-size: 10px;">免密登录</a> <a href="/tags/内存分析/" style="font-size: 10px;">内存分析</a> <a href="/tags/命令行安装虚拟机/" style="font-size: 10px;">命令行安装虚拟机</a> <a href="/tags/多线程/" style="font-size: 10px;">多线程</a> <a href="/tags/容器/" style="font-size: 10px;">容器</a> <a href="/tags/折线图/" style="font-size: 12.5px;">折线图</a> <a href="/tags/无人值守/" style="font-size: 10px;">无人值守</a> <a href="/tags/柱状图/" style="font-size: 12.5px;">柱状图</a> <a href="/tags/桥接/" style="font-size: 10px;">桥接</a> <a href="/tags/烘干机/" style="font-size: 10px;">烘干机</a> <a href="/tags/爬虫/" style="font-size: 10px;">爬虫</a> <a href="/tags/生产者消费者模式/" style="font-size: 10px;">生产者消费者模式</a> <a href="/tags/直方图/" style="font-size: 12.5px;">直方图</a> <a href="/tags/端口/" style="font-size: 10px;">端口</a> <a href="/tags/网络连接/" style="font-size: 10px;">网络连接</a> <a href="/tags/跨主机/" style="font-size: 10px;">跨主机</a> <a href="/tags/防火墙/" style="font-size: 10px;">防火墙</a> <a href="/tags/集群/" style="font-size: 12.5px;">集群</a> <a href="/tags/饼状图/" style="font-size: 12.5px;">饼状图</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/12/01/Python-机器学习类库-Scikit-Learn-Tutorial/">Python 机器学习类库 Scikit-Learn Tutorial</a>
          </li>
        
          <li>
            <a href="/2018/11/30/交互式数据可视化-Plotly-Tutorial/">交互式数据可视化 Plotly Tutorial</a>
          </li>
        
          <li>
            <a href="/2018/11/30/数据可视化-Matplotlib-Tutorial/">数据可视化 Matplotlib Tutorial</a>
          </li>
        
          <li>
            <a href="/2018/11/29/Pandas-Tutorial/">Pandas Tutorial</a>
          </li>
        
          <li>
            <a href="/2018/11/29/Numpy-Tutorial/">Numpy Tutorial</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 weilu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    

<script src="http://libs.baidu.com/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>